---
title: Beyond main effects? Affect level as a moderator in the relation between affect dynamics and depressive symptoms
author: "BLINDED"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    keep_tex: yes
    number_sections: yes
    toc: yes
    toc_float: yes
    toc_depth: 6
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```

# DISCLAIMER

For anyone taking the time to read this script. I am not working in R for such a long time. Thus, I am not good (yet) in functions, loops, if statements etc. This script could probably been have considerably shorter. Sorry for that!
If you have any questions or find any mistakes, please don't hesitate to e-mail me under <BLINDED>.

# DEVIATIONS FROM THE PRE-REGISTRATION

There were several deviations from the pre-registration (<https://osf.io/4dvsc>), which are described on the OSF page as well as the supplementary material of the paper (<https://osf.io/djufr/>).

After comments from reviewers, who (rightfully) criticized that the rMSSD and SD are so highly correlated, which questions the utility of analyzing them separately, I decided to put results of the rMSSD is the supplement and only to focus on the SD (variability) and autocorrelation (inertia), as those capture two distinct dimensions (dispersion and the time element). The models reported in the paper contain both variability (SD) and inertia (autocorrelation) as simultaneous predictors, but I have still the individual results in here for transparency.

# LIBRARY 
```{r library, cache=FALSE}
options(scipen=999) #to prevent scientific notation


library(haven)  #for reading in SPSS files
library(esmpack) #for data preparation of ESM data
library(data.table) #for data.tables, subsetting
library(psych) #for descriptives and affective dynamics calculations (count, M, SD)
library(DescTools) #for winsorizing outliers
library(tibble) #for reshaping
library(tidyverse) #for data wrangling
library(Hmisc) #for correlations
library(here) #for finding files
library(plyr) #for data wrangling
library(dplyr)#for data wrangling
library(ggplot2) #for data visualization
library(RColorBrewer) # for data visualization
library(scales) # for data visualization
library(ggpubr) #for data visualization
library(lme4) #for autocorrelation and ICCs
library(optimx) #optimizer for MLM models
library(lavaan) #for MCFA's
library(performance) #for ICC
library(fastDummies) #for making dummy variables
library(lm.beta) #for standardized estimates from lm
library(multilevelTools) #for omega calculation
library(pander) #for tables
library(knitr) #for tables
library(interactions) #for Johnson-Neyman
library(apaTables) #for apaTables
library(jtools) #for nice regression tables
library(car) #for VIF
library(metafor) #for pooled correlations
library(relativeVariability) #for calculating relative SD
library(purrr) 
library(cowplot)

set.alignment('left', row.names = 'right') #for alignment of tables

sessionInfo()

```

Note: We initially also pre-registered specific calculation procedures, which we later replaced with more parsimonious code resulting in equivalent results. 
Example 1: We used the esmpack package for a majority of the data wrangling and thus some detailed steps that we described in the pre-registration were not needed anymore).

Example 2: Our initial plan was to calculate the affective dynamic indices per week (for daily diary studies) / per day (for ESM studies) and then to average those per individual. This was to ensure that differences between the last assessment of one week/day and the first assessment of the following week/day were not calculated, because these were too far apart. However, we just manually added a missing day for calculations of MSSD and used the esmpack package (Viechtbauer, 2021) that calculates lagged variables while taking into account the days for the autocorrelation. By doing that, it was possible to calculate the affective dynamic indices across the whole period at once. It also made the code considerably shorter.

# FUNCTIONS
```{r functions, cache=FALSE}

#######FUNCTION 1: add missing days after each day/week (Daily Diary/ESM)
##this is important, because for the MSSD, we do not want to calculate the
##difference between the last beep of one day/week and the first beep of the next day/week
##For the autocorrelation, the lagged function from the esmpack takes care of that automatically

insert_NA <- function(df, ...){
  df_selected <- df %>% dplyr::select(...) %>% unique()
  result <- plyr::rbind.fill(df, df_selected) %>%
    dplyr::arrange(...)
  return(result)
}

#######FUNCTION 2: MSSD
##from https://quantdev.ssri.psu.edu/tutorials/analysis-experience-sampling-ema-data-chapter-22-univariate-intraindividual-variability
##We pre-registered using the function from the psych package, but as described by Ram (2017), than function
##incorrectly divides by t-2

my.mssd <- function(data)
{
    diffToNext<-data[2:length(data)]-data[1:(length(data)-1)] #this computes the difference between each value and the next
    diffToNext2<-diffToNext^2                  #this squares the difference
    SSdiff<- sum(diffToNext2,na.rm=TRUE)       #this takes the sum of the squared differences
    denominator<-sum(!is.na(diffToNext))       #this computes the number of non-missing elements (denominator)
                                               #which corresponds to the t-1 value
    mssd<-sqrt(SSdiff/denominator)             #this computes the rMSSD
    return(mssd)
}

#######FUNCTION 3: Cohen's f2
##to calculate the effect size of regression models. From https://stackoverflow.com/questions/72229235/cohens-f2-in-r
cohen_f2 <- function(fit, fit2){
  R2 <- summary(fit)$r.squared
  if(missing(fit2)) {
    R2/(1 - R2)
  } else {
    R2B <- summary(fit2)$r.squared
    (R2B - R2)/(1 - R2B)
  }
}

```

# OVERVIEW STUDIES

## ITEMS

1. NA: Negative Affect  
2. PA: Positive Affect  
3. BA: Bipolar Affect  

Note that only studies 6 and 7 had data on bipolar affect. Because of the low
number, we did not conduct main analyses for bipolar affect.

### Dataset 1: RADAR

|               | Item Number | Item Dutch | Item English   |
|---------------|-------------|------------|----------------|
| Happiness (PA)| 1           | blij       | glad           |
|               | 2           | vrolijk    | happy          |
|               | 3           | opgewekt   | cheerful       |
| Anger     (NA)| 4           | boos       | angry          |
|               | 5           | kwaad      | cross          |
|               | 6           | driftig    | short-tempered |
| Anxiety   (NA)| 7           | bang       | afraid         |
|               | 8           | angstig    | anxious        |
|               | 9           | bezorgd    | worried        |
| Sadness   (NA)| 10          | verdrietig | sad            |
|               | 11          | droef      | down           |
|               | 12          | triest     | dreary         |

### Dataset 2: SWINGING MOODS


|               | Item Number | Item Dutch | Item English   |
|---------------|-------------|------------|----------------|
| PA            | 1           | opgewekt   | cheerful       |
|               | 2           | tevreden   | content        |
|               | 3           | gelukkig   | lucky          |
|               | 4           | energiek   | energetic      |
|               | 5           | ontspannen | relaxed        |
|               | 6           | vrolijk    | happy          |
| NA            | 1           | onzeker    | insecure       |
|               | 2           | angstig    | anxious        |
|               | 3           | geïrriteerd| irritated      |
|               | 4           | bezorgd    | worried        |
|               | 5           | somber     | depressed      |
|               | 6           | schuldig   | guilty         |

### Dataset 3: MOOD IN EMERGING ADULTS


|               | Item Number | Item Dutch | Item English   |
|---------------|-------------|------------|----------------|
| PA            | 1           | opgewekt   | cheerful       |
|               | 2           | tevreden   | content        |
|               | 3           | gelukkig   | lucky          |
|               | 4           | energiek   | energetic      |
|               | 5           | ontspannen | relaxed        |
|               | 6           | vrolijk    | happy          |
| NA            | 1           | onzeker    | insecure       |
|               | 2           | angstig    | anxious        |
|               | 3           | geïrriteerd| irritated      |
|               | 4           | bezorgd    | worried        |
|               | 5           | somber     | depressed      |
|               | 6           | schuldig   | guilty         |
 
### Dataset 4: EMOTIONS IN DAILY LIFE

|               | Item Number | Item Dutch  | Item English   |
|---------------|-------------|-------------|----------------|
| PA            | 1           | enthousiast | enthusiastic   |
|               | 2           | tevreden    | content        |
|               | 3           | energiek    | energetic      |
|               | 4           | kalm        | calm           |
|               | 5           | daadkrachtig| decisive       |
|               | 6           | vrolijk     | happy          |
|               | 7           | dankbaar    | thankful       |
| NA            | 1           | geïrriteerd | irritated      |
|               | 2           | verveeld    | bored          |
|               | 3           | nerveus     | nervous        |
|               | 4           | verdrietig  | sad            |
|               | 5           | boos        | angry          |
|               | 6           | somber      | depressed      |


### Dataset 5: EMOTION REGULATION IN ACTION


|               | Item Number | Item Dutch  | Item English   |
|---------------|-------------|-------------|----------------|
| PA            | 1           | opgewekt    | cheerful       |
|               | 2           | tevreden    | satisfied      |
|               | 3           | ontspannen  | relaxed        |
|               | 4           | vrolijk     | happy          |
|               | 5           | trots       | proud          |
|               | 6           | bewondering | admiration     |
|               | 7           | dankbaar    | thankful       |
| NA            | 1           | jaloers     | jealous        |
|               | 2           | angstig     | anxious        |
|               | 3           | beschaamd   | ashamed        |
|               | 4           | geïrriteerd | irritated      |
|               | 5           | bezorgd     | worried        |
|               | 6           | boos        | angry          |
|               | 7           | schuldig    | guilty         |
|               | 8           | verdrietig  | sad            |
|               | 9           | eenzam      | lonely         |

### Dataset 6: LASER

|               | Item Number | Item English                               |
|---------------|-------------|--------------------------------------------|
| BA            | 1           | How do you feel right now?                 | 
| NA            | 2           | Rate the intensity of the                  |
|               |             | strongest negative emotion in the last hour| 

For item 1: Response option went from very negative to very positive.

For item 2: Participants chose from a list of 3 emotions: Anger, Anxiety, Sadness. The strongest experienced emotion was subsequently rated on intensity.

### Dataset 7: YES

|               | Item Number | Item English                               |
|---------------|-------------|--------------------------------------------|
| BA            | 1           | How do you feel right now?                 | 
| NA            | 2           | Rate the intensity of the                  |
|               |             | strongest negative emotion in the last hour| 

For item 1: Response option went from very negative to very positive.

For item 2: Participants chose from a list of 3 emotions: Anger, Anxiety, Sadness. The strongest experienced emotion was subsequently rated on intensity.

## AFFECT AND DEPRESSION

```{r Data, cache=FALSE, echo=FALSE}


data<-
  data.frame(Study=c("RADAR", "Swinging Moods", "Mood in Emerging Adults", "Emotions in daily life", "Emotion Regulation in Action",  "LASER","YES"),
             Depression_Base = c("X","X","X","X","X","X","X") ,
             Depression_FU = c("X","X","X"," "," "," ","X") ,
             Positive_affect=c("X","X","X","X","X"," "," ") ,
             Negative_affect=c("X","X","X","X","X","X^","X^"),
             Bipolar_affect=c(" "," "," "," "," ","X","X"))

pander(data,caption = "Data available")

pandoc.footnote("Most intense negative affect")

```

# DATAPREPARATION PER DATASET

## DATASET 1 - RADAR

### Read in data and datapreparation
```{r RADAR read in data, cache=FALSE}

####################### Read in data #######################
dataset1_RADAR_processed<-read.csv(here::here("data", "2_processed_data_for_OSF", "dataset1_RADAR_processed.csv"), sep=",",
                             na.strings="NA")

####################### number of participants with valid Daily Diary data ##################### 
dataset1_RADAR_processed$count<-calc.nomiss(PAff, ID, dataset1_RADAR_processed, prop=FALSE, expand=TRUE)
range(dataset1_RADAR_processed$count)

# exclude those with not valid daily diary data
n_RADAR_orig<-nsub(ID, dataset1_RADAR_processed)
n_RADAR_orig

dataset1_RADAR_processed <-subset(dataset1_RADAR_processed, count > 0) 
range(dataset1_RADAR_processed$count)

n_RADAR_orig<-nsub(ID, dataset1_RADAR_processed)
n_RADAR_orig


```

### Psychometrics
#### Multilevel Confirmatory Factor Analysis

Following procedures from Eisele et al (2021).

```{r RADAR MFCA, cache=FALSE}

##################### delete observations with 0 variance #####################
dataset1_RADAR_processed1<-dataset1_RADAR_processed
dataset1_RADAR_processed1 <- check.timeinvar(PA1, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(PA2, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(PA3, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(NA1, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(NA2, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(NA3, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(NA4, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(NA5, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(NA6, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(NA7, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(NA8, ID, dataset1_RADAR_processed1, out = 3)
dataset1_RADAR_processed1 <- check.timeinvar(NA9, ID, dataset1_RADAR_processed1, out = 3)

##################### MCFA ######################
#### Between-person (within level saturated) ####
## NA PA structure ##

# Model with NA and PA as factor, not taking into account
# the overall emotions (sadness, anxiety, anger, happiness were each measured with 3 items)


mcfa_between_NAPA <- '
level: 1

#correlation between all items
PA1 ~~ PA2 + PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA2 ~~ PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA3 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA2 ~~ NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA3 ~~ NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA4 ~~ NA5 + NA6 + NA7 + NA8 + NA9
NA5 ~~ NA6 + NA7 + NA8 + NA9
NA6 ~~ NA7 + NA8 + NA9
NA7 ~~ NA8 + NA9
NA8 ~~ NA9

level: 2

#factor structure
pa =~ PA1 + PA2 + PA3
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

'

m1_R_B_RADAR <- sem(model = mcfa_between_NAPA, data = dataset1_RADAR_processed1, cluster = "ID")
summary(m1_R_B_RADAR, fit.measures=TRUE, standardized = TRUE)

# Factor loadings are very high (>.84)
# Model fit is good (CFI=.99, RMSEA=.04)
# Negative residual variance of PA2 (p = .498). Will be fixed to 0

## NA PA structure - fixing negative residual variance of PA2 to 0 ##

mcfa_between_NAPA_PA2 <- '
level: 1

#correlation between all items
PA1 ~~ PA2 + PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA2 ~~ PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA3 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA2 ~~ NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA3 ~~ NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA4 ~~ NA5 + NA6 + NA7 + NA8 + NA9
NA5 ~~ NA6 + NA7 + NA8 + NA9
NA6 ~~ NA7 + NA8 + NA9
NA7 ~~ NA8 + NA9
NA8 ~~ NA9

level: 2

#factor structure
pa =~ PA1 + PA2 + PA3
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

#fixing neg res variance to 0
PA2 ~~ 0*PA2

'

m2_R_B_RADAR <- sem(model = mcfa_between_NAPA_PA2, data = dataset1_RADAR_processed1, cluster = "ID")
summary(m2_R_B_RADAR, fit.measures=TRUE, standardized = TRUE)

# no errors
# Factor loadings are very high (>.84)
# Model fit is good (CFI=.99, RMSEA=.04)

## NA PA structure with taking emotions into account ##

#In the study, four different emotions were measured with 3 items each. Thus, I am also testing a 
#model taking into account that structure (i.e., one overall NA factor made up of sadness, anger, anxiety)

mcfa_between_NA2PA <- '
level: 1

#correlation between all items
PA1 ~~ PA2 + PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA2 ~~ PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA3 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA2 ~~ NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA3 ~~ NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA4 ~~ NA5 + NA6 + NA7 + NA8 + NA9
NA5 ~~ NA6 + NA7 + NA8 + NA9
NA6 ~~ NA7 + NA8 + NA9
NA7 ~~ NA8 + NA9
NA8 ~~ NA9

level: 2

#factor structure
pa =~ PA1 + PA2 + PA3
na1 =~ NA1 + NA2 + NA3 
na2 =~ NA4 + NA5 + NA6 
na3 =~ NA7 + NA8 + NA9
na =~ na1 + na2 + na3

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

'

m3_R_B_RADAR <- sem(model = mcfa_between_NA2PA, data = dataset1_RADAR_processed1, cluster = "ID")
summary(m3_R_B_RADAR, fit.measures =  TRUE, standardized = TRUE)

#Factor loadings are very high (>.85)
#Model fit is good (CFI=1.00,RMSEA=.03)
#some variances are negative (PA2, NA2)

## NA PA structure - fixing negative residual variance of PA2 & NA2 to 0 ##

mcfa_between_NA2PA2 <- '
level: 1

#correlation between all items
PA1 ~~ PA2 + PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA2 ~~ PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA3 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA2 ~~ NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA3 ~~ NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA4 ~~ NA5 + NA6 + NA7 + NA8 + NA9
NA5 ~~ NA6 + NA7 + NA8 + NA9
NA6 ~~ NA7 + NA8 + NA9
NA7 ~~ NA8 + NA9
NA8 ~~ NA9



level: 2

#factor structure
pa =~ PA1 + PA2 + PA3
na1 =~ NA1 + NA2 + NA3 
na2 =~ NA4 + NA5 + NA6 
na3 =~ NA7 + NA8 + NA9
na =~ na1 + na2 + na3

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

PA2 ~~ 0*PA2
NA2 ~~ 0*NA2
'

m4_R_B_RADAR <- sem(model = mcfa_between_NA2PA2, data = dataset1_RADAR_processed1, cluster = "ID")
summary(m4_R_B_RADAR, fit.measures =  TRUE, standardized = TRUE)
#no errors
#Factor loadings are very high (>.85)
#Model fit is good (CFI=1.00,RMSEA=.03)

## compare the two model strategies ## 
#first-order vs. second-order model
#use BIC, can't use chi-square cause models are not nested

BIC(m2_R_B_RADAR) #611171
BIC(m4_R_B_RADAR) #610520.3
BIC(m2_R_B_RADAR)-BIC(m4_R_B_RADAR) #BIC difference of 650.6913 (in favor of second-order model)

#### Within level (between level saturated) ####

## NA PA structure ##

#Model with NA and PA as factor, not taking into account
#the overall emotions (sadness, anxiety, anger, happiness  were each measured with
#3 items, so a second-order factor model might fit better)


mcfa_within_NAPA <- '
level: 1

#factor structure
pa =~ PA1 + PA2 + PA3
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

level: 2

#correlation between all items
PA1 ~~ PA2 + PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA2 ~~ PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA3 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA2 ~~ NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA3 ~~ NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA4 ~~ NA5 + NA6 + NA7 + NA8 + NA9
NA5 ~~ NA6 + NA7 + NA8 + NA9
NA6 ~~ NA7 + NA8 + NA9
NA7 ~~ NA8 + NA9
NA8 ~~ NA9
'

m1_R_W_RADAR <- sem(model = mcfa_within_NAPA, data = dataset1_RADAR_processed1, cluster = "ID")
summary(m1_R_W_RADAR, fit.measures =  TRUE, standardized = TRUE)

#Factorloadings are high (>.59)
#Model fit is not good (CFI = .78, RMSEA=.21)

## NA PA structure with taking emotions into account ##

#In the study, four different emotions were measured with 3 items each. Thus, I am also testing a 
#model taking into account that structure (i.e., one overall NA factor made up of sadness, anger, anxiety)

mcfa_within_NA2PA <- '
level: 1

#factor structure
pa =~ PA1 + PA2 + PA3
na1 =~ NA1 + NA2 + NA3 
na2 =~ NA4 + NA5 + NA6 
na3 =~ NA7 + NA8 + NA9
na =~ na1 + na2 + na3

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

level: 2

#correlation between all items
PA1 ~~ PA2 + PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA2 ~~ PA3 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA3 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA2 ~~ NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA3 ~~ NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA4 ~~ NA5 + NA6 + NA7 + NA8 + NA9
NA5 ~~ NA6 + NA7 + NA8 + NA9
NA6 ~~ NA7 + NA8 + NA9
NA7 ~~ NA8 + NA9
NA8 ~~ NA9

'

m2_R_W_RADAR <- sem(model = mcfa_within_NA2PA, data = dataset1_RADAR_processed1, cluster = "ID")
summary(m2_R_W_RADAR, fit.measures =  TRUE, standardized = TRUE)

#Factorloadings are high (>.64)
#Model fit is good (CFI=.98, RMSEA=.07)


## compare the two model strategies ##
#first-order vs. second-order model
#use BIC, can't use chi-square cause models are not nested

BIC(m1_R_W_RADAR) #653995.8
BIC(m2_R_W_RADAR) #614184.7
BIC(m1_R_W_RADAR)-BIC(m2_R_W_RADAR) #BIC difference of 39811.09 (in favor of second-order model)
```
Overall, the MCFA showed that the factor structure of PA and NA was replicated on both within-level and between-level - but for the within-level only for the model with the second order structure with NA split into sadness, anxiety, and sadness. This makes sense, given that these NA items per emotion are much more alike per emotion and thus modeling it as a second-order model is more logical.



#### Within-and between-person reliabilities

```{r RADAR reliabilities, cache=FALSE}

##################### Depression #####################
# make new dataset into wide format with only depression scores
dataset1_RADAR_dep <- 
  dataset1_RADAR_processed %>%
  select("ID", starts_with('ra11aa', ignore.case = FALSE),
         starts_with('ra61aa', ignore.case = FALSE))

# reshape into wideformat (note: depression is constant, so I summarize across time-points)
dataset1_RADAR_dep_wide<-dataset1_RADAR_dep %>% group_by(ID) %>% summarise_all(mean)


which(colnames(dataset1_RADAR_dep_wide)=="ra11aa02") # dep item 2 - wave 1 (baseline)
which(colnames(dataset1_RADAR_dep_wide)=="ra11aa30") # dep item 30 - wave 1 (baseline)
which(colnames(dataset1_RADAR_dep_wide)=="ra61aa02") # dep item 2 - wave 6 (follow-up)
which(colnames(dataset1_RADAR_dep_wide)=="ra61aa30") # dep item 30 - wave 6 (follow-up)

#### Baseline
alpha_DepB_RADAR<-psych::alpha(dataset1_RADAR_dep_wide[c(2:24)], check.keys=TRUE)
alpha_DepB_RADAR
#### Follow-up
alpha_DepF_RADAR<-psych::alpha(dataset1_RADAR_dep_wide[c(25:47)], check.keys=TRUE)
alpha_DepF_RADAR

##################### Emotions #####################

#### PA ####

### Omega ######
omega_PA_RADAR <- omegaSEM(
  items = c("PA1", "PA2", "PA3"),
  id = "ID",
  data = dataset1_RADAR_processed1,
  savemodel = FALSE)

#### NA ####

### Omega ######
omega_NA_RADAR <- omegaSEM(
  items = c("NA1","NA2", "NA3",
            "NA4","NA5", "NA6",
            "NA7","NA8", "NA9"),
  id = "ID",
  data = dataset1_RADAR_processed1,
  savemodel = FALSE)


###### All results #### ####
omega_PA_RADAR
omega_NA_RADAR


```


### Intra-class correlations


```{r RADAR ICCs, cache=FALSE}
##################### PA #####################
PA_ICC <- lmer(PAff ~ 1 + (1 | ID), data = dataset1_RADAR_processed)
PA_ICCr_RADAR <- performance::icc(PA_ICC)
PA_ICCr_RADAR

##################### NA #####################
NA_ICC <- lmer(NAff ~ 1 + (1 | ID), data = dataset1_RADAR_processed)
NA_ICCr_RADAR <- performance::icc(NA_ICC)
NA_ICCr_RADAR

```

### Exclusion and Compliance


```{r RADAR Exclusion and Compliance, cache=FALSE}

##################### Details study design #####################
#### Total beeps ####
tot_ass_RADAR<-75
#### Total weeks ####
tot_weeks_RADAR <- 15
#### Total beeps per day ####
tot_b_d_RADAR<-1

#### Number of valid beeps before exclusion ####
mean(calc.mean(count, ID, data=dataset1_RADAR_processed)) 

#### Percentage of valid beeps before exclusion ####
dataset1_RADAR_processed$perc<-100*(calc.nomiss(PAff, ID, dataset1_RADAR_processed, prop=TRUE, expand=TRUE))
mean(calc.mean(perc, ID, data=dataset1_RADAR_processed)) 


###### Removing participants with compliance below 33% ####  ####

## one-third of beeps ##
valbeep_RADAR <- (tot_ass_RADAR/100)*33
valbeep_RADAR 

## excluding those with fewer than 33% ##
dataset1_RADAR_processed <-subset(dataset1_RADAR_processed, count > valbeep_RADAR) 



#### Delete participants without any variance on NAff and PAff ####
## n before ##
nsub(ID, dataset1_RADAR_processed)

## deleting ##
dataset1_RADAR_processed <- check.timeinvar(NAff, ID, dataset1_RADAR_processed, out = 3)
dataset1_RADAR_processed <- check.timeinvar(PAff, ID, dataset1_RADAR_processed, out = 3)

## n after ##
nsub(ID, dataset1_RADAR_processed) #no participants excluded

#### final sample ####
n_fin_RADAR<-nsub(ID, dataset1_RADAR_processed)
n_fin_RADAR #number
perc_inc_RADAR <- (100/n_RADAR_orig)*n_fin_RADAR
perc_inc_RADAR #percentage


#### mean of valid beeps per participant ####
mean_comp_RADAR<-mean(calc.mean(count, ID, data=dataset1_RADAR_processed)) 
mean_comp_RADAR 

#### percentage of valid beeps per participant ####
mean_com_per_RADAR<-mean(calc.mean(perc, ID, data=dataset1_RADAR_processed)) 
mean_com_per_RADAR


```




### Calculation of affective dynamics

#### Datapreparation

```{r RADAR affective dynamics dataprep, cache=FALSE}

#####################  sort by ID and time #####################
dataset1_RADAR_processed<-dataset1_RADAR_processed[order(dataset1_RADAR_processed$ID,dataset1_RADAR_processed$time) , ]

##################### subset most important variables #####################
dataset1_RADAR_processed_affdyn <- subset(dataset1_RADAR_processed, select=c("ID","time","week","PAff","NAff"))

##################### delete missing values #####################
#so that differences are also calculated between beeps that were
#not directly next to each other (e.g., 3, 4(NA), 5) 
#I still want to calculate the difference between 3 and 5
dataset1_RADAR_processed_affdyn<-dataset1_RADAR_processed_affdyn[!is.na(dataset1_RADAR_processed_affdyn$PAff),]

##################### make a new time variable #####################
dataset1_RADAR_processed_affdyn<-dataset1_RADAR_processed_affdyn %>% group_by(ID) %>% dplyr::mutate(time = rank(time, ties.method = 'first'))%>% arrange(time)

##################### add missing days after each week ##################### 
#this is important, because for the MSSD, we do not want to calculate the
#difference between the last beep of one day and the first beep of the next day/week
dataset1_RADAR_processed_affdyn_NA <-insert_NA(dataset1_RADAR_processed_affdyn, ID, week)

``` 

#### Calculation

For count, M, SD, MSSD, see <https://quantdev.ssri.psu.edu/tutorials/analysis-experience-sampling-ema-data-chapter-22-univariate-intraindividual-variability>

For autocorrelation, see de Haan-Rietdijk et al. (2016).

```{r RADAR affective dynamics calculation, cache=FALSE}


##################### Calculate affective dynamics #####################
#### Mean, SD ####

aff_dyn_RADAR <- ddply(dataset1_RADAR_processed_affdyn_NA, "ID",  plyr::summarize,
                           icountNA = sum(!is.na(NAff)),  #count of observations
                           imeanNA  = mean(NAff, na.rm=TRUE), #imean (continuous)
                           isdNA   = sd(NAff, na.rm=TRUE), #isd (continuous)
                           icountPA = sum(!is.na(PAff)),  #count of observations
                           imeanPA  = mean(PAff, na.rm=TRUE), #imean (continuous)
                           isdPA    = sd(PAff, na.rm=TRUE),   #isd   (continuous)  
                           relisdNA = relativeSD(NAff, 1, 9), #relative isd (continuous)
                           relisdPA = relativeSD(PAff, 1, 9)) #relative isd (continuous))

#### MSSD ####
mssd.stats.NA <- ddply(dataset1_RADAR_processed_affdyn_NA, "ID", plyr::summarize, iMSSDNA=my.mssd(NAff))
mssd.stats.PA <- ddply(dataset1_RADAR_processed_affdyn_NA, "ID", plyr::summarize, iMSSDPA=my.mssd(PAff))

aff_dyn_RADAR <- merge(aff_dyn_RADAR,mssd.stats.NA,by="ID",all=TRUE)
aff_dyn_RADAR <- merge(aff_dyn_RADAR,mssd.stats.PA,by="ID",all=TRUE)


#### autocorrelation ####

## Lagged effect ##
dataset1_RADAR_processed_affdyn$NAff_lag<-lagvar(NAff, id=ID, obs=time, day=week,data=dataset1_RADAR_processed_affdyn)
dataset1_RADAR_processed_affdyn$PAff_lag<-lagvar(PAff, id=ID, obs=time, day=week,data=dataset1_RADAR_processed_affdyn)

## Mixed model ##
auto_NA <- lmer(NAff ~ 1 + NAff_lag + (1 + NAff_lag | ID), 
                     data = dataset1_RADAR_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_NA)

auto_PA <- lmer(PAff ~ 1 + PAff_lag + (1 + PAff_lag | ID), 
                     data = dataset1_RADAR_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_PA)

## Extract individual slopes ##
beta_NA<-tibble::rownames_to_column(coef(auto_NA)$ID)
beta_NA<-plyr::rename(beta_NA, 
                c("NAff_lag" = "autocorNA",
                  "rowname" =  "ID"))

beta_PA<-tibble::rownames_to_column(coef(auto_PA)$ID)
beta_PA<-plyr::rename(beta_PA, 
                c("PAff_lag" = "autocorPA",
                  "rowname" =  "ID"))


##################### Merge data #####################
aff_dyn_RADAR <- merge(aff_dyn_RADAR,beta_NA,by="ID",all=TRUE)
aff_dyn_RADAR <- merge(aff_dyn_RADAR,beta_PA,by="ID",all=TRUE)

##################### Delete intercept from aff_dyn files #####################
aff_dyn_RADAR<-subset(aff_dyn_RADAR, select=c(1:11,13,15))
names(aff_dyn_RADAR)

```

### Merging

```{r RADAR merging observations, cache=FALSE}

##################### restructure depression/demographics data #####################
#The dataset was in longformat, however the affective dynamics measures are in wide format
#Thus, I restructure the dataset (which only contain time-invariant variables anyway)
#and merge with the affective dynamics data
data_id_dep_RADAR <- subset(dataset1_RADAR_processed, select=c("ID","Sex","Age","DepB","DepF"))
data_id_dep_RADAR<-data_id_dep_RADAR %>% group_by(ID) %>% summarise_all(mean)

##################### Merge affective dynamics and depression/demographics data #####################
datasetRADAR_final <- merge(data_id_dep_RADAR,aff_dyn_RADAR,by="ID",all=TRUE)
names(datasetRADAR_final)


```


### Winzorising and transformation of skewed variables

```{r RADAR transformation + winsorizing, cache=FALSE}

##################### winsorizing #####################
names(datasetRADAR_final)

datasetRADAR_final<-datasetRADAR_final[order(datasetRADAR_final$ID) , ]

psych::describe(datasetRADAR_final)

dataRADAR_finalW<-data.frame(datasetRADAR_final)

#select predictors and outcome variables (affective dynamics and depression)
predictor_wins<-c(4,5,7,8,10:17) 

#winsorizing
dataRADAR_finalW<-apply(dataRADAR_finalW[,predictor_wins], 
                                 2,  
                                 Winsorize, probs=c(0.01,0.99), na.rm = TRUE)



dataRADAR_finalW<-data.frame(dataRADAR_finalW)

psych::describe(dataRADAR_finalW)               

#add id number
dataRADAR_finalW$ID <- datasetRADAR_final$ID

#make subset of variables that were not included in the winsorizing
sub1<-subset(datasetRADAR_final, select=c(1:3,6,9))

#Merge files
dataRADAR_finalW <- merge(dataRADAR_finalW,sub1,by="ID",all=TRUE)

##################### Transforming ######################
#if skewness is > 3 (Kline, 2011)

#### identifying ####
skew<-skew(select(dataRADAR_finalW, -ID))>3
skew

length(skew[skew== TRUE]) #no transformations necessary

names(dataRADAR_finalW)
```

### Center/standardize study variables

- Center of predictors (for moderation)
- Standardization of the depression scale

```{r RADAR standardize variables, cache=FALSE}

names(dataRADAR_finalW)

psych::describe(dataRADAR_finalW)

##################### center predictors #####################
predictor_scale<-c(4:13)

dataRADAR_finalWC<-apply(dataRADAR_finalW[,predictor_scale], 
                           2,  
                           scale, scale = FALSE)

dataRADAR_finalWC<-data.frame(dataRADAR_finalWC)

psych::describe(dataRADAR_finalWC)               

#add id number
dataRADAR_finalWC$ID<- dataRADAR_finalW$ID

#make subset of variables that were not included in the standardization
sub2<-subset(dataRADAR_finalW, select=c(1:3,14:17))

#####################Merge files #####################
dataRADAR_finalWC <- merge(dataRADAR_finalWC,sub2,by="ID",all=TRUE)


##################### standardize Depression #####################
dataRADAR_finalWC$DepB<-scale(dataRADAR_finalWC$DepB, center = TRUE, scale = TRUE)
dataRADAR_finalWC$DepF<-scale(dataRADAR_finalWC$DepF, center = TRUE, scale = TRUE)
psych::describe(dataRADAR_finalWC)               


names(dataRADAR_finalWC)

```
### Write cleaned datasets 

The "raw" (i.e., not centered & standardized) and centered/standardized versions.

```{r RADAR write datasets, cache=FALSE}
write.csv(dataRADAR_finalW, here::here("data", "3_cleaned_data_for_OSF", "dataset1_RADAR_clean.csv"), row.names = FALSE)
write.csv(dataRADAR_finalWC, here::here("data", "3_cleaned_data_for_OSF", "dataset1_RADAR_clean_cen_std.csv"), row.names = FALSE)

```

## DATASET 2 - SWINGING MOODS

### Read in data and datapreparation

```{r Swinging Moods read in data, cache=FALSE}

####################### Read in data #######################
dataset2_SM_processed<-read.csv(here::here("data", "2_processed_data_for_OSF", "dataset2_SM_processed.csv"), sep=",",
                                   na.strings="NA")

####################### number of participants with valid ESM data ##################### 
dataset2_SM_processed$count<-calc.nomiss(PAff, ID, dataset2_SM_processed, prop=FALSE, expand=TRUE)
range(dataset2_SM_processed$count)

# exclude those with not valid daily diary data
n_SM_orig<-nsub(ID, dataset2_SM_processed)
n_SM_orig

dataset2_SM_processed <-subset(dataset2_SM_processed, count > 0) 
range(dataset2_SM_processed$count)

n_SM_orig<-nsub(ID, dataset2_SM_processed)
n_SM_orig




``` 


### Psychometrics 
#### Multilevel Confirmatory Factor Analysis

```{r Swinging Moods MFCA, cache=FALSE}

##################### delete observations with 0 variance #####################

dataset2_SM_processed1<-dataset2_SM_processed
dataset2_SM_processed1 <- check.timeinvar(PA1, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(PA2, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(PA3, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(PA4, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(PA5, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(PA6, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(NA1, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(NA2, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(NA3, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(NA4, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(NA5, ID, dataset2_SM_processed1, out = 3)
dataset2_SM_processed1 <- check.timeinvar(NA6, ID, dataset2_SM_processed1, out = 3)

##################### MCFA ######################
#### Between-person (within level saturated) ####

mcfa_between_NAPA <- '
level: 1

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA2 ~~ PA3 + PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA3 ~~ PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA4 ~~ PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA5 ~~ PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA6 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 
NA2 ~~ NA3 + NA4 + NA5 + NA6 
NA3 ~~ NA4 + NA5 + NA6 
NA4 ~~ NA5 + NA6 
NA5 ~~ NA6 

level: 2

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

'

m1_R_B_SM <- sem(model = mcfa_between_NAPA, data = dataset2_SM_processed1, cluster = "ID")
summary(m1_R_B_SM, fit.measures=TRUE, standardized = TRUE)

#Factor loadings are very high (>.70)
#Model fit is good (CFI=1.00, RMSEA=.02)

#### Within-person (between-level saturated) ####

mcfa_within_NAPA <- '
level: 1

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

level: 2

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA2 ~~ PA3 + PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA3 ~~ PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA4 ~~ PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA5 ~~ PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA6 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 
NA2 ~~ NA3 + NA4 + NA5 + NA6 
NA3 ~~ NA4 + NA5 + NA6 
NA4 ~~ NA5 + NA6 
NA5 ~~ NA6 
'

m1_R_W_SM <- sem(model = mcfa_within_NAPA, data = dataset2_SM_processed1, cluster = "ID")
summary(m1_R_W_SM, fit.measures=TRUE, standardized = TRUE)

#Factor loadings are high (>.44)
#Model fit is acceptable (CFI=.94, RMSEA=.06)

``` 
Overall, the MCFA showed that the factor structure of PA and NA was replicated on both within-level and between-level.


#### Within-and between-person reliabilities

```{r Swinging Moods reliabilities, cache=FALSE}

# make new dataset into wide format with only depression scores
dataset2_SM_dep <- 
  dataset2_SM_processed %>%
  select("ID", starts_with('dep_', ignore.case = FALSE),
         starts_with('Fdep', ignore.case = FALSE))

# reshape into wideformat (note: depression is constant, so I summarize across time-points)
dataset2_SM_dep_wide<-dataset2_SM_dep %>% group_by(ID) %>% summarise_all(mean)

names(dataset2_SM_dep_wide)

##################### Depression #####################
which(colnames(dataset2_SM_dep_wide)=="dep_1") # dep item 1 - baseline
which(colnames(dataset2_SM_dep_wide)=="dep_20") # dep item 20 - baseline
which(colnames(dataset2_SM_dep_wide)=="Fdep1") # dep item 1 - follow-up
which(colnames(dataset2_SM_dep_wide)=="Fdep20") # dep item 20 - follow-up

#### Baseline ####
alpha_DepB_SM<-psych::alpha(dataset2_SM_dep_wide[c(2:21)], check.keys=TRUE)
alpha_DepB_SM

#### Follow-up ####
alpha_DepF_SM<-psych::alpha(dataset2_SM_dep_wide[c(26:45)], check.keys=TRUE)
alpha_DepF_SM

#negative items were correctly recoded by alpha function

##################### Emotions #####################

#### PA ####

### Omega ######
omega_PA_SM <- omegaSEM(
  items = c("PA1", "PA2", "PA3", "PA4", "PA5", "PA6"),
  id = "ID",
  data = dataset2_SM_processed1,
  savemodel = FALSE)
 
#### NA ####

### Omega ######
omega_NA_SM <- omegaSEM(
  items = c("NA1","NA2", "NA3",
            "NA4","NA5", "NA6"),
  id = "ID",
  data = dataset2_SM_processed1,
  savemodel = FALSE)

###### All results #### ####
omega_PA_SM
omega_NA_SM

```



### Intra-class correlations


```{r Swinging Moods ICCs, cache=FALSE}
##################### PA #####################
PA_ICC <- lmer(PAff ~ 1 + (1 | ID), data = dataset2_SM_processed)
PA_ICCr_SM <- performance::icc(PA_ICC)
PA_ICCr_SM

##################### NA #####################
NA_ICC <- lmer(NAff ~ 1 + (1 | ID), data = dataset2_SM_processed)
NA_ICCr_SM <- performance::icc(NA_ICC)
NA_ICCr_SM

```

### Exclusion and Compliance
```{r Swinging Moods Exclusion and Compliance, cache=FALSE}

##################### Details study design #####################
#### Total beeps ####
tot_ass_SM<-54 
#### Total days #### 
tot_days_SM <- 6
#### Total beeps per day #### 
tot_b_d_SM<-9

#### Number of valid beeps before exclusion ####
mean(calc.mean(count, ID, data=dataset2_SM_processed)) 

#### Percentage of valid beeps before exclusion #### 
dataset2_SM_processed$perc<-(100/tot_ass_SM)*dataset2_SM_processed$count
mean(calc.mean(perc, ID, data=dataset2_SM_processed)) 

#### Number of participants before exclusion ####
n_SM<-nsub(ID, dataset2_SM_processed)
n_SM

###### Removing participants with compliance below 33% ####  #### 

## one-third of beeps ##
valbeep_SM <- (tot_ass_SM/100)*33
valbeep_SM

## excluding those with fewer than 33% ##
dataset2_SM_processed <-subset(dataset2_SM_processed, count > valbeep_SM) 

#### Delete participants without any variance on NAff and PAff ####
## n before ##
nsub(ID, dataset2_SM_processed)

## deleting ##
dataset2_SM_processed <- check.timeinvar(NAff, ID, dataset2_SM_processed, out = 3)
dataset2_SM_processed <- check.timeinvar(PAff, ID, dataset2_SM_processed, out = 3)

## n after ##
nsub(ID, dataset2_SM_processed) #2 participants excluded


#### final sample ####
n_fin_SM<-nsub(ID, dataset2_SM_processed)
n_fin_SM
perc_inc_SM <- (100/n_SM_orig)*n_fin_SM
perc_inc_SM



#### mean of valid beeps per participant ####
mean_comp_SM<-mean(calc.mean(count, ID, data=dataset2_SM_processed))  
mean_comp_SM 

#### percentage of valid beeps per participant ####
mean_com_per_SM<-mean(calc.mean(perc, ID, data=dataset2_SM_processed)) 
mean_com_per_SM



```

### Calculation of affective dynamics

#### Datapreparation

```{r Swinging Moods affective dynamics dataprep, cache=FALSE}

##################### Deleting rows with missings on days #####################
#beeps were not always all sent out for all days.  
#(later no NA's are allowed on days for the lagging of variables for the autocorrelation)
dataset2_SM_processed<-dataset2_SM_processed[!is.na(dataset2_SM_processed$Day),]

##################### sort by ID and time #####################
dataset2_SM_processed<-dataset2_SM_processed[order(dataset2_SM_processed$ID,dataset2_SM_processed$time) , ]

##################### subset most important variables #####################
dataset2_SM_processed_affdyn <- subset(dataset2_SM_processed, select=c("ID","time","Day","PAff","NAff"))

##################### delete missing values #####################
#Note that Swinging Moods did not have information on missing data in between beeps.
dataset2_SM_processed_affdyn<-dataset2_SM_processed_affdyn[!is.na(dataset2_SM_processed_affdyn$PAff),]

##################### make a new time variable #####################
dataset2_SM_processed_affdyn<-dataset2_SM_processed_affdyn %>% group_by(ID) %>% dplyr::mutate(time = rank(time, ties.method = 'first'))%>% arrange(time)

##################### add missing days after each week ##################### 
#this is important, because for the MSSD, we do not want to calculate the
#difference between the last beep of one day and the first beep of the next day/week

dataset2_SM_processed_affdyn_NA <-insert_NA(dataset2_SM_processed_affdyn, ID, Day)

```

#### Calculation 
```{r Swinging Moods affective dynamics calc, cache=FALSE}

##################### Calculate affective dynamics #####################
#### Mean, SD ####

aff_dyn_SM <- ddply(dataset2_SM_processed_affdyn_NA, "ID",  plyr::summarize,
                           icountNA = sum(!is.na(NAff)),  #count of observations
                           imeanNA  = mean(NAff, na.rm=TRUE), #imean (continuous)
                           isdNA   = sd(NAff, na.rm=TRUE), #isd (continuous)
                           icountPA = sum(!is.na(PAff)),  #count of observations
                           imeanPA  = mean(PAff, na.rm=TRUE), #imean (continuous)
                           isdPA    = sd(PAff, na.rm=TRUE),   #isd   (continuous)  
                           relisdNA = relativeSD(NAff, 1, 9), #relative isd (continuous)
                           relisdPA = relativeSD(PAff, 1, 9)) #relative isd (continuous))
#### MSSD ####

mssd.stats.NA <- ddply(dataset2_SM_processed_affdyn_NA, "ID", plyr::summarize, iMSSDNA=my.mssd(NAff))
mssd.stats.PA <- ddply(dataset2_SM_processed_affdyn_NA, "ID", plyr::summarize, iMSSDPA=my.mssd(PAff))

aff_dyn_SM <- merge(aff_dyn_SM,mssd.stats.NA,by="ID",all=TRUE)
aff_dyn_SM <- merge(aff_dyn_SM,mssd.stats.PA,by="ID",all=TRUE)


#### autocorrelation ####

## Lagged effect ##
dataset2_SM_processed_affdyn$NAff_lag<-lagvar(NAff, id=ID, obs=time, day=Day,data=dataset2_SM_processed_affdyn)
dataset2_SM_processed_affdyn$PAff_lag<-lagvar(PAff, id=ID, obs=time, day=Day,data=dataset2_SM_processed_affdyn)

## Mixed model ##
auto_NA <- lmer(NAff ~ 1 + NAff_lag + (1 + NAff_lag | ID), 
                     data = dataset2_SM_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_NA)

auto_PA <- lmer(PAff ~ 1 + PAff_lag + (1 + PAff_lag | ID), 
                     data = dataset2_SM_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_PA)

## Extract individual slopes ##
beta_NA<-tibble::rownames_to_column(coef(auto_NA)$ID)
beta_NA<-plyr::rename(beta_NA, 
                c("NAff_lag" = "autocorNA",
                  "rowname" =  "ID"))

beta_PA<-tibble::rownames_to_column(coef(auto_PA)$ID)
beta_PA<-plyr::rename(beta_PA, 
                c("PAff_lag" = "autocorPA",
                  "rowname" =  "ID"))

##################### Merge data #####################
aff_dyn_SM <- merge(aff_dyn_SM,beta_NA,by="ID",all=TRUE)
aff_dyn_SM <- merge(aff_dyn_SM,beta_PA,by="ID",all=TRUE)

##################### Delete intercept from aff_dyn files #####################
aff_dyn_SM<-subset(aff_dyn_SM, select=c(1:11,13,15))
names(aff_dyn_SM)
```

### Merging

```{r Swinging Moods merging, cache=FALSE}

##################### restructure depression/demographics data #####################
#The dataset was in longformat, however the affective dynamics measures are in wide format
#Thus, I restructure the dataset (which only contain time-invariant variables anyway)
#and merge with the affective dynamics data
data_id_dep_SM <- subset(dataset2_SM_processed, select=c("ID","Sex","Age","DepB","DepF"))
data_id_dep_SM<-data_id_dep_SM %>% group_by(ID) %>% summarise_all(mean)


##################### Merge affective dynamics and depression/demographics data #####################
datasetSM_final <- merge(data_id_dep_SM,aff_dyn_SM,by="ID",all=TRUE)
names(datasetSM_final)

```


### Winzorising and transformation of skewed variables

```{r Swinging Moods transformation + winsorizing, cache=FALSE}

##################### winsorizing #####################
names(datasetSM_final)

datasetSM_final<-datasetSM_final[order(datasetSM_final$ID) , ]

psych::describe(datasetSM_final)

dataSM_finalW<-data.frame(datasetSM_final)

#select predictors and outcome variables (affective dynamics and depression)
predictor_wins<-c(4,5,7,8,10:17) 

#winsorizing
dataSM_finalW<-apply(dataSM_finalW[,predictor_wins], 
                                 2,  
                                 Winsorize, probs=c(0.01,0.99), na.rm = TRUE)



dataSM_finalW<-data.frame(dataSM_finalW)

psych::describe(dataSM_finalW)               

#add id number
dataSM_finalW$ID <- datasetSM_final$ID

#make subset of variables that were not included in the winsorizing
sub1<-subset(datasetSM_final, select=c(1:3,6,9))

#Merge files
dataSM_finalW <- merge(dataSM_finalW,sub1,by="ID",all=TRUE)

##################### Transforming ######################
#if skewness is > 3 (Kline, 2011)

#### identifying ####
skew<-skew(select(dataSM_finalW, -ID))>3
skew

length(skew[skew== TRUE])

#no transformations necessary

names(dataSM_finalW)


```


### Center/standardize study variables

- Center of predictors (for moderation)
- Standardization of the depression scale

```{r Swinging Moods center predictors, cache=FALSE}

names(dataSM_finalW)

psych::describe(dataSM_finalW)

##################### center predictors #####################
predictor_scale<-c(4:13)

dataSM_finalWC<-apply(dataSM_finalW[,predictor_scale], 
                           2,  
                           scale, scale = FALSE)

dataSM_finalWC<-data.frame(dataSM_finalWC)

psych::describe(dataSM_finalWC)               

#add id number
dataSM_finalWC$ID<- dataSM_finalW$ID

#make subset of variables that were not included in the standardization
sub2<-subset(dataSM_finalW, select=c(1:3,14:17))

#Merge files
dataSM_finalWC <- merge(dataSM_finalWC,sub2,by="ID",all=TRUE)

##################### standardize Depression #####################
dataSM_finalWC$DepB<-scale(dataSM_finalWC$DepB, center = TRUE, scale = TRUE)
dataSM_finalWC$DepF<-scale(dataSM_finalWC$DepF, center = TRUE, scale = TRUE)
psych::describe(dataSM_finalWC)

names(dataSM_finalWC)


```

### Write cleaned datasets 

The "raw" (i.e., not centered & standardized) and centered/standardized versions.

```{r Swinging Moods write datasets, cache=FALSE}
write.csv(dataSM_finalW, here::here("data", "3_cleaned_data_for_OSF", "dataset2_SM_clean.csv"), row.names = FALSE)
write.csv(dataSM_finalWC, here::here("data", "3_cleaned_data_for_OSF", "dataset2_SM_clean_cen_std.csv"), row.names = FALSE)

```

## DATASET 3 - MOOD IN EMERGING ADULTS

### Read in data and datapreparation
```{r Mood in Emerging Adults read in data, cache=FALSE}

####################### Read in data #######################
dataset3_MA_processed<-read.csv(here::here("data", "2_processed_data_for_OSF", "dataset3_MA_processed.csv"), sep=",",
                                na.strings="NA")

####################### number of participants with valid ESM data ##################### 
dataset3_MA_processed$count<-calc.nomiss(PAff, ID, dataset3_MA_processed, prop=FALSE, expand=TRUE)
range(dataset3_MA_processed$count)

# exclude those with not valid daily diary data
n_MA_orig<-nsub(ID, dataset3_MA_processed)
n_MA_orig

dataset3_MA_processed <-subset(dataset3_MA_processed, count > 0) 
range(dataset3_MA_processed$count)

n_MA_orig<-nsub(ID, dataset3_MA_processed)
n_MA_orig


```


### Psychometrics
#### Multilevel Confirmatory Factor Analysis


```{r Mood in Emerging Adults MFCA, cache=FALSE}

##################### delete observations with 0 variance #####################

dataset3_MA_processed1<-dataset3_MA_processed
dataset3_MA_processed1 <- check.timeinvar(PA1, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(PA2, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(PA3, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(PA4, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(PA5, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(PA6, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(NA1, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(NA2, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(NA3, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(NA4, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(NA5, ID, dataset3_MA_processed1, out = 3)
dataset3_MA_processed1 <- check.timeinvar(NA6, ID, dataset3_MA_processed1, out = 3)

##################### MCFA ######################
#### Between-person (within level saturated) ####

mcfa_between_NAPA <- '
level: 1

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA2 ~~ PA3 + PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA3 ~~ PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA4 ~~ PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA5 ~~ PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA6 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 
NA2 ~~ NA3 + NA4 + NA5 + NA6 
NA3 ~~ NA4 + NA5 + NA6 
NA4 ~~ NA5 + NA6 
NA5 ~~ NA6 

level: 2

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

'

m1_R_B_MA <- sem(model = mcfa_between_NAPA, data = dataset3_MA_processed1, cluster = "ID")
summary(m1_R_B_MA, fit.measures=TRUE, standardized = TRUE)

#Factor loadings are very high (>.68)
#Model fit is good (CFI=1.00, RMSEA=.02)

#### Within-person (between-level saturated) ####

mcfa_within_NAPA <- '
level: 1

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

level: 2

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA2 ~~ PA3 + PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA3 ~~ PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA4 ~~ PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA5 ~~ PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA6 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 
NA2 ~~ NA3 + NA4 + NA5 + NA6 
NA3 ~~ NA4 + NA5 + NA6 
NA4 ~~ NA5 + NA6 
NA5 ~~ NA6 
'

m1_R_W_MA <- sem(model = mcfa_within_NAPA, data = dataset3_MA_processed1, cluster = "ID")
summary(m1_R_W_MA, fit.measures=TRUE, standardized = TRUE)

#Factor loadings are high (>.42)
#Model fit is acceptable (CFI=.94, RMSEA=.07)


``` 
Overall, the MCFA showed that the factor structure of PA and NA was replicated on both within-level and between-level. 



#### Within-and between-person reliabilities

```{r Mood in Emerging Adults reliabilities, cache=FALSE}

# make new dataset into wide format with only depression scores
dataset3_MA_dep <- 
  dataset3_MA_processed %>%
  select("ID", starts_with('cesd', ignore.case = FALSE),
         starts_with('Fdep', ignore.case = FALSE))

# reshape into wideformat (note: depression is constant, so I summarize across time-points)
dataset3_MA_dep_wide<-dataset3_MA_dep %>% group_by(ID) %>% summarise_all(mean)

names(dataset3_MA_dep_wide)

##################### Depression #####################

which(colnames(dataset3_MA_dep_wide)=="cesd1") # dep item 1 - baseline
which(colnames(dataset3_MA_dep_wide)=="cesd20") # dep item 20 - baseline
which(colnames(dataset3_MA_dep_wide)=="Fdep1") # dep item 1 - follow-up
which(colnames(dataset3_MA_dep_wide)=="Fdep20") # dep item 20 - follow-up

#### Baseline ####
alpha_DepB_MA<-psych::alpha(dataset3_MA_dep_wide[c(2:21)], check.keys=TRUE)
alpha_DepB_MA
#### Follow-up #### 
alpha_DepF_MA<-psych::alpha(dataset3_MA_dep_wide[c(26:45)], check.keys=TRUE)
alpha_DepF_MA

#negative items were correctly recoded by alpha function

##################### Emotions #####################

################## PA ##################

### Omega ###
omega_PA_MA <- omegaSEM(
  items = c("PA1", "PA2", "PA3", 
            "PA4", "PA5", "PA6"),
  id = "ID",
  data = dataset3_MA_processed1,
  savemodel = FALSE)


################## NA ##################

### Omega ###
omega_NA_MA <- omegaSEM(
  items = c("NA1","NA2", "NA3",
            "NA4","NA5", "NA6"),
  id = "ID",
  data = dataset3_MA_processed1,
  savemodel = FALSE)


#### All results ####
omega_PA_MA
omega_NA_MA

```

### Intra-class correlations


```{r Mood in Emerging Adults ICCs, cache=FALSE}
##################### PA #####################
PA_ICC <- lmer(PAff ~ 1 + (1 | ID), data = dataset3_MA_processed)
PA_ICCr_MA <- performance::icc(PA_ICC)
PA_ICCr_MA

##################### NA #####################
NA_ICC <- lmer(NAff ~ 1 + (1 | ID), data = dataset3_MA_processed)
NA_ICCr_MA <- performance::icc(NA_ICC)
NA_ICCr_MA

```

### Exclusion and Compliance
```{r Mood in Emerging Adults Exclusion and Compliance, cache=FALSE}
##################### Details study design #####################
#### Total beeps ####
tot_ass_MA<-55
#### Total days ####
tot_days_MA <-11 
#### Total beeps per day ####
tot_b_d_MA<-5

#### Number of valid beeps before exclusion ####
mean(calc.mean(count, ID, data=dataset3_MA_processed)) 

#### Percentage of valid beeps before exclusion #### 
dataset3_MA_processed$perc<-(100/tot_ass_MA)*dataset3_MA_processed$count
mean(calc.mean(perc, ID, data=dataset3_MA_processed)) 

#### Number of participants before exclusion ####
n_MA<-nsub(ID, dataset3_MA_processed)
n_MA

#### Removing participants with compliance below 33% #### 

## one-third of beeps ##
valbeep_MA <- (tot_ass_MA/100)*33
valbeep_MA

## excluding those with fewer than 33% ##
dataset3_MA_processed <-subset(dataset3_MA_processed, count > valbeep_MA) 

#### Delete participants without any variance on NAff and PAff ###
## n before ##
nsub(ID, dataset3_MA_processed)

#### Delete participants without any variance on NAff and PAff ####
dataset3_MA_processed <- check.timeinvar(NAff, ID, dataset3_MA_processed, out = 3)
dataset3_MA_processed <- check.timeinvar(PAff, ID, dataset3_MA_processed, out = 3)

## n after ##
nsub(ID, dataset3_MA_processed) #no participants excluded


#### final sample ####
n_fin_MA<-nsub(ID, dataset3_MA_processed)
n_fin_MA
perc_inc_MA <- (100/n_MA_orig)*n_fin_MA
perc_inc_MA



#### mean of valid beeps per participant ####
mean_comp_MA<-mean(calc.mean(count, ID, data=dataset3_MA_processed)) 
mean_comp_MA 

#### percentage of valid beeps per participant ####
mean_com_per_MA<-mean(calc.mean(perc, ID, data=dataset3_MA_processed)) 
mean_com_per_MA




```


### Calculation of affective dynamics

#### Datapreparation

```{r Mood in Emerging Adults affective dynamics dataprep, cache=FALSE}
##################### Deleting rows with missings on days ##################### 
#beeps were not always all sent out for all days.   
#(later no NA's are allowed on days for the lagging of variables for the autocorrelation)
dataset3_MA_processed<-dataset3_MA_processed[!is.na(dataset3_MA_processed$Day),]

##################### sort by ID and time #####################
dataset3_MA_processed<-dataset3_MA_processed[order(dataset3_MA_processed$ID,dataset3_MA_processed$time) , ]

##################### subset most important variables #####################
dataset3_MA_processed_affdyn <- subset(dataset3_MA_processed, select=c("ID","time","Day","PAff","NAff"))

##################### delete missing values ##################### 
#Note that Mood in Emerging Adults did not have information on missing data in between beeps.
dataset3_MA_processed_affdyn<-dataset3_MA_processed_affdyn[!is.na(dataset3_MA_processed_affdyn$PAff),]

##################### make a new time variable #####################
dataset3_MA_processed_affdyn<-dataset3_MA_processed_affdyn %>% group_by(ID) %>% dplyr::mutate(time = rank(time, ties.method = 'first'))%>% arrange(time)

##################### add missing days after each week ##################### 
#this is important, because for the MSSD, we do not want to calculate the
#difference between the last beep of one day and the first beep of the next day/week
dataset3_MA_processed_affdyn_NA <-insert_NA(dataset3_MA_processed_affdyn, ID, Day)



```

#### Calculation 
```{r Mood in Emerging Adults affective dynamics calc, cache=FALSE}


##################### Calculate affective dynamics #####################
#### Mean, SD ####


aff_dyn_MA <- ddply(dataset3_MA_processed_affdyn_NA, "ID",  plyr::summarize,
                           icountNA = sum(!is.na(NAff)),  #count of observations
                           imeanNA  = mean(NAff, na.rm=TRUE), #imean (continuous)
                           isdNA   = sd(NAff, na.rm=TRUE), #isd (continuous)
                           icountPA = sum(!is.na(PAff)),  #count of observations
                           imeanPA  = mean(PAff, na.rm=TRUE), #imean (continuous)
                           isdPA    = sd(PAff, na.rm=TRUE),   #isd   (continuous)  
                           relisdNA = relativeSD(NAff, 1, 9), #relative isd (continuous)
                           relisdPA = relativeSD(PAff, 1, 9)) #relative isd (continuous))

#### MSSD ####

mssd.stats.NA <- ddply(dataset3_MA_processed_affdyn_NA, "ID", plyr::summarize, iMSSDNA=my.mssd(NAff))
mssd.stats.PA <- ddply(dataset3_MA_processed_affdyn_NA, "ID", plyr::summarize, iMSSDPA=my.mssd(PAff))

aff_dyn_MA <- merge(aff_dyn_MA,mssd.stats.NA,by="ID",all=TRUE)
aff_dyn_MA <- merge(aff_dyn_MA,mssd.stats.PA,by="ID",all=TRUE)


#### autocorrelation ####

## Lagged effect ##

dataset3_MA_processed_affdyn$NAff_lag<-lagvar(NAff, id=ID, obs=time, day=Day,data=dataset3_MA_processed_affdyn)
dataset3_MA_processed_affdyn$PAff_lag<-lagvar(PAff, id=ID, obs=time, day=Day,data=dataset3_MA_processed_affdyn)

## Mixed model ##
auto_NA <- lmer(NAff ~ 1 + NAff_lag + (1 + NAff_lag | ID), 
                     data = dataset3_MA_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_NA)

auto_PA <- lmer(PAff ~ 1 + PAff_lag + (1 + PAff_lag | ID), 
                     data = dataset3_MA_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_PA)

## Extract individual slopes ##
beta_NA<-tibble::rownames_to_column(coef(auto_NA)$ID)
beta_NA<-plyr::rename(beta_NA, 
                c("NAff_lag" = "autocorNA",
                  "rowname" =  "ID"))

beta_PA<-tibble::rownames_to_column(coef(auto_PA)$ID)
beta_PA<-plyr::rename(beta_PA, 
                c("PAff_lag" = "autocorPA",
                  "rowname" =  "ID"))

##################### Merge data #####################
aff_dyn_MA <- merge(aff_dyn_MA,beta_NA,by="ID",all=TRUE)
aff_dyn_MA <- merge(aff_dyn_MA,beta_PA,by="ID",all=TRUE)

##################### Delete intercept from aff_dyn files #####################
aff_dyn_MA<-subset(aff_dyn_MA, select=c(1:11,13,15))
names(aff_dyn_MA)

```


### Merging

```{r Mood in Emerging Adults merging, cache=FALSE}

##################### restructure depression/demographics data #####################
#The dataset was in longformat, however the affective dynamics measures are in wide format
#Thus, I restructure the dataset (which only contain time-invariant variables anyway)
#and merge with the affective dynamics data
data_id_dep_MA <- subset(dataset3_MA_processed, select=c("ID","Sex","Age","DepB","DepF"))
data_id_dep_MA<-data_id_dep_MA %>% group_by(ID) %>% summarise_all(mean)


##################### Merge affective dynamics and depression/demographics data #####################
datasetMA_final <- merge(data_id_dep_MA,aff_dyn_MA,by="ID",all=TRUE)
names(datasetMA_final)

```

### Winzorising and transformation of skewed variables

```{r Mood in Emerging Adults transformation + winsorizing, cache=FALSE}

##################### winsorizing #####################
names(datasetMA_final)

datasetMA_final<-datasetMA_final[order(datasetMA_final$ID) , ]

psych::describe(datasetMA_final)

dataMA_finalW<-data.frame(datasetMA_final)

#select predictors and outcome variables (affective dynamics and depression)
predictor_wins<-c(4,5,7,8,10:17)

#winsorizing
dataMA_finalW<-apply(dataMA_finalW[,predictor_wins], 
                                 2,  
                                 Winsorize, probs=c(0.01,0.99), na.rm = TRUE)



dataMA_finalW<-data.frame(dataMA_finalW)

psych::describe(dataMA_finalW)               

#add id number
dataMA_finalW$ID <- datasetMA_final$ID

#make subset of variables that were not included in the winsorizing
sub1<-subset(datasetMA_final, select=c(1:3,6,9))

#Merge files
dataMA_finalW <- merge(dataMA_finalW,sub1,by="ID",all=TRUE)

##################### Transforming ###################### 
#if skewness is > 3 (Kline, 2011)
#### identifying ####
skew<-skew(select(dataMA_finalW, -ID))>3
skew

length(skew[skew== TRUE])

#no transformations necessary

names(dataMA_finalW)



```


### Center/standardize study variables

- Center of predictors (for moderation)
- Standardization of the depression scale

```{r Mood in Emerging Adults center predictors, cache=FALSE}

names(dataMA_finalW)

psych::describe(dataMA_finalW)

##################### center predictors #####################
predictor_scale<-c(4:13)

dataMA_finalWC<-apply(dataMA_finalW[,predictor_scale], 
                           2,  
                           scale, scale = FALSE)

dataMA_finalWC<-data.frame(dataMA_finalWC)

psych::describe(dataMA_finalWC)               

#add id number
dataMA_finalWC$ID<- dataMA_finalW$ID

#make subset of variables that were not included in the standardization
sub2<-subset(dataMA_finalW, select=c(1:3,14:17))

#Merge files
dataMA_finalWC <- merge(dataMA_finalWC,sub2,by="ID",all=TRUE)

##################### standardize Depression #####################
dataMA_finalWC$DepB<-scale(dataMA_finalWC$DepB, center = TRUE, scale = TRUE)
dataMA_finalWC$DepF<-scale(dataMA_finalWC$DepF, center = TRUE, scale = TRUE)
psych::describe(dataMA_finalWC)

names(dataMA_finalWC)

```


### Write cleaned datasets 

The "raw" (i.e., not centered & standardized) and centered/standardized versions.

```{r Mood in EMerging Adults write datasets, cache=FALSE}
write.csv(dataMA_finalW, here::here("data", "3_cleaned_data_for_OSF", "dataset3_MA_clean.csv"), row.names = FALSE)
write.csv(dataMA_finalWC, here::here("data", "3_cleaned_data_for_OSF", "dataset3_MA_clean_cen_std.csv"), row.names = FALSE)

```
## DATASET 4: EMOTIONS IN DAILY LIFE

### Read in data and datapreparation

```{r Emotions in daily life read in data, cache=FALSE}

####################### Read in data #######################
dataset4_ED_processed<-read.csv(here::here("data", "2_processed_data_for_OSF", "dataset4_ED_processed.csv"), sep=",",
                                na.strings="NA")

####################### number of participants with valid ESM data ##################### 
dataset4_ED_processed$count<-calc.nomiss(PAff, ID, dataset4_ED_processed, prop=FALSE, expand=TRUE)
range(dataset4_ED_processed$count)

# exclude those with not valid daily diary data
n_ED_orig<-nsub(ID, dataset4_ED_processed)
n_ED_orig

dataset4_ED_processed <-subset(dataset4_ED_processed, count > 0) 
range(dataset4_ED_processed$count)

n_ED_orig<-nsub(ID, dataset4_ED_processed)
n_ED_orig


```


### Psychometrics
#### Multilevel Confirmatory Factor Analysis
  
```{r Emotions in daily life MFCA, cache=FALSE}

##################### delete observations with 0 variance #####################

dataset4_ED_processed1<-dataset4_ED_processed
dataset4_ED_processed1 <- check.timeinvar(PA1, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(PA2, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(PA3, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(PA4, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(PA5, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(PA6, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(PA7, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(NA1, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(NA2, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(NA3, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(NA4, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(NA5, ID, dataset4_ED_processed1, out = 3)
dataset4_ED_processed1 <- check.timeinvar(NA6, ID, dataset4_ED_processed1, out = 3)

##################### MCFA ######################
#### Between-person (within level saturated) ####

mcfa_between_NAPA <- '
level: 1

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA2 ~~ PA3 + PA4 + PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA3 ~~ PA4 + PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA4 ~~ PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA5 ~~ PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA6 ~~ PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA7 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6
NA2 ~~ NA3 + NA4 + NA5 + NA6 
NA3 ~~ NA4 + NA5 + NA6 
NA4 ~~ NA5 + NA6 
NA5 ~~ NA6 


level: 2

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

'


m1_R_B_ED <- sem(model = mcfa_between_NAPA, data = dataset4_ED_processed1, cluster = "ID")
#ID 3844 has zero variance on PA2. Will be deleted
dataset4_ED_processed1 <- dataset4_ED_processed1[dataset4_ED_processed1$ID != "ED_114",]

#fit model again
m1_R_B_ED <- sem(model = mcfa_between_NAPA, data = dataset4_ED_processed1, cluster = "ID")
summary(m1_R_B_ED, fit.measures=TRUE, standardized = TRUE)

#Factor loadings are high (>.39) 
#Model fit is good (CFI=.99, RMSEA=.03)

#### Within-person (between-level saturated) ####

mcfa_within_NAPA <- '
level: 1

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

level: 2

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA2 ~~ PA3 + PA4 + PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA3 ~~ PA4 + PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA4 ~~ PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA5 ~~ PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA6 ~~ PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA7 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6
NA2 ~~ NA3 + NA4 + NA5 + NA6 
NA3 ~~ NA4 + NA5 + NA6 
NA4 ~~ NA5 + NA6 
NA5 ~~ NA6 
'

m1_R_W_ED <- sem(model = mcfa_within_NAPA, data = dataset4_ED_processed1, cluster = "ID")
summary(m1_R_W_ED, fit.measures=TRUE, standardized = TRUE)

#Factor loadings are medium to high (>.26 ; NA2 and PA4 load a little lower) 

#Model fit is not sufficient (e.g., CFI = .88, RMSEA = .09)
#Thus, I will check out modification indices now
modindices(m1_R_W_ED, sort = TRUE, maximum.number = 5)

#Residuals of NA1 (angry) and NA5 (irritated) should be correlated
#Residuals of NA4 (depressed) and NA6 (sad) should be correlated
#This makes sense, because the wording is very similar.
#So, I will try this out

mcfa_within_NAPA_MI <- '
level: 1

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 + PA7
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

#add correlation based on modindices
NA1 ~~ NA5
NA4 ~~ NA6

level: 2

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA2 ~~ PA3 + PA4 + PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6
PA3 ~~ PA4 + PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA4 ~~ PA5 + PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA5 ~~ PA6 + PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA6 ~~ PA7 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
PA7 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6
NA2 ~~ NA3 + NA4 + NA5 + NA6 
NA3 ~~ NA4 + NA5 + NA6 
NA4 ~~ NA5 + NA6 
NA5 ~~ NA6
'

m2_R_W_ED <- sem(model = mcfa_within_NAPA_MI, data = dataset4_ED_processed1, cluster = "ID")
summary(m2_R_W_ED, fit.measures=TRUE, standardized = TRUE)


#model fit is acceptable (CFI=.90, RMSEA=.08)

#Comparison with first model
BIC(m1_R_W_ED) #339655.4
BIC(m2_R_W_ED) #338878.1
BIC(m1_R_W_ED)-BIC(m2_R_W_ED) #BIC difference of 777.3485 (in favor of model 2)

``` 

Overall, the results suggest that the PA and NA factors are found on the between-level. The within-level however was not acceptable initially. Given the similar wording between some items, modification indices suggested to include correlations between residuals of two item sets (sad with depressed and angry with irritated).
After that, the model fit was acceptable at the within-person level. The between-person level showed a good fit anyways.


#### Within-and between-person reliabilities

```{r Emotions in Daily Life reliabilities, cache=FALSE}
# make new dataset into wide format with only depression scores
dataset4_ED_dep <- 
  dataset4_ED_processed %>%
  select("ID", starts_with('bdi', ignore.case = FALSE))

# reshape into wideformat (note: depression is constant, so I summarize across time-points)
dataset4_ED_dep_wide<-dataset4_ED_dep %>% group_by(ID) %>% summarise_all(mean)

names(dataset4_ED_dep_wide)

##################### Depression #####################

which(colnames(dataset4_ED_dep_wide)=="bdi1") # dep item 1 - baseline
which(colnames(dataset4_ED_dep_wide)=="bdi21") # dep item 20 - baseline

#Baseline
alpha_DepB_ED<-psych::alpha(dataset4_ED_dep_wide[c(2:22)], check.keys=TRUE)
alpha_DepB_ED

##################### Emotions #####################
#### PA ####

### Omega ###
omega_PA_ED <- omegaSEM(
  items = c("PA1", "PA2", "PA3", 
            "PA4", "PA5", "PA6", 
            "PA7"),
  id = "ID",
  data = dataset4_ED_processed1,
  savemodel = FALSE)

#### NA ####

### Omega ###
omega_NA_ED <- omegaSEM(
  items = c("NA1","NA2", "NA3",
            "NA4","NA5", "NA6"),
  id = "ID",
  data = dataset4_ED_processed1,
  savemodel = FALSE)

#### All results ####
omega_PA_ED
omega_NA_ED

```



### Intra-class correlations


```{r Emotions in daily life ICCs, cache=FALSE}
##################### PA #####################
PA_ICC <- lmer(PAff ~ 1 + (1 | ID), data = dataset4_ED_processed)
PA_ICCr_ED <- performance::icc(PA_ICC)
PA_ICCr_ED

##################### NA #####################
NA_ICC <- lmer(NAff ~ 1 + (1 | ID), data = dataset4_ED_processed)
NA_ICCr_ED <- performance::icc(NA_ICC)
NA_ICCr_ED

```
### Exclusion and Compliance
```{r Emotions in Daily Life Exclusion and Compliance, cache=FALSE}

##################### Details study design #####################
#### Total beeps ####
tot_ass_ED<-70
#### Total days ####
tot_days_ED <- 14
#### Total beeps per day ####
tot_b_d_ED<-5

#### Number of valid beeps before exclusion ####
mean(calc.mean(count, ID, data=dataset4_ED_processed)) 

#### Percentage of valid beeps before exclusion #### 
dataset4_ED_processed$perc<-(100/tot_ass_ED)*dataset4_ED_processed$count
mean(calc.mean(perc, ID, data=dataset4_ED_processed)) 

#### Number of participants before exclusion ####
n_ED<-nsub(ID, dataset4_ED_processed)
n_ED

#### Removing participants with compliance below 33% #### 

## one-third of beeps ##
valbeep_ED <- (tot_ass_ED/100)*33
valbeep_ED

## excluding those with fewer than 33% ##
dataset4_ED_processed <-subset(dataset4_ED_processed, count > valbeep_ED) 

#### Delete participants without any variance on NAff and PAff ###
## n before ##
nsub(ID, dataset4_ED_processed)

#### Delete participants without any variance on NAff and PAff ####
dataset4_ED_processed <- check.timeinvar(NAff, ID, dataset4_ED_processed, out = 3)
dataset4_ED_processed <- check.timeinvar(PAff, ID, dataset4_ED_processed, out = 3)

## n after ##
nsub(ID, dataset4_ED_processed) #no participant excluded


#### final sample ####
n_fin_ED<-nsub(ID, dataset4_ED_processed)
n_fin_ED
perc_inc_ED <- (100/n_ED_orig)*n_fin_ED
perc_inc_ED



#### mean of valid beeps per participant ####
mean_comp_ED<-mean(calc.mean(count, ID, data=dataset4_ED_processed)) 
mean_comp_ED 

#### percentage of valid beeps per participant ####
mean_com_per_ED<-mean(calc.mean(perc, ID, data=dataset4_ED_processed)) 
mean_com_per_ED


```

### Calculation of affective dynamics

#### Datapreparation

```{r Emotions in daily life number affective dynamics dataprep, cache=FALSE}

##################### Deleting rows with missings on days #####################
#(later no NA's are allowed on days for the lagging of variables for the autocorrelation)
dataset4_ED_processed<-dataset4_ED_processed[!is.na(dataset4_ED_processed$Day),]

##################### sort by ID and time #####################
dataset4_ED_processed<-dataset4_ED_processed[order(dataset4_ED_processed$ID,dataset4_ED_processed$time) , ]

##################### subset most important variables #####################
dataset4_ED_processed_affdyn <- subset(dataset4_ED_processed, select=c("ID","time","Day","PAff","NAff"))

##################### delete missing values ##################### 
#so that differences are also calculated between beeps that were
#not directly next to each other (e.g., 3, 4(NA), 5) 
#I still want to calculate the difference between 3 and 5
dataset4_ED_processed_affdyn<-dataset4_ED_processed_affdyn[!is.na(dataset4_ED_processed_affdyn$PAff),]

##################### make a new time variable #####################


dataset4_ED_processed_affdyn<-dataset4_ED_processed_affdyn %>% group_by(ID) %>% dplyr::mutate(time = rank(time, ties.method = 'first'))%>% arrange(time)

##################### add missing days after each week ##################### 
#this is important, because for the MSSD, we do not want to calculate the
#difference between the last beep of one day and the first beep of the next day/week
dataset4_ED_processed_affdyn_NA <-insert_NA(dataset4_ED_processed_affdyn, ID, Day)


```




#### Calculation 
```{r Emotions in daily life affective dynamics calc, cache=FALSE}


##################### Calculate affective dynamics #####################
#### Mean, SD ####

aff_dyn_ED <- ddply(dataset4_ED_processed_affdyn_NA, "ID",  plyr::summarize,
                           icountNA = sum(!is.na(NAff)),  #count of observations
                           imeanNA  = mean(NAff, na.rm=TRUE), #imean (continuous)
                           isdNA   = sd(NAff, na.rm=TRUE), #isd (continuous)
                           icountPA = sum(!is.na(PAff)),  #count of observations
                           imeanPA  = mean(PAff, na.rm=TRUE), #imean (continuous)
                           isdPA    = sd(PAff, na.rm=TRUE),   #isd   (continuous)  
                           relisdNA = relativeSD(NAff, 1, 9), #relative isd (continuous)
                           relisdPA = relativeSD(PAff, 1, 9)) #relative isd (continuous))

#### MSSD ####

mssd.stats.NA <- ddply(dataset4_ED_processed_affdyn_NA, "ID", plyr::summarize, iMSSDNA=my.mssd(NAff))
mssd.stats.PA <- ddply(dataset4_ED_processed_affdyn_NA, "ID", plyr::summarize, iMSSDPA=my.mssd(PAff))

aff_dyn_ED <- merge(aff_dyn_ED,mssd.stats.NA,by="ID",all=TRUE)
aff_dyn_ED <- merge(aff_dyn_ED,mssd.stats.PA,by="ID",all=TRUE)


#### autocorrelation ####

## Lagged effect ##

dataset4_ED_processed_affdyn$NAff_lag<-lagvar(NAff, id=ID, obs=time, day=Day,data=dataset4_ED_processed_affdyn)
dataset4_ED_processed_affdyn$PAff_lag<-lagvar(PAff, id=ID, obs=time, day=Day,data=dataset4_ED_processed_affdyn)

## Mixed model ##
auto_NA <- lmer(NAff ~ 1 + NAff_lag + (1 + NAff_lag | ID), 
                     data = dataset4_ED_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_NA)

auto_PA <- lmer(PAff ~ 1 + PAff_lag + (1 + PAff_lag | ID), 
                     data = dataset4_ED_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_PA)

## Extract individual slopes ##
beta_NA<-tibble::rownames_to_column(coef(auto_NA)$ID)
beta_NA<-plyr::rename(beta_NA, 
                c("NAff_lag" = "autocorNA",
                  "rowname" =  "ID"))

beta_PA<-tibble::rownames_to_column(coef(auto_PA)$ID)
beta_PA<-plyr::rename(beta_PA, 
                c("PAff_lag" = "autocorPA",
                  "rowname" =  "ID"))

##################### Merge data #####################
aff_dyn_ED <- merge(aff_dyn_ED,beta_NA,by="ID",all=TRUE)
aff_dyn_ED <- merge(aff_dyn_ED,beta_PA,by="ID",all=TRUE)

##################### Delete intercept from aff_dyn files #####################
aff_dyn_ED<-subset(aff_dyn_ED, select=c(1:11,13,15))
names(aff_dyn_ED)
```

### Merging

```{r Emotions in daily life merging, cache=FALSE}

##################### restructure depression/demographics data #####################
#The dataset was in longformat, however the affective dynamics measures are in wide format
#Thus, I restructure the dataset (which only contain time-invariant variables anyway)
#and merge with the affective dynamics data
data_id_dep_ED <- subset(dataset4_ED_processed, select=c("ID","Sex","Age","DepB"))
data_id_dep_ED<-data_id_dep_ED %>% group_by(ID) %>% summarise_all(mean)


##################### Merge affective dynamics and depression/demographics data #####################
datasetED_final <- merge(data_id_dep_ED,aff_dyn_ED,by="ID",all=TRUE)
names(datasetED_final)

```




### Winzorising and transformation of skewed variables

```{r Emotions in daily life transformation + winsorizing, cache=FALSE}

##################### winsorizing #####################
names(datasetED_final)

datasetED_final<-datasetED_final[order(datasetED_final$ID) , ]

psych::describe(datasetED_final)

dataED_finalW<-data.frame(datasetED_final)

#select predictors and outcome variables (affective dynamics and depression)
predictor_wins<-c(4,6,7,9:16)

#winsorizing
dataED_finalW<-apply(dataED_finalW[,predictor_wins], 
                                 2,  
                                 Winsorize, probs=c(0.01,0.99), na.rm = TRUE)

dataED_finalW<-data.frame(dataED_finalW)

psych::describe(dataED_finalW)               

#add id number
dataED_finalW$ID <- datasetED_final$ID

#make subset of variables that were not included in the winsorizing
sub1<-subset(datasetED_final, select=c(1:3,5,8))

#Merge files
dataED_finalW <- merge(dataED_finalW,sub1,by="ID",all=TRUE)

##################### Transforming ###################### 
#if skewness is > 3 (Kline, 2011)
#### identifying ####
skew<-skew(select(dataED_finalW, -ID))>3
skew

length(skew[skew== TRUE])

#no transformations necessary

names(dataED_finalW)

```


### Center/standardize study variables

- Center of predictors (for moderation)
- Standardization of the depression scale

```{r Emotions in daily life center predictors, cache=FALSE}

names(dataED_finalW)

psych::describe(dataED_finalW)

##################### center predictors #####################
predictor_scale<-c(3:12)

dataED_finalWC<-apply(dataED_finalW[,predictor_scale], 
                           2,  
                           scale, scale = FALSE)

dataED_finalWC<-data.frame(dataED_finalWC)

psych::describe(dataED_finalWC)               

#add id number
dataED_finalWC$ID<- dataED_finalW$ID

#make subset of variables that were not included in the standardization
sub2<-subset(dataED_finalW, select=c(1:2,13:16))

#Merge files
dataED_finalWC <- merge(dataED_finalWC,sub2,by="ID",all=TRUE)

##################### standardize Depression #####################
dataED_finalWC$DepB<-scale(dataED_finalWC$DepB, center = TRUE, scale = TRUE)
psych::describe(dataED_finalWC)


names(dataED_finalWC)

```


### Write cleaned datasets 

The "raw" (i.e., not centered & standardized) and centered/standardized versions.

```{r Emotions in daily life write datasets, cache=FALSE}
write.csv(dataED_finalW, here::here("data", "3_cleaned_data_for_OSF", "dataset4_ED_clean.csv"), row.names = FALSE)
write.csv(dataED_finalWC, here::here("data", "3_cleaned_data_for_OSF", "dataset4_ED_clean_cen_std.csv"), row.names = FALSE)

```
## DATASET 5: EMOTION REGULATION IN ACTION

### Read in data and datapreparation

```{r Emotion regulation in action read in data, cache=FALSE}

####################### Read in data #######################
dataset5_EA_processed<-read.csv(here::here("data", "2_processed_data_for_OSF", "dataset5_EA_processed.csv"), sep=",",
                                na.strings="NA")

####################### number of participants with valid ESM data ##################### 
dataset5_EA_processed$count<-calc.nomiss(PAff, ID, dataset5_EA_processed, prop=FALSE, expand=TRUE)
range(dataset5_EA_processed$count)

# exclude those with not valid daily diary data
n_EA_orig<-nsub(ID, dataset5_EA_processed)
n_EA_orig

dataset5_EA_processed <-subset(dataset5_EA_processed, count > 0) 
range(dataset5_EA_processed$count)

n_EA_orig<-nsub(ID, dataset5_EA_processed)
n_EA_orig

```
### Psychometrics
#### Multilevel Confirmatory Factor Analysis

  
```{r Emotion regulation in action MFCA, cache=FALSE}

##################### delete observations with 0 variance #####################
dataset5_EA_processed1<-dataset5_EA_processed
dataset5_EA_processed1 <- check.timeinvar(PA1, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(PA2, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(PA3, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(PA4, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(PA5, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(PA6, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(NA1, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(NA2, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(NA3, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(NA4, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(NA5, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(NA6, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(NA7, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(NA8, ID, dataset5_EA_processed1, out = 3)
dataset5_EA_processed1 <- check.timeinvar(NA9, ID, dataset5_EA_processed1, out = 3)

##################### MCFA ######################
#### Between-person (within level saturated) ####

mcfa_between_NAPA <- '
level: 1

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA2 ~~ PA3 + PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA3 ~~ PA4 + PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA4 ~~ PA5 + PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA5 ~~ PA6 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA6 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA2 ~~ NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA3 ~~ NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA4 ~~ NA5 + NA6 + NA7 + NA8 + NA9
NA5 ~~ NA6 + NA7 + NA8 + NA9
NA6 ~~ NA7 + NA8 + NA9
NA7 ~~ NA8 + NA9
NA8 ~~ NA9

level: 2

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5 + PA6 
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

'

m1_R_B_EA <- sem(model = mcfa_between_NAPA, data = dataset5_EA_processed1, cluster = "ID")
#ID EA_47 and EA_41 has zero variance on some items. Will be deleted HERE!!!!!!!!!!
dataset5_EA_processed1 <- dataset5_EA_processed1[dataset5_EA_processed1$ID != "EA_47",]
dataset5_EA_processed1 <- dataset5_EA_processed1[dataset5_EA_processed1$ID != "EA_41",]

#fit again
m1_R_B_EA <- sem(model = mcfa_between_NAPA, data = dataset5_EA_processed1, cluster = "ID")
summary(m1_R_B_EA, fit.measures=TRUE, standardized = TRUE)

#Factor loadings are high (>.60), except for PA6 (admiration, FL = .07, p = .74). This item will be deleted.
#Model fit is good (CFI=.98, RMSEA=.03)

## no PA6 ##

mcfa_between_NAPA_noPA6 <- '
level: 1

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA2 ~~ PA3 + PA4 + PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA3 ~~ PA4 + PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA4 ~~ PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA5 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA2 ~~ NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA3 ~~ NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA4 ~~ NA5 + NA6 + NA7 + NA8 + NA9
NA5 ~~ NA6 + NA7 + NA8 + NA9
NA6 ~~ NA7 + NA8 + NA9
NA7 ~~ NA8 + NA9
NA8 ~~ NA9

level: 2

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5  
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

'

m2_R_B_EA <- sem(model = mcfa_between_NAPA_noPA6, data = dataset5_EA_processed1, cluster = "ID")
summary(m2_R_B_EA, fit.measures=TRUE, standardized = TRUE)

#Factor loadings are high (>.68)
#Model fit is good (CFI=.98, RMSEA=.03)


#### Within-person (between-level saturated) ####
#no PA6 (as shown in between-person MCFA)

mcfa_within_NAPA <- '
level: 1

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5  
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

level: 2

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA2 ~~ PA3 + PA4 + PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA3 ~~ PA4 + PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA4 ~~ PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA5 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA2 ~~ NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA3 ~~ NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA4 ~~ NA5 + NA6 + NA7 + NA8 + NA9
NA5 ~~ NA6 + NA7 + NA8 + NA9
NA6 ~~ NA7 + NA8 + NA9
NA7 ~~ NA8 + NA9
NA8 ~~ NA9
'

m1_R_W_EA <- sem(model = mcfa_within_NAPA, data = dataset5_EA_processed1, cluster = "ID")
summary(m1_R_W_EA, fit.measures=TRUE, standardized = TRUE)

#Factor loadings are high (>.45) 
#Model fit is not sufficient (CFI = .88, RMSEA = .08), so checking out modindices

modindices(m1_R_W_EA, sort = TRUE, maximum.number = 5)

#suggest that NA4 (irritated) and NA6 (angry) should be correlated. Makes sense cause extremely similar wording.
#actually saw the same with emotion regulation in daily life

mcfa_within_NAPA_MI <- '
level: 1

#factor structure
pa =~ PA1 + PA2 + PA3 + PA4 + PA5  
na =~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9

#correlation higher-order factors
pa ~~ pa + na
na ~~ na

#add correlation based on modindices
NA4 ~~ NA6

level: 2

#correlation between all items
PA1 ~~ PA2 + PA3 + PA4 + PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA2 ~~ PA3 + PA4 + PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA3 ~~ PA4 + PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA4 ~~ PA5 + NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
PA5 ~~ NA1 + NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA1 ~~ NA2 + NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA2 ~~ NA3 + NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA3 ~~ NA4 + NA5 + NA6 + NA7 + NA8 + NA9
NA4 ~~ NA5 + NA6 + NA7 + NA8 + NA9
NA5 ~~ NA6 + NA7 + NA8 + NA9
NA6 ~~ NA7 + NA8 + NA9
NA7 ~~ NA8 + NA9
NA8 ~~ NA9
'

m2_R_W_EA <- sem(model = mcfa_within_NAPA_MI, data = dataset5_EA_processed1, cluster = "ID")
summary(m2_R_W_EA, fit.measures=TRUE, standardized = TRUE)

#Model fit is sufficient (CFI = .91, RMSEA = .07)

#Comparison with first model
BIC(m1_R_W_EA) #65790.47
BIC(m2_R_W_EA) #65646.16
BIC(m1_R_W_EA)-BIC(m2_R_W_EA) #BIC difference of 144.3089 (in favor of model 2)

##### Calculate new PAff factor without PA6
dataset5_EA_processed$PAff <- combitems(c(6:10), data=dataset5_EA_processed)

``` 
Overall, the results suggest that the PA and NA factors are found on the between-level. The within-level however was not acceptable initially. Given the similar wording between NA4 and NA6 (angry and irritated), modification indices suggested to include correlations between those residuals.
After that, the model fit was acceptable at the within-person level. The between-person level showed a good fit anyways.

PA6 (admiration) was deleted, because the MCFA suggested that it did not load on the PA factor.


#### Within-and between-person reliabilities

```{r Emotion regulation in action reliabilities, cache=FALSE}
# make new dataset into wide format with only depression scores
dataset5_EA_dep <- 
  dataset5_EA_processed %>%
  select("ID", starts_with('cdi', ignore.case = FALSE))

# reshape into wideformat (note: depression is constant, so I summarize across time-points)
dataset5_EA_dep_wide<-dataset5_EA_dep %>% group_by(ID) %>% summarise_all(mean)

names(dataset5_EA_dep_wide)

##################### Depression #####################
which(colnames(dataset5_EA_dep_wide)=="cdi1") # dep item 1 - baseline
which(colnames(dataset5_EA_dep_wide)=="cdi27") # dep item 20 - baseline

#### Baseline ####
alpha_DepB_EA<-psych::alpha(dataset5_EA_dep_wide[c(2:27)], check.keys=TRUE)
alpha_DepB_EA

##################### Emotions #####################
#### PA ####

### Omega ###
omega_PA_EA <- omegaSEM(
  items = c("PA1", "PA2", "PA3", "PA4", "PA5"),
  id = "ID",
  data = dataset5_EA_processed1,
  savemodel = FALSE)

#### NA ####

### Omega ###
omega_NA_EA <- omegaSEM(
  items = c("NA1", "NA2", "NA3",
            "NA4", "NA5", "NA6",
            "NA7", "NA8", "NA9"),
  id = "ID",
  data = dataset5_EA_processed1,
  savemodel = FALSE)

#### All results ####
omega_PA_EA
omega_NA_EA


```


### Intra-class correlations


```{r Emotion regulation in action ICCs, cache=FALSE}
##################### PA #####################
#did not include PA6, cause it was deleted based on low factor loadings from MCFA
PA_ICC <- lmer(PAff ~ 1 + (1 | ID), data = dataset5_EA_processed)
PA_ICCr_EA <- performance::icc(PA_ICC)
PA_ICCr_EA

##################### NA #####################
NA_ICC <- lmer(NAff ~ 1 + (1 | ID), data = dataset5_EA_processed)
NA_ICCr_EA <- performance::icc(NA_ICC)
NA_ICCr_EA

```


### Exclusion and Compliance
```{r Emotion regulation in action exclusion and compliance, cache=FALSE}

##################### Details study design #####################
#### Total beeps ####
tot_ass_EA<-44
#### Total days #### (2 Fridays, 2 Saturdays, 2 Sundays)
tot_days_EA <- 6
#### Total beeps per day ####
tot_b_d_EA_F<-4 #Friday
tot_b_d_EA_SS<-9 #Saturday/Sunday

#### Number of valid beeps before exclusion ####
mean(calc.mean(count, ID, data=dataset5_EA_processed)) 

#### Percentage of valid beeps before exclusion #### 
possassEA<-tot_ass_EA*n_EA_orig #possible assessments
validassEA<-sum(calc.mean(count, ID, data=dataset5_EA_processed)) #number of all valid assessments
((100/possassEA)*validassEA) #percentage of valid beeps

dataset5_EA_processed$perc<-(100/tot_ass_EA)*dataset5_EA_processed$count
mean(calc.mean(perc, ID, data=dataset5_EA_processed)) 

#### Number of participants before exclusion ####
n_EA<-nsub(ID, dataset5_EA_processed)
n_EA

#### Removing participants with compliance below 33% #### 

## one-third of beeps ##
valbeep_EA <- (tot_ass_EA/100)*33
valbeep_EA

## excluding those with fewer than 33% ##
dataset5_EA_processed <-subset(dataset5_EA_processed, count > valbeep_EA) 

#### Delete participants without any variance on NAff and PAff ###
## n before ##
nsub(ID, dataset5_EA_processed)

#### Delete participants without any variance on NAff and PAff ####
dataset5_EA_processed <- check.timeinvar(NAff, ID, dataset5_EA_processed, out = 3)
dataset5_EA_processed <- check.timeinvar(PAff, ID, dataset5_EA_processed, out = 3)

## n after ##
nsub(ID, dataset5_EA_processed) #no participants excluded

#### final sample ####
n_fin_EA<-nsub(ID, dataset5_EA_processed)
n_fin_EA
perc_inc_EA <- (100/n_EA_orig)*n_fin_EA
perc_inc_EA



#### mean of valid beeps per participant ####
mean_comp_EA<-mean(calc.mean(count, ID, data=dataset5_EA_processed)) 
mean_comp_EA 

#### percentage of valid beeps per participant ####
validassEA<-sum(calc.mean(count, ID, data=dataset5_EA_processed)) #number of all valid assessments
mean_com_per_EA<-((100/possassEA)*validassEA) #percentage of valid beeps
mean_com_per_EA

```

### Calculation of affective dynamics

#### Datapreparation

```{r Emotion regulation in action number affective dynamics dataprep, cache=FALSE}

##################### sort by ID and time #####################
dataset5_EA_processed<-dataset5_EA_processed[order(dataset5_EA_processed$ID,dataset5_EA_processed$time) , ]

##################### subset most important variables #####################
dataset5_EA_processed_affdyn <- subset(dataset5_EA_processed, select=c("ID","time","Day","PAff","NAff"))

##################### delete missing values ##################### 
#Note that Emotion regulation in action did not have information on missing data in between beeps.
dataset5_EA_processed_affdyn<-dataset5_EA_processed_affdyn[!is.na(dataset5_EA_processed_affdyn$PAff),]

##################### make a new time variable #####################


dataset5_EA_processed_affdyn<-dataset5_EA_processed_affdyn %>% group_by(ID) %>% dplyr::mutate(time = rank(time, ties.method = 'first'))%>% arrange(time)

##################### add missing days after each week ##################### 
#this is important, because for the MSSD, we do not want to calculate the
#difference between the last beep of one day and the first beep of the next day/week
dataset5_EA_processed_affdyn_NA <-insert_NA(dataset5_EA_processed_affdyn, ID, Day)



```



#### Calculation 
```{r Emotion regulation in action affective dynamics calc, cache=FALSE}

##################### Calculate affective dynamics #####################
#### Mean, SD ####


aff_dyn_EA <- ddply(dataset5_EA_processed_affdyn_NA, "ID",  plyr::summarize,
                           icountNA = sum(!is.na(NAff)),  #count of observations
                           imeanNA  = mean(NAff, na.rm=TRUE), #imean (continuous)
                           isdNA   = sd(NAff, na.rm=TRUE), #isd (continuous)
                           icountPA = sum(!is.na(PAff)),  #count of observations
                           imeanPA  = mean(PAff, na.rm=TRUE), #imean (continuous)
                           isdPA    = sd(PAff, na.rm=TRUE),   #isd   (continuous)  
                           relisdNA = relativeSD(NAff, 1, 9), #relative isd (continuous)
                           relisdPA = relativeSD(PAff, 1, 9)) #relative isd (continuous))

#### MSSD ####

mssd.stats.NA <- ddply(dataset5_EA_processed_affdyn_NA, "ID", plyr::summarize, iMSSDNA=my.mssd(NAff))
mssd.stats.PA <- ddply(dataset5_EA_processed_affdyn_NA, "ID", plyr::summarize, iMSSDPA=my.mssd(PAff))

aff_dyn_EA <- merge(aff_dyn_EA,mssd.stats.NA,by="ID",all=TRUE)
aff_dyn_EA <- merge(aff_dyn_EA,mssd.stats.PA,by="ID",all=TRUE)


#### autocorrelation ####
## Lagged effect ##

dataset5_EA_processed_affdyn$NAff_lag<-lagvar(NAff, id=ID, obs=time, day=Day,data=dataset5_EA_processed_affdyn)
dataset5_EA_processed_affdyn$PAff_lag<-lagvar(PAff, id=ID, obs=time, day=Day,data=dataset5_EA_processed_affdyn)

## Mixed model ##
auto_NA <- lmer(NAff ~ 1 + NAff_lag + (1 + NAff_lag | ID), 
                     data = dataset5_EA_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_NA)

auto_PA <- lmer(PAff ~ 1 + PAff_lag + (1 + PAff_lag | ID), 
                     data = dataset5_EA_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_PA)

## Extract individual slopes ##
beta_NA<-tibble::rownames_to_column(coef(auto_NA)$ID)
beta_NA<-plyr::rename(beta_NA, 
                c("NAff_lag" = "autocorNA",
                  "rowname" =  "ID"))

beta_PA<-tibble::rownames_to_column(coef(auto_PA)$ID)
beta_PA<-plyr::rename(beta_PA, 
                c("PAff_lag" = "autocorPA",
                  "rowname" =  "ID"))

##################### Merge data #####################
aff_dyn_EA <- merge(aff_dyn_EA,beta_NA,by="ID",all=TRUE)
aff_dyn_EA <- merge(aff_dyn_EA,beta_PA,by="ID",all=TRUE)

##################### Delete intercept from aff_dyn files #####################
aff_dyn_EA<-subset(aff_dyn_EA, select=c(1:11,13,15))
names(aff_dyn_EA)

```
### Merging

```{r Emotion regulation in action merging, cache=FALSE}

##################### restructure depression/demographics data #####################
#The dataset was in longformat, however the affective dynamics measures are in wide format
#Thus, I restructure the dataset (which only contain time-invariant variables anyway)
#and merge with the affective dynamics data
data_id_dep_EA <- subset(dataset5_EA_processed, select=c("ID","Sex","Age","DepB"))
data_id_dep_EA<-data_id_dep_EA %>% group_by(ID) %>% summarise_all(mean)

length(data_id_dep_EA$ID)

##################### Merge affective dynamics and depression/demographics data #####################
datasetEA_final <- merge(data_id_dep_EA,aff_dyn_EA,by="ID",all=TRUE)
names(datasetEA_final)

```



### Winzorising and transformation of skewed variables

```{r Emotion regulation in action transformation + winsorizing, cache=FALSE}
##################### winsorizing #####################
names(datasetEA_final)

datasetEA_final<-datasetEA_final[order(datasetEA_final$ID) , ]

psych::describe(datasetEA_final)

dataEA_finalW<-data.frame(datasetEA_final)

predictor_wins<-c(4,6,7,9:16)

dataEA_finalW<-apply(dataEA_finalW[,predictor_wins], 
                                 2,  
                                 Winsorize, probs=c(0.01,0.99), na.rm = TRUE)

dataEA_finalW<-data.frame(dataEA_finalW)

psych::describe(dataEA_finalW)               

#add id number
dataEA_finalW$ID <- datasetEA_final$ID

#make subset of variables that were not included in the winsorizing
sub1<-subset(datasetEA_final, select=c(1:3,5,8))

#Merge files
dataEA_finalW <- merge(dataEA_finalW,sub1,by="ID",all=TRUE)

##################### Transforming ###################### 
#if skewness is > 3 (Kline, 2011)
#### identifying ####
skew<-skew(select(dataEA_finalW, -ID))>3
skew

length(skew[skew== TRUE])

#no transformations necessary

names(dataEA_finalW)


```


### Center/standardize study variables

- Center of predictors (for moderation)
- Standardization of the depression scale

```{r Emotion regulation in action center predictors, cache=FALSE}

names(dataEA_finalW)

psych::describe(dataEA_finalW)

##################### center predictors #####################
predictor_scale<-c(3:12)

dataEA_finalWC<-apply(dataEA_finalW[,predictor_scale], 
                           2,  
                           scale, scale = FALSE)

dataEA_finalWC<-data.frame(dataEA_finalWC)

psych::describe(dataEA_finalWC)               

#add id number
dataEA_finalWC$ID<- dataEA_finalW$ID

#make subset of variables that were not included in the standardization
sub2<-subset(dataEA_finalW, select=c(1:2,13:16))

#Merge files
dataEA_finalWC <- merge(dataEA_finalWC,sub2,by="ID",all=TRUE)

names(dataEA_finalWC)

##################### standardize Depression #####################
dataEA_finalWC$DepB<-scale(dataEA_finalWC$DepB, center = TRUE, scale = TRUE)
psych::describe(dataEA_finalWC)

```

### Write cleaned datasets 

The "raw" (i.e., not centered & standardized) and centered/standardized versions.

```{r Emotion regulation in action write datasets, cache=FALSE}
write.csv(dataEA_finalW, here::here("data", "3_cleaned_data_for_OSF", "dataset5_EA_clean.csv"), row.names = FALSE)
write.csv(dataEA_finalWC, here::here("data", "3_cleaned_data_for_OSF", "dataset5_EA_clean_cen_std.csv"), row.names = FALSE)

```
## DATASET 6: LASER

For the LASER dataset, there were only 2 emotion items assessed. First, participants had to rate their overall mood on a scale from -10 to +10 (very negative to very positive). This is what is referred to as Bipolar Affect in this paper (BA). Second, participants had to rate only their most negative emotion (selected from sadness, anger and anxiety). 


Given that there is only 1 items for NA, we cannot calculate MCFA's and alpha and omega. We can calculate the ICCs.


### Read in data and datapreparation

```{r LASER read in data, cache=FALSE}
####################### Read in data #######################
dataset6_LASER_processed<-read.csv(here::here("data", "2_processed_data_for_OSF", "dataset6_LASER_processed.csv"), sep=",",
                                na.strings="NA")

####################### number of participants with valid ESM data ##################### 
dataset6_LASER_processed$count<-calc.nomiss(NAff, ID, dataset6_LASER_processed, prop=FALSE, expand=TRUE)
range(dataset6_LASER_processed$count)

# exclude those with not valid daily diary data
n_LASER_orig<-nsub(ID, dataset6_LASER_processed)
n_LASER_orig

dataset6_LASER_processed <-subset(dataset6_LASER_processed, count > 0) 
range(dataset6_LASER_processed$count)

n_LASER_orig<-nsub(ID, dataset6_LASER_processed)
n_LASER_orig

```
### Between-person reliabilities

No calculation of within-person reliabilities on the emotion items were possible here, since only 1 item available for NA and BA.
So, we only calculate reliability of the BDI here.

```{r LASER reliabilities, cache=FALSE}

# make new dataset into wide format with only depression scores
dataset6_LASER_dep <- 
  dataset6_LASER_processed %>%
  select("ID", starts_with('BDI', ignore.case = FALSE))

# reshape into wideformat (note: depression is constant, so I summarize across time-points)
dataset6_LASER_dep_wide<-dataset6_LASER_dep %>% group_by(ID) %>% summarise_all(mean)

names(dataset6_LASER_dep_wide)

##################### Depression #####################
which(colnames(dataset6_LASER_dep_wide)=="BDI_01") # dep item 1 - baseline
which(colnames(dataset6_LASER_dep_wide)=="BDI_17R") # dep item 17R - baseline


#### Baseline ####
alpha_DepB_LASER<-psych::alpha(dataset6_LASER_dep_wide[c(2:20)], check.keys=TRUE)
alpha_DepB_LASER

```
### Intra-class correlations

```{r LASER ICC, cache=FALSE}

##################### delete observations with 0 variance #####################

dataset6_LASER_processed1<-dataset6_LASER_processed
dataset6_LASER_processed1 <- check.timeinvar(NAff, ID, dataset6_LASER_processed1, out = 3)


##################### ICCs #####################

#### NA ####
NA_ICC <- lmer(NAff ~ 1 + (1 | ID), data = dataset6_LASER_processed)
NA_ICCr_LASER <- performance::icc(NA_ICC)
NA_ICCr_LASER

#### Bipolar Affect scale #### 
Bip_aff_ICC <- lmer(Bip_aff ~ 1 + (1 | ID), data = dataset6_LASER_processed)
Bip_aff_ICCr_LASER <- performance::icc(Bip_aff_ICC)
Bip_aff_ICCr_LASER

```
### Exclusion and Compliance
```{r LASER exclusion and compliance, cache=FALSE}

##################### Details study design #####################
#### Total beeps ####
tot_ass_LASER<-42
#### Total days ####
tot_days_LASER <- 14
#### Total beeps per day ####
tot_b_d_LASER<-3

#### Number of valid beeps before exclusion ####
mean(calc.mean(count, ID, data=dataset6_LASER_processed)) 

#### Percentage of valid beeps before exclusion #### 
dataset6_LASER_processed$perc<-(100/tot_ass_LASER)*dataset6_LASER_processed$count
mean(calc.mean(perc, ID, data=dataset6_LASER_processed)) 

#### Number of participants before exclusion ####
n_LASER<-nsub(ID, dataset6_LASER_processed)
n_LASER

#### Removing participants with compliance below 33% #### 

## one-third of beeps ##
valbeep_LASER <- (tot_ass_LASER/100)*33
valbeep_LASER

## excluding those with fewer than 33% ##
dataset6_LASER_processed <-subset(dataset6_LASER_processed, count > valbeep_LASER) 

#### Delete participants without any variance on NAff and PAff ###
## n before ##
nsub(ID, dataset6_LASER_processed)

#### Delete participants without any variance on NAff and PAff ####
dataset6_LASER_processed <- check.timeinvar(NAff, ID, dataset6_LASER_processed, out = 3)
dataset6_LASER_processed <- check.timeinvar(Bip_aff, ID, dataset6_LASER_processed, out = 3)

## n after ##
nsub(ID, dataset6_LASER_processed) #no participants excluded


#### final sample ####
n_fin_LASER<-nsub(ID, dataset6_LASER_processed)
n_fin_LASER
perc_inc_LASER <- (100/n_LASER_orig)*n_fin_LASER
perc_inc_LASER



#### mean of valid beeps per participant ####
mean_comp_LASER<-mean(calc.mean(count, ID, data=dataset6_LASER_processed)) 
mean_comp_LASER 

#### percentage of valid beeps per participant ####
mean_com_per_LASER<-mean(calc.mean(perc, ID, data=dataset6_LASER_processed)) 
mean_com_per_LASER




```


### Calculation of affective dynamics

#### Datapreparation

```{r LASER number affective dynamics dataprep, cache=FALSE}

##################### sort by ID and time #####################
dataset6_LASER_processed<-dataset6_LASER_processed[order(dataset6_LASER_processed$ID,dataset6_LASER_processed$time) , ]

##################### subset most important variables #####################
dataset6_LASER_processed_affdyn <- subset(dataset6_LASER_processed, select=c("ID","time","Day","NAff","Bip_aff"))

##################### delete missing values ##################### 
#so that differences are also calculated between beeps that were
#not directly next to each other (e.g., 3, 4(NA), 5) 
#I still want to calculate the difference between 3 and 5
dataset6_LASER_processed_affdyn<-dataset6_LASER_processed_affdyn[!is.na(dataset6_LASER_processed_affdyn$NAff),]

##################### make a new time variable #####################
dataset6_LASER_processed_affdyn<-dataset6_LASER_processed_affdyn %>% group_by(ID) %>% dplyr::mutate(time = rank(time, ties.method = 'first'))%>% arrange(time)

##################### add missing days after each week ##################### 
#this is important, because for the MSSD, we do not want to calculate the
#difference between the last beep of one day and the first beep of the next day/week

dataset6_LASER_processed_affdyn_NA <-insert_NA(dataset6_LASER_processed_affdyn, ID, Day)


```

#### Calculation 
```{r LASER affective dynamics calc, cache=FALSE}


##################### Calculate affective dynamics #####################
#### Mean, SD ####

aff_dyn_LASER <- ddply(dataset6_LASER_processed_affdyn_NA, "ID",  plyr::summarize,
                           icountNA = sum(!is.na(NAff)),  #count of observations
                           imeanNA  = mean(NAff, na.rm=TRUE), #imean (continuous)
                           isdNA   = sd(NAff, na.rm=TRUE), #isd (continuous)
                           icountBip = sum(!is.na(Bip_aff)),  #count of observations
                           imeanBip_aff  = mean(Bip_aff, na.rm=TRUE), #imean (continuous)
                           isdBip_aff   = sd(Bip_aff, na.rm=TRUE),   #isd   (continuous)  
                           relisdNA = relativeSD(NAff, 1, 9)) #relative isd (continuous)


#### MSSD ####

mssd.stats.NA <- ddply(dataset6_LASER_processed_affdyn_NA, "ID", plyr::summarize, iMSSDNA=my.mssd(NAff))
mssd.stats.Bip_aff <- ddply(dataset6_LASER_processed_affdyn_NA, "ID", plyr::summarize, iMSSDBip_aff=my.mssd(Bip_aff))

aff_dyn_LASER <- merge(aff_dyn_LASER,mssd.stats.NA,by="ID",all=TRUE)
aff_dyn_LASER <- merge(aff_dyn_LASER,mssd.stats.Bip_aff,by="ID",all=TRUE)


#### autocorrelation ####


## Lagged effect ##

dataset6_LASER_processed_affdyn$NAff_lag<-lagvar(NAff, id=ID, obs=time, day=Day,data=dataset6_LASER_processed_affdyn)
dataset6_LASER_processed_affdyn$Bip_aff_lag<-lagvar(Bip_aff, id=ID, obs=time, day=Day,data=dataset6_LASER_processed_affdyn)

## Mixed model ##
auto_NA <- lmer(NAff ~ 1 + NAff_lag + (1 + NAff_lag | ID), 
                     data = dataset6_LASER_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_NA)

auto_Bip_aff <- lmer(Bip_aff ~ 1 + Bip_aff_lag + (1 + Bip_aff_lag | ID), 
                     data = dataset6_LASER_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_Bip_aff)

## Extract individual slopes ##
beta_NA<-tibble::rownames_to_column(coef(auto_NA)$ID)
beta_NA<-plyr::rename(beta_NA, 
                c("NAff_lag" = "autocorNA",
                  "rowname" =  "ID"))

beta_Bip_aff<-tibble::rownames_to_column(coef(auto_Bip_aff)$ID)
beta_Bip_aff<-plyr::rename(beta_Bip_aff, 
                c("Bip_aff_lag" = "autocorBip_aff",
                  "rowname" =  "ID"))


##################### Merge data #####################
aff_dyn_LASER <- merge(aff_dyn_LASER,beta_NA,by="ID",all=TRUE)
aff_dyn_LASER <- merge(aff_dyn_LASER,beta_Bip_aff,by="ID",all=TRUE)

##################### Delete intercept from aff_dyn files #####################
aff_dyn_LASER<-subset(aff_dyn_LASER, select=c(1:10,12,14))
names(aff_dyn_LASER)

```
### Merging

```{r LASER merging, cache=FALSE}

##################### restructure depression/demographics data #####################
#The dataset was in longformat, however the affective dynamics measures are in wide format
#Thus, I restructure the dataset (which only contain time-invariant variables anyway)
#and merge with the affective dynamics data
data_id_dep_LASER <- subset(dataset6_LASER_processed, select=c("ID","Sex","Age","DepB"))
data_id_dep_LASER<-data_id_dep_LASER %>% group_by(ID) %>% summarise_all(mean)

##################### Merge affective dynamics and depression/demographics data #####################
datasetLASER_final <- merge(data_id_dep_LASER,aff_dyn_LASER,by="ID",all=TRUE)
names(datasetLASER_final)

```


### Winzorising and transformation of skewed variables

```{r LASER transformation + winsorizing, cache=FALSE}

##################### winsorizing #####################
names(datasetLASER_final)

datasetLASER_final<-datasetLASER_final[order(datasetLASER_final$ID) , ]

psych::describe(datasetLASER_final)

dataLASER_finalW<-data.frame(datasetLASER_final)

#select predictors and outcome variables (affective dynamics and depression)
predictor_wins<-c(4,6,7,9:15)

#winsorizing
dataLASER_finalW<-apply(dataLASER_finalW[,predictor_wins], 
                                 2,  
                                 Winsorize, probs=c(0.01,0.99), na.rm = TRUE)

dataLASER_finalW<-data.frame(dataLASER_finalW)

psych::describe(dataLASER_finalW)               

#add id number
dataLASER_finalW$ID <- datasetLASER_final$ID

#make subset of variables that were not included in the winsorizing
sub1<-subset(datasetLASER_final, select=c(1:3,5,8))

#Merge files
dataLASER_finalW <- merge(dataLASER_finalW,sub1,by="ID",all=TRUE)

##################### Transforming ###################### 
#if skewness is > 3 (Kline, 2011)
#### identifying ####
skew<-skew(select(dataLASER_finalW, -ID))>3
skew

length(skew[skew== TRUE])

#no transformations necessary

names(dataLASER_finalW)


```

### Center/standardize study variables

- Center of predictors (for moderation)

- Standardization of the depression scale

```{r LASER center predictors, cache=FALSE}

names(dataLASER_finalW)

psych::describe(dataLASER_finalW)

##################### center predictors #####################
predictor_scale<-c(3:11)

dataLASER_finalWC<-apply(dataLASER_finalW[,predictor_scale], 
                           2,  
                           scale, scale = FALSE)

dataLASER_finalWC<-data.frame(dataLASER_finalWC)

psych::describe(dataLASER_finalWC)               

#add id number
dataLASER_finalWC$ID<- dataLASER_finalW$ID

#make subset of variables that were not included in the standardization
sub2<-subset(dataLASER_finalW, select=c(1:2,12:15))

#Merge files
dataLASER_finalWC <- merge(dataLASER_finalWC,sub2,by="ID",all=TRUE)

##################### standardize Depression #####################
dataLASER_finalWC$DepB<-scale(dataLASER_finalWC$DepB, center = TRUE, scale = TRUE)
psych::describe(dataLASER_finalWC)

names(dataLASER_finalWC)

```


### Write cleaned datasets 

The "raw" (i.e., not centered & standardized) and centered/standardized versions.

```{r LASER write datasets, cache=FALSE}
write.csv(dataLASER_finalW, here::here("data", "3_cleaned_data_for_OSF", "dataset6_LASER_clean.csv"), row.names = FALSE)
write.csv(dataLASER_finalWC, here::here("data", "3_cleaned_data_for_OSF", "dataset6_LASER_clean_cen_std.csv"), row.names = FALSE)

```
## DATASET 7: YES

For the YES dataset, there were only 2 emotion items assessed. First, participants had to rate their overall mood on a scale from -10 to +10 (very negative to very positive). This is what is referred to as Bipolar Affect in this paper (BA). Second, participants had to rate only their most negative emotion (selected from sadness, anger and anxiety). 


Given that there is only 1 items for NA, we cannot calculate MCFA's and alpha and omega. We can calculate the ICCs.

### Read in data and datapreparation

```{r YES read in data, cache=FALSE}

####################### Read in data #######################
dataset7_YES_processed<-read.csv(here::here("data", "2_processed_data_for_OSF", "dataset7_YES_processed.csv"), sep=",",
                                na.strings="NA")

####################### number of participants with valid ESM data ##################### 
dataset7_YES_processed$count<-calc.nomiss(NAff, ID, dataset7_YES_processed, prop=FALSE, expand=TRUE)
range(dataset7_YES_processed$count)

# exclude those with not valid daily diary data
n_YES_orig<-nsub(ID, dataset7_YES_processed)
n_YES_orig

dataset7_YES_processed <-subset(dataset7_YES_processed, count > 0) 
range(dataset7_YES_processed$count)

n_YES_orig<-nsub(ID, dataset7_YES_processed)
n_YES_orig

```
### Between-person reliabilities

No calculation of within-person reliabilities on the emotion items were possible here, since only 1 item available for NA and BA.
So, we only calculate reliability of the BDI here.

```{r YES reliabilities, cache=FALSE}

# make new dataset into wide format with only depression scores
dataset7_YES_dep <- 
  dataset7_YES_processed %>%
  select("ID", starts_with('CDI', ignore.case = FALSE))

# reshape into wideformat (note: depression is constant, so I summarize across time-points)
dataset7_YES_dep_wide<-dataset7_YES_dep %>% group_by(ID) %>% summarise_all(mean)

names(dataset7_YES_dep_wide)

##################### Depression #####################
which(colnames(dataset7_YES_dep_wide)=="CDI1_01") # dep item 1 - baseline
which(colnames(dataset7_YES_dep_wide)=="CDI1_26") # dep item 26 - baseline
which(colnames(dataset7_YES_dep_wide)=="CDI2_01") # dep item 1 - follow-up
which(colnames(dataset7_YES_dep_wide)=="CDI2_26") # dep item 26 - follow-up

#### Baseline ####
alpha_DepB_YES<-psych::alpha(dataset7_YES_dep_wide[c(2:27)], check.keys=TRUE)
alpha_DepB_YES

#### Follow-up ####
alpha_DepF_YES<-psych::alpha(dataset7_YES_dep_wide[c(41:66)], check.keys=TRUE)
alpha_DepF_YES

#negative items were correctly recoded by alpha function

```

### Intra-class correlations

```{r YES ICC, cache=FALSE}

##################### delete observations with 0 variance #####################
dataset7_YES_processed1<-dataset7_YES_processed
dataset7_YES_processed1 <- check.timeinvar(NAff, ID, dataset7_YES_processed1, out = 3)


##################### ICCs #####################

#### NA #### 
NA_ICC <- lmer(NAff ~ 1 + (1 | ID), data = dataset7_YES_processed1)
NA_ICCr_YES <- performance::icc(NA_ICC)
NA_ICCr_YES

#### Bipolar Affect scale #### 
Bip_aff_ICC <- lmer(Bip_aff ~ 1 + (1 | ID), data = dataset7_YES_processed1)
Bip_aff_ICCr_YES <- performance::icc(Bip_aff_ICC)
Bip_aff_ICCr_YES
```
### Exclusion and Compliance
```{r YES exclusion and compliance, cache=FALSE}

##################### Details study design #####################
#### Total beeps ####
tot_ass_YES<-42
#### Total days ####
tot_days_YES <- 14
#### Total beeps per day ####
tot_b_d_YES<-3

#### Number of valid beeps before exclusion #### 
mean(calc.mean(count, ID, data=dataset7_YES_processed)) 

#### Percentage of valid beeps before exclusion #### 
dataset7_YES_processed$perc<-(100/tot_ass_YES)*dataset7_YES_processed$count
mean(calc.mean(perc, ID, data=dataset7_YES_processed)) 

#### Number of participants before exclusion ####
n_YES<-nsub(ID, dataset7_YES_processed)
n_YES

#### Removing participants with compliance below 33% #### 

## one-third of beeps ##
valbeep_YES <- (tot_ass_YES/100)*33
valbeep_YES

## excluding those with fewer than 33% ##
dataset7_YES_processed <-subset(dataset7_YES_processed, count > valbeep_YES) 
nsub(ID, dataset7_YES_processed)



#### Delete participants without any variance on NAff and PAff ###
## n before ##
nsub(ID, dataset7_YES_processed)

#### Delete participants without any variance on NAff and PAff ####
dataset7_YES_processed <- check.timeinvar(NAff, ID, dataset7_YES_processed, out = 3)
dataset7_YES_processed <- check.timeinvar(Bip_aff, ID, dataset7_YES_processed, out = 3)

## n after ##
nsub(ID, dataset7_YES_processed) #1 participant excluded

#### final sample ####
n_fin_YES<-nsub(ID, dataset7_YES_processed)
n_fin_YES
perc_inc_YES <- (100/n_YES_orig)*n_fin_YES
perc_inc_YES

#### mean of valid beeps per participant ####
mean_comp_YES<-mean(calc.mean(count, ID, data=dataset7_YES_processed)) 
mean_comp_YES 

#### percentage of valid beeps per participant ####
mean_com_per_YES<-mean(calc.mean(perc, ID, data=dataset7_YES_processed)) 
mean_com_per_YES


```


### Calculation of affective dynamics

#### Datapreparation

```{r YES number affective dynamics dataprep, cache=FALSE}

##################### Deleting rows with missings on days #####################
#There were some rows with missing on days and also affect. Those were deleted
dataset7_YES_processed<-dataset7_YES_processed[!is.na(dataset7_YES_processed$Day),]

##################### sort by ID and time #####################
dataset7_YES_processed<-dataset7_YES_processed[order(dataset7_YES_processed$ID,dataset7_YES_processed$time) , ]

##################### subset most important variables #####################
dataset7_YES_processed_affdyn <- subset(dataset7_YES_processed, select=c("ID","time","Day","NAff","Bip_aff"))

##################### delete missing values ##################### 
#so that differences are also calculated between beeps that were
#not directly next to each other (e.g., 3, 4(NA), 5) 
#I still want to calculate the difference between 3 and 5
dataset7_YES_processed_affdyn<-dataset7_YES_processed_affdyn[!is.na(dataset7_YES_processed_affdyn$NAff),]

##################### make a new time variable #####################
dataset7_YES_processed_affdyn<-dataset7_YES_processed_affdyn %>% group_by(ID) %>% dplyr::mutate(time = rank(time, ties.method = 'first'))%>% arrange(time)

##################### add missing days after each week ##################### 
#this is important, because for the MSSD, we do not want to calculate the
#difference between the last beep of one day and the first beep of the next day/week
dataset7_YES_processed_affdyn_NA <-insert_NA(dataset7_YES_processed_affdyn, ID, Day)


```

#### Calculation 
```{r YES affective dynamics calc, cache=FALSE}


##################### Calculate affective dynamics #####################
#### Mean, SD ####

aff_dyn_YES <- ddply(dataset7_YES_processed_affdyn_NA, "ID",  plyr::summarize,
                           icountNA = sum(!is.na(NAff)),  #count of observations
                           imeanNA  = mean(NAff, na.rm=TRUE), #imean (continuous)
                           isdNA   = sd(NAff, na.rm=TRUE), #isd (continuous)
                           icountBip = sum(!is.na(Bip_aff)),  #count of observations
                           imeanBip_aff  = mean(Bip_aff, na.rm=TRUE), #imean (continuous)
                           isdBip_aff   = sd(Bip_aff, na.rm=TRUE),   #isd   (continuous)  
                           relisdNA = relativeSD(NAff, 1, 9)) #relative isd (continuous)


#### MSSD ####

mssd.stats.NA <- ddply(dataset7_YES_processed_affdyn_NA, "ID", plyr::summarize, iMSSDNA=my.mssd(NAff))
mssd.stats.Bip_aff <- ddply(dataset7_YES_processed_affdyn_NA, "ID", plyr::summarize, iMSSDBip_aff=my.mssd(Bip_aff))

aff_dyn_YES <- merge(aff_dyn_YES,mssd.stats.NA,by="ID",all=TRUE)
aff_dyn_YES <- merge(aff_dyn_YES,mssd.stats.Bip_aff,by="ID",all=TRUE)


#### autocorrelation ####


## Lagged effect ##

dataset7_YES_processed_affdyn$NAff_lag<-lagvar(NAff, id=ID, obs=time, day=Day,data=dataset7_YES_processed_affdyn)
dataset7_YES_processed_affdyn$Bip_aff_lag<-lagvar(Bip_aff, id=ID, obs=time, day=Day,data=dataset7_YES_processed_affdyn)

## Mixed model ##
auto_NA <- lmer(NAff ~ 1 + NAff_lag + (1 + NAff_lag | ID), 
                     data = dataset7_YES_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_NA)

auto_Bip_aff <- lmer(Bip_aff ~ 1 + Bip_aff_lag + (1 + Bip_aff_lag | ID), 
                     data = dataset7_YES_processed_affdyn, REML = TRUE,
                     control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
summary(auto_Bip_aff)

## Extract individual slopes ##
beta_NA<-tibble::rownames_to_column(coef(auto_NA)$ID)
beta_NA<-plyr::rename(beta_NA, 
                c("NAff_lag" = "autocorNA",
                  "rowname" =  "ID"))

beta_Bip_aff<-tibble::rownames_to_column(coef(auto_Bip_aff)$ID)
beta_Bip_aff<-plyr::rename(beta_Bip_aff, 
                c("Bip_aff_lag" = "autocorBip_aff",
                  "rowname" =  "ID"))

##################### Merge data #####################
aff_dyn_YES <- merge(aff_dyn_YES,beta_NA,by="ID",all=TRUE)
aff_dyn_YES <- merge(aff_dyn_YES,beta_Bip_aff,by="ID",all=TRUE)

##################### Delete intercept from aff_dyn files #####################
aff_dyn_YES<-subset(aff_dyn_YES, select=c(1:10,12,14))
names(aff_dyn_YES)


```
### Merging

```{r YES merging, cache=FALSE}

##################### restructure depression/demographics data #####################
#The dataset was in longformat, however the affective dynamics measures are in wide format
#Thus, I restructure the dataset (which only contain time-invariant variables anyway)
#and merge with the affective dynamics data
data_id_dep_YES <- subset(dataset7_YES_processed, select=c("ID","Sex","Age","DepB","DepF"))
data_id_dep_YES<-data_id_dep_YES %>% group_by(ID) %>% summarise_all(mean)

##################### Merge affective dynamics and depression/demographics data #####################
datasetYES_final <- merge(data_id_dep_YES,aff_dyn_YES,by="ID",all=TRUE)
names(datasetYES_final)


```


### Winzorising and transformation of skewed variables

```{r YES transformation + winsorizing, cache=FALSE}

##################### winsorizing #####################
names(datasetYES_final)

datasetYES_final<-datasetYES_final[order(datasetYES_final$ID) , ]

psych::describe(datasetYES_final)

dataYES_finalW<-data.frame(datasetYES_final)

#select predictors and outcome variables (affective dynamics and depression)
predictor_wins<-c(4,5,7,8,10:16)

#winsorizing
dataYES_finalW<-apply(dataYES_finalW[,predictor_wins], 
                                 2,  
                                 Winsorize, probs=c(0.01,0.99), na.rm = TRUE)

dataYES_finalW<-data.frame(dataYES_finalW)

psych::describe(dataYES_finalW)               

#add id number
dataYES_finalW$ID <- datasetYES_final$ID

#make subset of variables that were not included in the winsorizing
sub1<-subset(datasetYES_final, select=c(1:3,6,9))

#Merge files
dataYES_finalW <- merge(dataYES_finalW,sub1,by="ID",all=TRUE)

##################### Transforming ###################### 
#if skewness is > 3 (Kline, 2011)
#### identifying ####
skew<-skew(select(dataYES_finalW, -ID))>3
skew

length(skew[skew== TRUE])

#no transformations necessary

names(dataYES_finalW)


```

### Center/standardize study variables

- Center of predictors (for moderation)

- Standardization of the depression scale

```{r YES center predictors, cache=FALSE}

names(dataYES_finalW)

psych::describe(dataYES_finalW)


##################### center predictors #####################
predictor_scale<-c(4:12)

dataYES_finalWC<-apply(dataYES_finalW[,predictor_scale], 
                           2,  
                           scale, scale = FALSE)

dataYES_finalWC<-data.frame(dataYES_finalWC)

psych::describe(dataYES_finalWC)               

#add id number
dataYES_finalWC$ID<- dataYES_finalW$ID

#make subset of variables that were not included in the standardization
sub2<-subset(dataYES_finalW, select=c(1:3,13:16))

#Merge files
dataYES_finalWC <- merge(dataYES_finalWC,sub2,by="ID",all=TRUE)

##################### standardize Depression #####################
dataYES_finalWC$DepB<-scale(dataYES_finalWC$DepB, center = TRUE, scale = TRUE)
dataYES_finalWC$DepF<-scale(dataYES_finalWC$DepF, center = TRUE, scale = TRUE)

psych::describe(dataYES_finalWC)


names(dataYES_finalWC)


```




### Write cleaned datasets 

The "raw" (i.e., not centered & standardized) and centered/standardized versions.

```{r YES write datasets, cache=FALSE}
write.csv(dataYES_finalW, here::here("data", "3_cleaned_data_for_OSF", "dataset7_YES_clean.csv"), row.names = FALSE)
write.csv(dataYES_finalWC, here::here("data", "3_cleaned_data_for_OSF", "dataset7_YES_clean_cen_std.csv"), row.names = FALSE)

```
# MERGING ALL DATASETS


## Raw data

The raw data I need for showing the descriptives (means and SD's)

```{r mega-analysis merging raw, cache=FALSE}
####################### read in datasets ####################### 
dataRADAR_finalW<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset1_RADAR_clean.csv"), sep=",", na.strings="NA")
dataSM_finalW<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset2_SM_clean.csv"), sep=",", na.strings="NA")
dataMA_finalW<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset3_MA_clean.csv"), sep=",", na.strings="NA")
dataED_finalW<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset4_ED_clean.csv"), sep=",", na.strings="NA")
dataEA_finalW<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset5_EA_clean.csv"), sep=",", na.strings="NA")
dataLASER_finalW<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset6_LASER_clean.csv"), sep=",", na.strings="NA")
dataYES_finalW<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset7_YES_clean.csv"), sep=",", na.strings="NA")

####################### add unique study number ####################### 
dataRADAR_finalW$study <- 1
dataSM_finalW$study <- 2
dataMA_finalW$study <- 3
dataED_finalW$study <- 4
dataEA_finalW$study <- 5
dataLASER_finalW$study <- 6
dataYES_finalW$study <- 7

####################### merge all datasets ####################### 
mega_all <- dplyr::bind_rows(dataRADAR_finalW, dataSM_finalW,dataMA_finalW,dataED_finalW,dataEA_finalW,dataLASER_finalW,dataYES_finalW)

#### sex and study as factor ####
mega_all$Sex <- as.factor(mega_all$Sex)
mega_all$study <- as.factor(mega_all$study)

#### icount as numeric ####
mega_all$icountNA <- as.numeric(mega_all$icountNA)
mega_all$icountPA <- as.numeric(mega_all$icountPA)
mega_all$icountBip <- as.numeric(mega_all$icountBip)

#### add new ID number ####
mega_all$MegaID <- 1:length(mega_all$ID)

#### Make dummy variables from study ####
mega_all <- dummy_cols(mega_all, select_columns = 'study')

str(mega_all)

#check for NaN's
df <- as.data.frame(cbind(lapply(lapply(mega_all, is.nan), sum)))
rownames(subset(df, df$V1 != 0))

####################### make mega dataset excluding datasets 6 and 7  ####################### 
# As explained in the paper, we ran analyses for all datasets and where dataset 6 and 7 were excluded
# So, there I am making a new dataset excluding those studies
mega_all_5 <-subset(mega_all, study == "1" | study == "2" | study == "3" | study == "4" | study == "5")


####################### write out files different datasets  ####################### 
write.csv(mega_all, here::here("data", "3_cleaned_data_for_OSF", "dataset_mega_all.csv"), row.names = FALSE)
write.csv(mega_all_5, here::here("data", "3_cleaned_data_for_OSF", "dataset_mega_all_5.csv"), row.names = FALSE)

```




## Centered and standardized data

The centered and standardized data I need for main analyses

```{r mega-analysis merging cen std, cache=FALSE}
####################### read in datasets ####################### 
dataRADAR_finalWC<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset1_RADAR_clean_cen_std.csv"), sep=",", na.strings="NA")
dataSM_finalWC<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset2_SM_clean_cen_std.csv"), sep=",", na.strings="NA")
dataMA_finalWC<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset3_MA_clean_cen_std.csv"), sep=",", na.strings="NA")
dataED_finalWC<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset4_ED_clean_cen_std.csv"), sep=",", na.strings="NA")
dataEA_finalWC<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset5_EA_clean_cen_std.csv"), sep=",", na.strings="NA")
dataLASER_finalWC<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset6_LASER_clean_cen_std.csv"), sep=",", na.strings="NA")
dataYES_finalWC<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset7_YES_clean_cen_std.csv"), sep=",", na.strings="NA")

####################### add unique study number ####################### 
dataRADAR_finalWC$study <- 1
dataSM_finalWC$study <- 2
dataMA_finalWC$study <- 3
dataED_finalWC$study <- 4
dataEA_finalWC$study <- 5
dataLASER_finalWC$study <- 6
dataYES_finalWC$study <- 7

####################### merge all datasets ####################### 
mega_all_cen_std<- dplyr::bind_rows(dataRADAR_finalWC, dataSM_finalWC,dataMA_finalWC,dataED_finalWC,dataEA_finalWC,dataLASER_finalWC,dataYES_finalWC)

#### sex and study as factor ####
mega_all_cen_std$Sex <- as.factor(mega_all_cen_std$Sex)
mega_all_cen_std$study <- as.factor(mega_all_cen_std$study)

#### icount as numeric ####
mega_all_cen_std$icountNA <- as.numeric(mega_all_cen_std$icountNA)
mega_all_cen_std$icountPA <- as.numeric(mega_all_cen_std$icountPA)
mega_all_cen_std$icountBip <- as.numeric(mega_all_cen_std$icountBip)

#### add new ID number ####
mega_all_cen_std$MegaID <- 1:length(mega_all_cen_std$ID)

#### Make dummy variables from study ####
mega_all_cen_std<- dummy_cols(mega_all_cen_std, select_columns = 'study')

str(mega_all_cen_std)



#check for NaN's
df <- as.data.frame(cbind(lapply(lapply(mega_all_cen_std, is.nan), sum)))
rownames(subset(df, df$V1 != 0))

####################### make mega dataset excluding datasets 6 and 7  ####################### 
# As explained in the paper, we ran analyses for all datasets and where dataset 6 and 7 were excluded
# So, there I am making a new dataset excluding those studies
mega_all_cen_std_5 <-subset(mega_all_cen_std, study == "1" | study == "2" | study == "3" | study == "4" | study == "5")


####################### write out files different datasets  ####################### 
write.csv(mega_all_cen_std, here::here("data", "3_cleaned_data_for_OSF", "dataset_mega_all_cen_std.csv"), row.names = FALSE)
write.csv(mega_all_cen_std_5, here::here("data", "3_cleaned_data_for_OSF", "dataset_mega_all_cen_std_5.csv"), row.names = FALSE)
```




# DESCRIPTIVES ALL STUDIES
```{r read in datasets, cache=FALSE}
####################### read in datasets ####################### 
mega_all<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset_mega_all.csv"), sep=",", na.strings="NA")
mega_all_5<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset_mega_all_5.csv"), sep=",", na.strings="NA")
mega_all_cen_std<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset_mega_all_cen_std.csv"), sep=",", na.strings="NA")
mega_all_cen_std_5<-read.csv(here::here("data", "3_cleaned_data_for_OSF", "dataset_mega_all_cen_std_5.csv"), sep=",", na.strings="NA")

#### sex and study as factor ####
mega_all$Sex <- as.factor(mega_all$Sex)
mega_all_5$Sex <- as.factor(mega_all_5$Sex)
mega_all_cen_std$Sex <- as.factor(mega_all_cen_std$Sex)
mega_all_cen_std_5$Sex <- as.factor(mega_all_cen_std_5$Sex)

mega_all$study <- as.factor(mega_all$study)
mega_all_5$study <- as.factor(mega_all_5$study)
mega_all_cen_std$study <- as.factor(mega_all_cen_std$study)
mega_all_cen_std_5$study <- as.factor(mega_all_cen_std_5$study)

#### icount as numeric ####
mega_all$icountNA <- as.numeric(mega_all$icountNA)
mega_all$icountPA <- as.numeric(mega_all$icountPA)
mega_all$icountBip <- as.numeric(mega_all$icountBip)

mega_all_5$icountNA <- as.numeric(mega_all_5$icountNA)
mega_all_5$icountPA <- as.numeric(mega_all_5$icountPA)
mega_all_5$icountBip <- as.numeric(mega_all_5$icountBip)

mega_all_cen_std$icountNA <- as.numeric(mega_all_cen_std$icountNA)
mega_all_cen_std$icountPA <- as.numeric(mega_all_cen_std$icountPA)
mega_all_cen_std$icountBip <- as.numeric(mega_all_cen_std$icountBip)

mega_all_cen_std_5$icountNA <- as.numeric(mega_all_cen_std_5$icountNA)
mega_all_cen_std_5$icountPA <- as.numeric(mega_all_cen_std_5$icountPA)
mega_all_cen_std_5$icountBip <- as.numeric(mega_all_cen_std_5$icountBip)

```
## Demographics

```{r demographics, cache=FALSE}
####################### Age #######################
#### overall ####
psych::describe(mega_all$Age)

#### per study ####
describeBy(mega_all$Age,mega_all$study)

####################### Sex #######################
#### exclude NA's ####
mega_all_sex<-mega_all[!is.na(mega_all$Sex),]

#### overall ####
mega_all_sex %>%
  group_by(Sex) %>%
   dplyr::summarise(n_sex = n()) %>%
   dplyr::mutate(share = n_sex / sum(n_sex))

#### per study ####
mega_all_sex %>%
  group_by(study,Sex) %>%
   dplyr::summarise(n_sex = n()) %>%
   dplyr::mutate(share = n_sex / sum(n_sex))



```

## Total number of days/weeks

```{r Total Number of Days/Weeks, cache=FALSE, echo=FALSE}


days_weeks<-data.frame(Study=c("RADAR", "Swinging Moods", "Mood in Emerging Adults", "Emotions in daily life", "Emotion Regulation in Action",  "LASER","YES"),
               Number=c(tot_ass_RADAR,tot_days_SM,tot_days_MA,tot_days_ED,tot_days_EA,tot_days_LASER,tot_days_YES))

pander(days_weeks,caption = "Total Number of Days/Weeks")

```

## Number of beeps per day

```{r N beeps per day, cache=FALSE, echo=FALSE}


days_weeks<-data.frame(Study=c("RADAR", "Swinging Moods", "Mood in Emerging Adults", "Emotions in daily life", "Emotion Regulation in Action",  "LASER","YES"),
                       Number=c(tot_b_d_RADAR,tot_b_d_SM,tot_b_d_MA,tot_b_d_ED,"4 Fri; 9 Sat/Sun",tot_b_d_LASER,tot_b_d_YES))

pander(days_weeks,caption = "Number of beeps per day")

```


## Total number of beeps
```{r all beeps, cache=FALSE, echo=FALSE}
beeps<-data.frame(Study=c("RADAR", "Swinging Moods", "Mood in Emerging Adults", "Emotions in daily life", "Emotion Regulation in Action", "LASER", "YES"),
               Beeps=c(tot_ass_RADAR,tot_ass_SM,tot_ass_MA,tot_ass_ED,tot_ass_EA,tot_ass_LASER,tot_ass_YES))
                       
pander(beeps,caption = "Total number of beeps")
```

## Sample size before exclusion compliance

```{r before exclusion n, cache=FALSE, echo=FALSE}

totaln_inds_orig<-c(n_RADAR_orig,n_SM_orig,n_MA_orig,n_ED_orig,n_EA_orig,n_LASER_orig,n_YES_orig)
totaln_orig <- sum(n_RADAR_orig,n_SM_orig,n_MA_orig,n_ED_orig,n_EA_orig,n_LASER_orig,n_YES_orig)

finaln<-data.frame(Study=c("RADAR", "Swinging Moods", "Mood in Emerging Adults", "Emotions in daily life", "Emotion Regulation in Action","LASER", "YES",  "Total Sample Size"),
               final_n=c(n_RADAR_orig,n_SM_orig,n_MA_orig,n_ED_orig,n_EA_orig,n_LASER_orig,n_YES_orig,totaln_orig))
                       
pander(finaln,caption = "Sample size before exclusion compliance")

round(mean(totaln_inds_orig),digits=0)
range(totaln_inds_orig)

```


## Final sample size 

```{r final n, cache=FALSE, echo=FALSE}

totaln_inds<-c(n_fin_RADAR,n_fin_SM,n_fin_MA,n_fin_ED,n_fin_EA,n_fin_LASER,n_fin_YES,n_fin_LASER)
totaln <- sum(n_fin_RADAR,n_fin_SM,n_fin_MA,n_fin_ED,n_fin_EA,n_fin_LASER,n_fin_YES)

finaln<-data.frame(Study=c("RADAR", "Swinging Moods", "Mood in Emerging Adults", "Emotions in daily life", "Emotion Regulation in Action","LASER", "YES",  "Total Sample Size"),
               final_n=c(n_fin_RADAR,n_fin_SM,n_fin_MA,n_fin_ED,n_fin_EA,n_fin_LASER,n_fin_YES,totaln))
                       
pander(finaln,caption = "Final sample size")

round(mean(totaln_inds),digits=0)
range(totaln_inds)

```

## Percentage of Included Cases

This is the percentage of included cases from the initial sample per dataset after excluding participants with fewer than 33% and 0 variance (the latter, there were only 3 participants excluded overall)

```{r exclusion, cache=FALSE, echo=FALSE}

inc<-data.frame(study=c("RADAR", "Swinging Moods", "Mood in Emerging Adults","Emotions in daily life", "Emotion Regulation in Action", "LASER", "YES"),
               final_p=round(c(perc_inc_RADAR,perc_inc_SM,perc_inc_MA,perc_inc_ED,perc_inc_EA,perc_inc_LASER,perc_inc_YES), digits = 1))

pander(inc,caption = "Percentage of Included Cases")


mean(inc$final_p)
range(inc$final_p)

# How many were excluded overall?
(100/totaln_orig)*totaln

```

## Compliance

### Percentage of answered beeps

```{r compliance percentage, cache=FALSE, echo=FALSE}
com_p<-data.frame(study=c("RADAR", "Swinging Moods", "Mood in Emerging Adults","Emotions in daily life", "Emotion Regulation in Action",  "LASER","YES"),
               beep_p=round(c(mean_com_per_RADAR,mean_com_per_SM,mean_com_per_MA,mean_com_per_ED,mean_com_per_EA,mean_com_per_LASER,mean_com_per_YES), digits=1))

pander(com_p,caption = "Percentage of answered beeps")

mean(com_p$beep_p)
range(com_p$beep_p)

```

### Mean number of answered beeps

```{r compliance, cache=FALSE, echo=FALSE}
com_m<-data.frame(study=c("RADAR", "Swinging Moods", "Mood in Emerging Adults","Emotions in daily life", "Emotion Regulation in Action", "LASER", "YES"),
               mean_compliance=round(c(mean_comp_RADAR, mean_comp_SM,mean_comp_MA,mean_comp_ED,mean_comp_EA,mean_comp_LASER,mean_comp_YES),digits=1))
                       
pander(com_m,caption = "Mean number of answered beeps")

```

### Association with study variables

```{r compliance influence, cache=FALSE}
##################### dataframes with sample size for metafor #####################
#Note: Not all studies have all variables available (e.g., PA or Depression FU data)

nstudy7 <- c(n_fin_RADAR,n_fin_SM,n_fin_MA,n_fin_ED,n_fin_EA,n_fin_LASER,n_fin_YES)

##################### all 7 studies #####################

#Compliance and Sex
df <- mega_all_cen_std %>%
                group_by(study) %>% 
                dplyr::summarise(m_male_i = mean(icountNA[Sex==0], na.rm="TRUE"),
                          m_female_i = mean(icountNA[Sex==1], na.rm="TRUE"),
                          sd_male_i = sd(icountNA[Sex==0], na.rm="TRUE"),
                          sd_female_i = sd(icountNA[Sex==1], na.rm="TRUE"),
                          n_male_i = length(icountNA[Sex==0]),
                          n_female_i = length(icountNA[Sex==1]))

head(df,n=7)

meta_esc <- escalc(measure="SMD", m1i=m_male_i, m2i=m_female_i, sd1i=sd_male_i, sd2i=sd_female_i, n1i=n_male_i, n2i=n_female_i, data=df)
meta <- rma(meta_esc,method ='REML')
forest(meta)

coef(summary(meta))$estimate
coef(summary(meta))$pval

#Compliance and other continous study variables
df <- mega_all_cen_std %>%
                group_by(study) %>% 
                dplyr::summarise(r_age = cor(icountNA,Age, use="na.or.complete"),
                                 r_meanNA = cor(icountNA,imeanNA, use="na.or.complete"),
                                 r_sdNA = cor(icountNA,isdNA, use="na.or.complete"),
                                 r_mssdNA = cor(icountNA,iMSSDNA, use="na.or.complete"),
                                 r_autoNA = cor(icountNA,autocorNA, use="na.or.complete"),
                                 r_meanPA = cor(icountNA,imeanPA, use="na.or.complete"),
                                 r_sdPA = cor(icountNA,isdPA, use="na.or.complete"),
                                 r_mssdPA = cor(icountNA,iMSSDPA, use="na.or.complete"),
                                 r_autoPA = cor(icountNA,autocorPA, use="na.or.complete"),
                                 r_DepB = cor(icountNA,DepB, use="na.or.complete"),
                                 r_DepF = cor(icountNA,DepF, use="na.or.complete"))
df <-cbind(df,nstudy7)

#fit model
meta_esc_age <-escalc(measure ='COR', ri = df$r_age, ni = df$nstudy7)
meta_esc_meanNA <-escalc(measure ='COR', ri = df$r_meanNA, ni = df$nstudy7)
meta_esc_sdNA <-escalc(measure ='COR', ri = df$r_sdNA, ni = df$nstudy7)
meta_esc_mssdNA <-escalc(measure ='COR', ri = df$r_mssdNA, ni = df$nstudy7)
meta_esc_autoNA <-escalc(measure ='COR', ri = df$r_autoNA, ni = df$nstudy7)
meta_esc_meanPA <-escalc(measure ='COR', ri = df$r_meanPA, ni = df$nstudy7)
meta_esc_sdPA <-escalc(measure ='COR', ri = df$r_sdPA, ni = df$nstudy7)
meta_esc_mssdPA <-escalc(measure ='COR', ri = df$r_mssdPA, ni = df$nstudy7)
meta_esc_autoPA <-escalc(measure ='COR', ri = df$r_autoPA, ni = df$nstudy7)
meta_esc_DepB <-escalc(measure ='COR', ri = df$r_DepB, ni = df$nstudy7)
meta_esc_DepF <-escalc(measure ='COR', ri = df$r_DepF, ni = df$nstudy7)

meta_age <- rma(meta_esc_age,method ='REML')
meta_meanNA <- rma(meta_esc_meanNA,method ='REML')
meta_sdNA <- rma(meta_esc_sdNA,method ='REML')
meta_mssdNA <- rma(meta_esc_mssdNA,method ='REML')
meta_autoNA <- rma(meta_esc_autoNA,method ='REML')
meta_meanPA <- rma(meta_esc_meanPA,method ='REML')
meta_sdPA <- rma(meta_esc_sdPA,method ='REML')
meta_mssdPA <- rma(meta_esc_mssdPA,method ='REML')
meta_autoPA <- rma(meta_esc_autoPA,method ='REML')
meta_DepB <- rma(meta_esc_DepB,method ='REML')
meta_DepF <- rma(meta_esc_DepF,method ='REML')

#make dataframe
var_comp <- c("age","meanPA","sdPA","MSSDPA","autoPA",
              "meanNA","sdNA","MSSDNA","autoNA",
              "DepB", "DepF")

cor_comp <-round(c(coef(summary(meta_age))$estimate,
                coef(summary(meta_meanPA))$estimate,
                coef(summary(meta_sdPA))$estimate,
                coef(summary(meta_mssdPA))$estimate,
                coef(summary(meta_autoPA))$estimate,                
                coef(summary(meta_meanNA))$estimate,
                coef(summary(meta_sdNA))$estimate,
                coef(summary(meta_mssdNA))$estimate,
                coef(summary(meta_autoNA))$estimate,
                coef(summary(meta_DepB))$estimate,
                coef(summary(meta_DepF))$estimate),digits = 2)

p_comp <-round(c(coef(summary(meta_age))$pval,
                coef(summary(meta_meanPA))$pval,
                coef(summary(meta_sdPA))$pval,
                coef(summary(meta_mssdPA))$pval,
                coef(summary(meta_autoPA))$pval,
                coef(summary(meta_meanNA))$pval,
                coef(summary(meta_sdNA))$pval,
                coef(summary(meta_mssdNA))$pval,
                coef(summary(meta_autoNA))$pval,
                coef(summary(meta_DepB))$pval,
                coef(summary(meta_DepF))$pval),digits = 3)


comp_study<-data.frame(var_comp,cor_comp,p_comp)

pander(comp_study,caption = "Correlation between compliance and study variables")

```

## Histograms

### RADAR

```{r histograms RADAR, echo=TRUE} 
####################### RADAR #######################

#### Depression ####

a<-ggplot(dataRADAR_finalW, aes(x=DepB))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms baseline") +
  xlim(1, 4) +
  ylim(0, 100) +
  theme(text = element_text(size = 15))  

b<-ggplot(dataRADAR_finalW, aes(x=DepF))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms follow-up") +
  xlim(1, 4)+
  ylim(0, 100) +
  theme(text = element_text(size = 15))  

#### Affective Dynamics ####
c<-ggplot(dataRADAR_finalW, aes(x=imeanNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA level") +
  xlim(1, 7.5) +
  ylim(0, 100) +
  theme(text = element_text(size = 15))  


d<-ggplot(dataRADAR_finalW, aes(x=isdNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA variability") +
  xlim(0, 3.5) +
  ylim(0, 100) +
  theme(text = element_text(size = 15))  


f<-ggplot(dataRADAR_finalW, aes(x=autocorNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA inertia") +
  xlim(0, 1) +
  ylim(0, 100) +
  theme(text = element_text(size = 15))  


g<-ggplot(dataRADAR_finalW, aes(x=imeanPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA level") +
  xlim(2, 9) +
  ylim(0, 100) +
  theme(text = element_text(size = 15))  


h<-ggplot(dataRADAR_finalW, aes(x=isdPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA variability") +
  xlim(0, 3) +
  ylim(0, 100) +
  theme(text = element_text(size = 15))  


j<-ggplot(dataRADAR_finalW, aes(x=autocorPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA inertia") +
  xlim(0, 1) +
  ylim(0, 100) +
  theme(text = element_text(size = 15)) 


hist_graph_RADAR<-ggarrange(a,b,c,d,f,g,h,j, 
                          ncol = 2, nrow = 5)
hist_graph_RADAR
ggsave("../output/histRADARgraph.png", hist_graph_RADAR, width = 8, height = 12)


```


### Swinging Moods 

```{r histograms SM, echo=TRUE}
#### Depression ####

a<-ggplot(dataSM_finalW, aes(x=DepB))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms baseline") +
  xlim(1, 4) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  

b<-ggplot(dataSM_finalW, aes(x=DepF))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms follow-up") +
  xlim(1, 4) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  


#### Affective Dynamics ####
c<-ggplot(dataSM_finalW, aes(x=imeanNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA level") +
  xlim(1, 7.5) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  


d<-ggplot(dataSM_finalW, aes(x=isdNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA variability") +
  xlim(0, 3.5) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  





f<-ggplot(dataSM_finalW, aes(x=autocorNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=21)+
  xlab("NA inertia") +
  xlim(0, 1) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  


g<-ggplot(dataSM_finalW, aes(x=imeanPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA level") +
  xlim(2, 9) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  


h<-ggplot(dataSM_finalW, aes(x=isdPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA variability") +
  xlim(0, 3) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  





j<-ggplot(dataSM_finalW, aes(x=autocorPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA inertia") +
  xlim(0, 1) +
  ylim(0, 80) +
  theme(text = element_text(size = 15)) 

hist_graph_SM<-ggarrange(a,b,c,d,f,g,h,j, 
                          ncol = 2, nrow = 5)
hist_graph_SM
ggsave("../output/histSMgraph.png", hist_graph_SM, width = 8, height = 12)



```

### Mood in Emerging Adults

```{r histograms MA, echo=TRUE} 
#### Depression ####

a<-ggplot(dataMA_finalW, aes(x=DepB))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms baseline") +
  xlim(1, 4) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  

b<-ggplot(dataMA_finalW, aes(x=DepF))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms follow-up") +
  xlim(1, 4)+
  ylim(0, 80) +
  theme(text = element_text(size = 15))  


#### Affective Dynamics ####
c<-ggplot(dataMA_finalW, aes(x=imeanNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA level") +
  xlim(1, 7.5) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  


d<-ggplot(dataMA_finalW, aes(x=isdNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA variability") +
  xlim(0, 3.5) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  




f<-ggplot(dataMA_finalW, aes(x=autocorNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA inertia") +
  xlim(0, 1) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  


g<-ggplot(dataMA_finalW, aes(x=imeanPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA level") +
  xlim(2, 9) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  


h<-ggplot(dataMA_finalW, aes(x=isdPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA variability") +
  xlim(0, 3) +
  ylim(0, 80) +
  theme(text = element_text(size = 15))  



j<-ggplot(dataMA_finalW, aes(x=autocorPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA inertia") +
  xlim(0, 1) +
  ylim(0, 80) +
  theme(text = element_text(size = 15)) 

hist_graph_MA<-ggarrange(a,b,c,d,f,g,h,j, 
                          ncol = 2, nrow = 5)
hist_graph_MA
ggsave("../output/histMAgraph.png", hist_graph_MA, width = 8, height = 12)

```

### Emotions in Daily Life
```{r histograms ED, echo=TRUE} 

#### Depression ####

a<-ggplot(dataED_finalW, aes(x=DepB))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms baseline") +
  xlim(0, 3) +
  ylim(0, 60) +
  theme(text = element_text(size = 15))  


#### Affective Dynamics ####
c<-ggplot(dataED_finalW, aes(x=imeanNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA level") +
  xlim(1, 7.5) +
  ylim(0, 60) +
  theme(text = element_text(size = 15))  


d<-ggplot(dataED_finalW, aes(x=isdNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA variability") +
  xlim(0, 3.5) +
  ylim(0, 60) +
  theme(text = element_text(size = 15))  


f<-ggplot(dataED_finalW, aes(x=autocorNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA inertia") +
  xlim(0, 1) +
  ylim(0, 60) +
  theme(text = element_text(size = 15))  


g<-ggplot(dataED_finalW, aes(x=imeanPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA level") +
  xlim(2, 9) +
  ylim(0, 60) +
  theme(text = element_text(size = 15))  


h<-ggplot(dataED_finalW, aes(x=isdPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA variability") +
  xlim(0, 3) +
  ylim(0, 60) +
  theme(text = element_text(size = 15))  



j<-ggplot(dataED_finalW, aes(x=autocorPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA inertia") +
  xlim(0, 1) +
  ylim(0, 60) +
  theme(text = element_text(size = 15)) 

hist_graph_ED<-ggarrange(a,NULL,c,d,f,g,h,j, 
                          ncol = 2, nrow = 5)
hist_graph_ED
ggsave("../output/histEDgraph.png", hist_graph_ED, width = 8, height = 12)
```

### Emotion Regulation in Action

```{r histograms EA, echo=TRUE} 

#### Depression ####

a<-ggplot(dataEA_finalW, aes(x=DepB))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms baseline") +
  xlim(1, 3) +
  ylim(0, 20) +
  theme(text = element_text(size = 15))  


#### Affective Dynamics ####
c<-ggplot(dataEA_finalW, aes(x=imeanNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA level") +
  xlim(1, 7.5) +
  ylim(0, 20) +
  theme(text = element_text(size = 15))  


d<-ggplot(dataEA_finalW, aes(x=isdNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA variability") +
  xlim(0, 3.5) +
  ylim(0, 20) +
  theme(text = element_text(size = 15))  



f<-ggplot(dataEA_finalW, aes(x=autocorNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA inertia") +
  xlim(0, 1) +
  ylim(0, 20) +
  theme(text = element_text(size = 15))  


g<-ggplot(dataEA_finalW, aes(x=imeanPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA level") +
  xlim(2, 9) +
  ylim(0, 20) +
  theme(text = element_text(size = 15))  


h<-ggplot(dataEA_finalW, aes(x=isdPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA variability") +
  xlim(0, 3) +
  ylim(0, 20) +
  theme(text = element_text(size = 15))  



j<-ggplot(dataEA_finalW, aes(x=autocorPA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("PA inertia") +
  xlim(0, 1) +
  ylim(0, 20) +
  theme(text = element_text(size = 15)) 


hist_graph_EA<-ggarrange(a,NULL,c,d,f,g,h,j, 
                          ncol = 2, nrow = 5)
hist_graph_EA
ggsave("../output/histEAgraph.png", hist_graph_EA, width = 8, height = 12)
```

### LASER

```{r histograms LASER, echo=TRUE} 
#### Depression ####

a<-ggplot(dataLASER_finalW, aes(x=DepB))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms baseline") +
  xlim(0, 3) +
  ylim(0, 60) +
  theme(text = element_text(size = 15))  


#### Affective Dynamics ####
c<-ggplot(dataLASER_finalW, aes(x=imeanNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA level") +
  xlim(1, 7.5) +
  ylim(0, 60) +
  theme(text = element_text(size = 15))  


d<-ggplot(dataLASER_finalW, aes(x=isdNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA variability") +
  xlim(0, 3.5) +
  ylim(0, 60) +
  theme(text = element_text(size = 15))  



f<-ggplot(dataLASER_finalW, aes(x=autocorNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA inertia") +
  xlim(0, 1) +
  ylim(0, 60) +
  theme(text = element_text(size = 15))  


hist_graph_LASER<-ggarrange(a,NULL,c,d,f, 
                          ncol = 2, nrow = 5)
hist_graph_LASER
ggsave("../output/histLASERgraph.png", hist_graph_LASER, width = 8, height = 12)
```

### YES

```{r histograms YES, echo=TRUE} 

#### Depression ####

a<-ggplot(dataYES_finalW, aes(x=DepB))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms baseline") +
  xlim(1, 3) +
  ylim(0, 45) +
  theme(text = element_text(size = 15))  

b<-ggplot(dataYES_finalW, aes(x=DepF))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("Depressive symptoms follow-up") +
  xlim(1, 3)+
  ylim(0, 45) +
  theme(text = element_text(size = 15))  


#### Affective Dynamics ####
c<-ggplot(dataYES_finalW, aes(x=imeanNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA level") +
  xlim(1, 7.5) +
  ylim(0, 45) +
  theme(text = element_text(size = 15))  


d<-ggplot(dataYES_finalW, aes(x=isdNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA variability") +
  xlim(0, 3.5) +
  ylim(0, 45) +
  theme(text = element_text(size = 15))  


f<-ggplot(dataYES_finalW, aes(x=autocorNA))+
  geom_histogram(color="darkblue", fill="lightblue", bins=20)+
  xlab("NA inertia") +
  xlim(0, 1) +
  ylim(0, 45) +
  theme(text = element_text(size = 15))  


hist_graph_YES<-ggarrange(a,b,c,d,f, 
                          ncol = 2, nrow = 5)

hist_graph_YES
ggsave("../output/histYESgraph.png", hist_graph_YES, width = 8, height = 12)


```

## Descriptives

On before standardized data.

For overall data, I did not ask for depression scores. These are on raw data and scales differ between studies. For those, only check out the scores per study.
```{r descriptives, cache=FALSE}

#overall 
numeric_columns = c(4:11,14,15)
kable(psych::describe(mega_all_5[numeric_columns]), 
      format='markdown', 
      digits=2)


#per study
numeric_columns = c(2:11,14,15)
describeBy(mega_all[numeric_columns],mega_all$study)



#### comparing means of NA affective dynamics between studies ####
## meanNA ##
describeBy(mega_all$imeanNA ,mega_all$study)
a1<-aov(mega_all$imeanNA ~ mega_all$study) 
summary(a1)
TukeyHSD(a1)

## SDNA ##
describeBy(mega_all$isdNA ,mega_all$study)
a1<-aov(mega_all$isdNA ~ mega_all$study) 
summary(a1)
TukeyHSD(a1)

## rMSSDNA ##
describeBy(mega_all$iMSSDNA ,mega_all$study)
a1<-aov(mega_all$iMSSDNA ~ mega_all$study) 
summary(a1)
TukeyHSD(a1)

## ARNA ##
describeBy(mega_all$autocorNA ,mega_all$study)
a1<-aov(mega_all$autocorNA ~ mega_all$study) 
summary(a1)
TukeyHSD(a1)


```

## Correlations

### Overall 

```{r correlations overall, cache=FALSE}

##################### dataframes with sample size for metafor #####################
#Note: Not all studies have all variables available (e.g., PA or Depression FU data)

nstudy7 <- c(n_fin_RADAR,n_fin_SM,n_fin_MA,n_fin_ED,n_fin_EA,n_fin_LASER,n_fin_YES)

##################### all 7 studies #####################
df <- mega_all_cen_std %>%
                group_by(study) %>% 
                dplyr::summarise(r_PAmeanPAsd = cor(imeanPA,isdPA, use="na.or.complete"),
                                 r_PAmeanPAmssd = cor(imeanPA,iMSSDPA, use="na.or.complete"),
                                 r_PAmeanPAauto = cor(imeanPA,autocorPA, use="na.or.complete"),
                                 r_PAmeanNAmean = cor(imeanPA,imeanNA, use="na.or.complete"),
                                 r_PAmeanNAsd = cor(imeanPA,isdNA, use="na.or.complete"),
                                 r_PAmeanNAmssd = cor(imeanPA,iMSSDNA, use="na.or.complete"),
                                 r_PAmeanNAauto = cor(imeanPA,autocorNA, use="na.or.complete"),
                                 r_PAmeanDepB = cor(imeanPA,DepB, use="na.or.complete"),
                                 r_PAmeanDepF = cor(imeanPA,DepF, use="na.or.complete"),
                                 
                                 r_PAsdPAmssd = cor(isdPA,iMSSDPA, use="na.or.complete"),
                                 r_PAsdPAauto = cor(isdPA,autocorPA, use="na.or.complete"),
                                 r_PAsdNAmean = cor(isdPA,imeanNA, use="na.or.complete"),
                                 r_PAsdNAsd = cor(isdPA,isdNA, use="na.or.complete"),
                                 r_PAsdNAmssd = cor(isdPA,iMSSDNA, use="na.or.complete"),
                                 r_PAsdNAauto = cor(isdPA,autocorNA, use="na.or.complete"),
                                 r_PAsdDepB = cor(isdPA,DepB, use="na.or.complete"),
                                 r_PAsdDepF = cor(isdPA,DepF, use="na.or.complete"),
                                 
                                 r_PAmssdPAauto = cor(iMSSDPA,autocorPA, use="na.or.complete"),
                                 r_PAmssdNAmean = cor(iMSSDPA,imeanNA, use="na.or.complete"),
                                 r_PAmssdNAsd = cor(iMSSDPA,isdNA, use="na.or.complete"),
                                 r_PAmssdNAmssd = cor(iMSSDPA,iMSSDNA, use="na.or.complete"),
                                 r_PAmssdNAauto = cor(iMSSDPA,autocorNA, use="na.or.complete"),
                                 r_PAmssdDepB = cor(iMSSDPA,DepB, use="na.or.complete"),
                                 r_PAmssdDepF = cor(iMSSDPA,DepF, use="na.or.complete"),
                                 
                                 r_PAautoNAmean = cor(autocorPA,imeanNA, use="na.or.complete"),
                                 r_PAautoNAsd = cor(autocorPA,isdNA, use="na.or.complete"),
                                 r_PAautoNAmssd = cor(autocorPA,iMSSDNA, use="na.or.complete"),
                                 r_PAautoNAauto = cor(autocorPA,autocorNA, use="na.or.complete"),
                                 r_PAautoDepB = cor(autocorPA,DepB, use="na.or.complete"),
                                 r_PAautoDepF = cor(autocorPA,DepF, use="na.or.complete"),
                                 
                                 r_NAmeanNAsd = cor(imeanNA,isdNA, use="na.or.complete"),
                                 r_NAmeanNAmssd = cor(imeanNA,iMSSDNA, use="na.or.complete"),
                                 r_NAmeanNAauto = cor(imeanNA,autocorNA, use="na.or.complete"),
                                 r_NAmeanDepB = cor(imeanNA,DepB, use="na.or.complete"),
                                 r_NAmeanDepF = cor(imeanNA,DepF, use="na.or.complete"),
                                 
                                 r_NAsdNAmssd = cor(isdNA,iMSSDNA, use="na.or.complete"),
                                 r_NAsdNAauto = cor(isdNA,autocorNA, use="na.or.complete"),
                                 r_NAsdDepB = cor(isdNA,DepB, use="na.or.complete"),
                                 r_NAsdDepF = cor(isdNA,DepF, use="na.or.complete"),
                                 
                                 r_NAmssdNAauto = cor(iMSSDNA,autocorNA, use="na.or.complete"),
                                 r_NAmssdDepB = cor(iMSSDNA,DepB, use="na.or.complete"),
                                 r_NAmssdDepF = cor(iMSSDNA,DepF, use="na.or.complete"),
                                 
                                 r_NAautoDepB = cor(autocorNA,DepB, use="na.or.complete"),
                                 r_NAautoDepF = cor(autocorNA,DepF, use="na.or.complete"),
                                 
                                 r_DepBDepF = cor(DepB,DepF, use="na.or.complete"))
df <-cbind(df,nstudy7)
df

#fit model
meta_esc_PAmeanPAsd <-escalc(measure ='COR', ri = df$r_PAmeanPAsd, ni = df$nstudy7)
meta_esc_PAmeanPAmssd <-escalc(measure ='COR', ri = df$r_PAmeanPAmssd, ni = df$nstudy7)
meta_esc_PAmeanPAauto <-escalc(measure ='COR', ri = df$r_PAmeanPAauto, ni = df$nstudy7)
meta_esc_PAmeanNAmean <-escalc(measure ='COR', ri = df$r_PAmeanNAmean, ni = df$nstudy7)
meta_esc_PAmeanNAsd <-escalc(measure ='COR', ri = df$r_PAmeanNAsd, ni = df$nstudy7)
meta_esc_PAmeanNAmssd <-escalc(measure ='COR', ri = df$r_PAmeanNAmssd, ni = df$nstudy7)
meta_esc_PAmeanNAauto <-escalc(measure ='COR', ri = df$r_PAmeanNAauto, ni = df$nstudy7)
meta_esc_PAmeanDepB <-escalc(measure ='COR', ri = df$r_PAmeanDepB, ni = df$nstudy7)
meta_esc_PAmeanDepF <-escalc(measure ='COR', ri = df$r_PAmeanDepF, ni = df$nstudy7)

meta_esc_PAsdPAmssd <-escalc(measure ='COR', ri = df$r_PAsdPAmssd, ni = df$nstudy7)
meta_esc_PAsdPAauto <-escalc(measure ='COR', ri = df$r_PAsdPAauto, ni = df$nstudy7)
meta_esc_PAsdNAmean <-escalc(measure ='COR', ri = df$r_PAsdNAmean, ni = df$nstudy7)
meta_esc_PAsdNAsd <-escalc(measure ='COR', ri = df$r_PAsdNAsd, ni = df$nstudy7)
meta_esc_PAsdNAmssd <-escalc(measure ='COR', ri = df$r_PAsdNAmssd, ni = df$nstudy7)
meta_esc_PAsdNAauto <-escalc(measure ='COR', ri = df$r_PAsdNAauto, ni = df$nstudy7)
meta_esc_PAsdDepB <-escalc(measure ='COR', ri = df$r_PAsdDepB, ni = df$nstudy7)
meta_esc_PAsdDepF <-escalc(measure ='COR', ri = df$r_PAsdDepF, ni = df$nstudy7)

meta_esc_PAmssdPAauto <-escalc(measure ='COR', ri = df$r_PAmssdPAauto, ni = df$nstudy7)
meta_esc_PAmssdNAmean <-escalc(measure ='COR', ri = df$r_PAmssdNAmean, ni = df$nstudy7)
meta_esc_PAmssdNAsd <-escalc(measure ='COR', ri = df$r_PAmssdNAsd, ni = df$nstudy7)
meta_esc_PAmssdNAmssd <-escalc(measure ='COR', ri = df$r_PAmssdNAmssd, ni = df$nstudy7)
meta_esc_PAmssdNAauto <-escalc(measure ='COR', ri = df$r_PAmssdNAauto, ni = df$nstudy7)
meta_esc_PAmssdDepB <-escalc(measure ='COR', ri = df$r_PAmssdDepB, ni = df$nstudy7)
meta_esc_PAmssdDepF <-escalc(measure ='COR', ri = df$r_PAmssdDepF, ni = df$nstudy7)

meta_esc_PAautoNAmean <-escalc(measure ='COR', ri = df$r_PAautoNAmean, ni = df$nstudy7)
meta_esc_PAautoNAsd <-escalc(measure ='COR', ri = df$r_PAautoNAsd, ni = df$nstudy7)
meta_esc_PAautoNAmssd <-escalc(measure ='COR', ri = df$r_PAautoNAmssd, ni = df$nstudy7)
meta_esc_PAautoNAauto <-escalc(measure ='COR', ri = df$r_PAautoNAauto, ni = df$nstudy7)
meta_esc_PAautoDepB <-escalc(measure ='COR', ri = df$r_PAautoDepB, ni = df$nstudy7)
meta_esc_PAautoDepF <-escalc(measure ='COR', ri = df$r_PAautoDepF, ni = df$nstudy7)


meta_esc_NAmeanNAsd <-escalc(measure ='COR', ri = df$r_NAmeanNAsd, ni = df$nstudy7)
meta_esc_NAmeanNAmssd <-escalc(measure ='COR', ri = df$r_NAmeanNAmssd, ni = df$nstudy7)
meta_esc_NAmeanNAauto <-escalc(measure ='COR', ri = df$r_NAmeanNAauto, ni = df$nstudy7)
meta_esc_NAmeanDepB <-escalc(measure ='COR', ri = df$r_NAmeanDepB, ni = df$nstudy7)
meta_esc_NAmeanDepF <-escalc(measure ='COR', ri = df$r_NAmeanDepF, ni = df$nstudy7)

meta_esc_NAsdNAmssd <-escalc(measure ='COR', ri = df$r_NAsdNAmssd, ni = df$nstudy7)
meta_esc_NAsdNAauto <-escalc(measure ='COR', ri = df$r_NAsdNAauto, ni = df$nstudy7)
meta_esc_NAsdDepB <-escalc(measure ='COR', ri = df$r_NAsdDepB, ni = df$nstudy7)
meta_esc_NAsdDepF <-escalc(measure ='COR', ri = df$r_NAsdDepF, ni = df$nstudy7)

meta_esc_NAmssdNAauto <-escalc(measure ='COR', ri = df$r_NAmssdNAauto, ni = df$nstudy7)
meta_esc_NAmssdDepB <-escalc(measure ='COR', ri = df$r_NAmssdDepB, ni = df$nstudy7)
meta_esc_NAmssdDepF <-escalc(measure ='COR', ri = df$r_NAmssdDepF, ni = df$nstudy7)

meta_esc_NAautoDepB <-escalc(measure ='COR', ri = df$r_NAautoDepB, ni = df$nstudy7)
meta_esc_NAautoDepF <-escalc(measure ='COR', ri = df$r_NAautoDepF, ni = df$nstudy7)

meta_esc_DepBDepF <-escalc(measure ='COR', ri = df$r_DepBDepF, ni = df$nstudy7)



meta_PAmeanPAsd <-rma(meta_esc_PAmeanPAsd,method ='REML')
meta_PAmeanPAmssd <-rma(meta_esc_PAmeanPAmssd,method ='REML')
meta_PAmeanPAauto<-rma(meta_esc_PAmeanPAauto,method ='REML')
meta_PAmeanNAmean<-rma(meta_esc_PAmeanNAmean,method ='REML')
meta_PAmeanNAsd <-rma(meta_esc_PAmeanNAsd,method ='REML')
meta_PAmeanNAmssd <-rma(meta_esc_PAmeanNAmssd,method ='REML')
meta_PAmeanNAauto <-rma(meta_esc_PAmeanNAauto,method ='REML')
meta_PAmeanDepB <-rma(meta_esc_PAmeanDepB,method ='REML')
meta_PAmeanDepF <-rma(meta_esc_PAmeanDepF,method ='REML')

meta_PAsdPAmssd <-rma(meta_esc_PAsdPAmssd,method ='REML')
meta_PAsdPAauto <-rma(meta_esc_PAsdPAauto,method ='REML')
meta_PAsdNAmean <-rma(meta_esc_PAsdNAmean,method ='REML')
meta_PAsdNAsd <-rma(meta_esc_PAsdNAsd,method ='REML')
meta_PAsdNAmssd <-rma(meta_esc_PAsdNAmssd,method ='REML')
meta_PAsdNAauto <-rma(meta_esc_PAsdNAauto,method ='REML')
meta_PAsdDepB <-rma(meta_esc_PAsdDepB,method ='REML')
meta_PAsdDepF <-rma(meta_esc_PAsdDepF,method ='REML')

meta_PAmssdPAauto <-rma(meta_esc_PAmssdPAauto,method ='REML')
meta_PAmssdNAmean <-rma(meta_esc_PAmssdNAmean,method ='REML')
meta_PAmssdNAsd <-rma(meta_esc_PAmssdNAsd,method ='REML')
meta_PAmssdNAmssd <-rma(meta_esc_PAmssdNAmssd,method ='REML')
meta_PAmssdNAauto <-rma(meta_esc_PAmssdNAauto,method ='REML')
meta_PAmssdDepB <-rma(meta_esc_PAmssdDepB,method ='REML')
meta_PAmssdDepF <-rma(meta_esc_PAmssdDepF,method ='REML')

meta_PAautoNAmean <-rma(meta_esc_PAautoNAmean,method ='REML')
meta_PAautoNAsd <-rma(meta_esc_PAautoNAsd,method ='REML')
meta_PAautoNAmssd <-rma(meta_esc_PAautoNAmssd,method ='REML')
meta_PAautoNAauto <-rma(meta_esc_PAautoNAauto,method ='REML')
meta_PAautoDepB <-rma(meta_esc_PAautoDepB,method ='REML')
meta_PAautoDepF <-rma(meta_esc_PAautoDepF,method ='REML')


meta_NAmeanNAsd <-rma(meta_esc_NAmeanNAsd,method ='REML')
meta_NAmeanNAmssd <-rma(meta_esc_NAmeanNAmssd,method ='REML')
meta_NAmeanNAauto <-rma(meta_esc_NAmeanNAauto,method ='REML')
meta_NAmeanDepB <-rma(meta_esc_NAmeanDepB,method ='REML')
meta_NAmeanDepF <-rma(meta_esc_NAmeanDepF,method ='REML')

meta_NAsdNAmssd <-rma(meta_esc_NAsdNAmssd,method ='REML')
meta_NAsdNAauto <-rma(meta_esc_NAsdNAauto,method ='REML')
meta_NAsdDepB <-rma(meta_esc_NAsdDepB,method ='REML')
meta_NAsdDepF <-rma(meta_esc_NAsdDepF,method ='REML')

meta_NAmssdNAauto <-rma(meta_esc_NAmssdNAauto,method ='REML')
meta_NAmssdDepB <-rma(meta_esc_NAmssdDepB,method ='REML')
meta_NAmssdDepF <-rma(meta_esc_NAmssdDepF,method ='REML')

meta_NAautoDepB <-rma(meta_esc_NAautoDepB,method ='REML')
meta_NAautoDepF <-rma(meta_esc_NAautoDepF,method ='REML')

meta_DepBDepF <-rma(meta_esc_DepBDepF,method ='REML')




#make dataframe
var_corr <- c("r_PAmeanPAsd","r_PAmeanPAmssd","r_PAmeanPAauto",
              "r_PAmeanNAmean","r_PAmeanNAsd","r_PAmeanNAmssd","r_PAmeanNAauto",
              "r_PAmeanDepB","r_PAmeanDepF",
              
              "r_PAsdPAmssd","r_PAsdPAauto",
              "r_PAsdNAmean","r_PAsdNAsd","r_PAsdNAmssd","r_PAsdNAauto",
              "r_PAsdDepB","r_PAsdDepF",
              
              "r_PAmssdPAauto",
              "r_PAmssdNAmean","r_PAmssdNAsd","r_PAmssdNAmssd","r_PAmssdNAauto",
              "r_PAmssdDepB","r_PAmssdDepF",
              
              "r_PAautoNAmean","r_PAautoNAsd","r_PAautoNAmssd","r_PAautoNAauto",
              "r_PAautoDepB","r_PAautoDepF",
              
              "r_NAmeanNAsd","r_NAmeanNAmssd","r_NAmeanNAauto",
              "r_NAmeanDepB","r_NAmeanDepF",
              
              "r_NAsdNAmssd","r_NAsdNAauto",
              "r_NAsdDepB","r_NAsdDepF",
              
              "r_NAmssdNAauto",
              "r_NAmssdDepB","r_NAmssdDepF",
              
              "r_NAautoDepB","r_NAautoDepF",
              
              "r_PAmeanDepF")

cor_corr <-round(c(coef(summary(meta_PAmeanPAsd))$estimate,
                coef(summary(meta_PAmeanPAmssd))$estimate,
                coef(summary(meta_PAmeanPAauto))$estimate,
                coef(summary(meta_PAmeanNAmean))$estimate,
                coef(summary(meta_PAmeanNAsd))$estimate,                
                coef(summary(meta_PAmeanNAmssd))$estimate,
                coef(summary(meta_PAmeanNAauto))$estimate,
                coef(summary(meta_PAmeanDepB))$estimate,
                coef(summary(meta_PAmeanDepF))$estimate,
                
                coef(summary(meta_PAsdPAmssd))$estimate,
                coef(summary(meta_PAsdPAauto))$estimate,
                coef(summary(meta_PAsdNAmean))$estimate,
                coef(summary(meta_PAsdNAsd))$estimate,                
                coef(summary(meta_PAsdNAmssd))$estimate,
                coef(summary(meta_PAsdNAauto))$estimate,
                coef(summary(meta_PAsdDepB))$estimate,
                coef(summary(meta_PAsdDepF))$estimate,
                
                coef(summary(meta_PAmssdPAauto))$estimate,
                coef(summary(meta_PAmssdNAmean))$estimate,
                coef(summary(meta_PAmssdNAsd))$estimate,                
                coef(summary(meta_PAmssdNAmssd))$estimate,
                coef(summary(meta_PAmssdNAauto))$estimate,
                coef(summary(meta_PAmssdDepB))$estimate,
                coef(summary(meta_PAmssdDepF))$estimate,
                
                coef(summary(meta_PAautoNAmean))$estimate,
                coef(summary(meta_PAautoNAsd))$estimate,                
                coef(summary(meta_PAautoNAmssd))$estimate,
                coef(summary(meta_PAautoNAauto))$estimate,
                coef(summary(meta_PAautoDepB))$estimate,
                coef(summary(meta_PAautoDepF))$estimate,
                
                coef(summary(meta_NAmeanNAsd))$estimate,                
                coef(summary(meta_NAmeanNAmssd))$estimate,
                coef(summary(meta_NAmeanNAauto))$estimate,
                coef(summary(meta_NAmeanDepB))$estimate,
                coef(summary(meta_NAmeanDepF))$estimate,
                
                coef(summary(meta_NAsdNAmssd))$estimate,
                coef(summary(meta_NAsdNAauto))$estimate,
                coef(summary(meta_NAsdDepB))$estimate,
                coef(summary(meta_NAsdDepF))$estimate,
                
                coef(summary(meta_NAmssdNAauto))$estimate,
                coef(summary(meta_NAmssdDepB))$estimate,
                coef(summary(meta_NAmssdDepF))$estimate,
                
                coef(summary(meta_NAautoDepB))$estimate,
                coef(summary(meta_NAautoDepF))$estimate,
                
                coef(summary(meta_DepBDepF))$estimate),digits = 2)

p_corr <-round(c(coef(summary(meta_PAmeanPAsd))$pval,
                coef(summary(meta_PAmeanPAmssd))$pval,
                coef(summary(meta_PAmeanPAauto))$pval,
                coef(summary(meta_PAmeanNAmean))$pval,
                coef(summary(meta_PAmeanNAsd))$pval,                
                coef(summary(meta_PAmeanNAmssd))$pval,
                coef(summary(meta_PAmeanNAauto))$pval,
                coef(summary(meta_PAmeanDepB))$pval,
                coef(summary(meta_PAmeanDepF))$pval,
                
                coef(summary(meta_PAsdPAmssd))$pval,
                coef(summary(meta_PAsdPAauto))$pval,
                coef(summary(meta_PAsdNAmean))$pval,
                coef(summary(meta_PAsdNAsd))$pval,                
                coef(summary(meta_PAsdNAmssd))$pval,
                coef(summary(meta_PAsdNAauto))$pval,
                coef(summary(meta_PAsdDepB))$pval,
                coef(summary(meta_PAsdDepF))$pval,
                
                coef(summary(meta_PAmssdPAauto))$pval,
                coef(summary(meta_PAmssdNAmean))$pval,
                coef(summary(meta_PAmssdNAsd))$pval,                
                coef(summary(meta_PAmssdNAmssd))$pval,
                coef(summary(meta_PAmssdNAauto))$pval,
                coef(summary(meta_PAmssdDepB))$pval,
                coef(summary(meta_PAmssdDepF))$pval,
                
                coef(summary(meta_PAautoNAmean))$pval,
                coef(summary(meta_PAautoNAsd))$pval,                
                coef(summary(meta_PAautoNAmssd))$pval,
                coef(summary(meta_PAautoNAauto))$pval,
                coef(summary(meta_PAautoDepB))$pval,
                coef(summary(meta_PAautoDepF))$pval,
                
                coef(summary(meta_NAmeanNAsd))$pval,                
                coef(summary(meta_NAmeanNAmssd))$pval,
                coef(summary(meta_NAmeanNAauto))$pval,
                coef(summary(meta_NAmeanDepB))$pval,
                coef(summary(meta_NAmeanDepF))$pval,
                
                coef(summary(meta_NAsdNAmssd))$pval,
                coef(summary(meta_NAsdNAauto))$pval,
                coef(summary(meta_NAsdDepB))$pval,
                coef(summary(meta_NAsdDepF))$pval,
                
                coef(summary(meta_NAmssdNAauto))$pval,
                coef(summary(meta_NAmssdDepB))$pval,
                coef(summary(meta_NAmssdDepF))$pval,
                
                coef(summary(meta_NAautoDepB))$pval,
                coef(summary(meta_NAautoDepF))$pval,
                
                coef(summary(meta_DepBDepF))$pval),digits = 3)



comp_study<-data.frame(var_corr,cor_corr,p_corr)

pander(comp_study,caption = "Correlation between study variables whole sample")
```

### Excluding Datasets 6 and 7 (LASER and YES)

Only done on NA variables, since datasets 6 and 7 did not have data available on PA.

```{r correlations dataset 1 to 5, cache=FALSE}

##################### dataframes with sample size for metafor #####################
#Note: Not all studies have Depression FU data

nstudy5 <- c(n_fin_RADAR,n_fin_SM,n_fin_MA,n_fin_ED,n_fin_EA)

##################### only 5 studies #####################
df <- mega_all_cen_std_5 %>%
                group_by(study) %>% 
                dplyr::summarise(r_NAmeanNAsd = cor(imeanNA,isdNA, use="na.or.complete"),
                                 r_NAmeanNAmssd = cor(imeanNA,iMSSDNA, use="na.or.complete"),
                                 r_NAmeanNAauto = cor(imeanNA,autocorNA, use="na.or.complete"),
                                 r_NAmeanDepB = cor(imeanNA,DepB, use="na.or.complete"),
                                 r_NAmeanDepF = cor(imeanNA,DepF, use="na.or.complete"),
                                 
                                 r_NAsdNAmssd = cor(isdNA,iMSSDNA, use="na.or.complete"),
                                 r_NAsdNAauto = cor(isdNA,autocorNA, use="na.or.complete"),
                                 r_NAsdDepB = cor(isdNA,DepB, use="na.or.complete"),
                                 r_NAsdDepF = cor(isdNA,DepF, use="na.or.complete"),
                                 
                                 r_NAmssdNAauto = cor(iMSSDNA,autocorNA, use="na.or.complete"),
                                 r_NAmssdDepB = cor(iMSSDNA,DepB, use="na.or.complete"),
                                 r_NAmssdDepF = cor(iMSSDNA,DepF, use="na.or.complete"),
                                 
                                 r_NAautoDepB = cor(autocorNA,DepB, use="na.or.complete"),
                                 r_NAautoDepF = cor(autocorNA,DepF, use="na.or.complete"),
                                 
                                 r_DepBDepF = cor(DepB,DepF, use="na.or.complete"))
df <-cbind(df,nstudy5)
df

#fit model]
meta_esc_NAmeanNAsd <-escalc(measure ='COR', ri = df$r_NAmeanNAsd, ni = df$nstudy5)
meta_esc_NAmeanNAmssd <-escalc(measure ='COR', ri = df$r_NAmeanNAmssd, ni = df$nstudy5)
meta_esc_NAmeanNAauto <-escalc(measure ='COR', ri = df$r_NAmeanNAauto, ni = df$nstudy5)
meta_esc_NAmeanDepB <-escalc(measure ='COR', ri = df$r_NAmeanDepB, ni = df$nstudy5)
meta_esc_NAmeanDepF <-escalc(measure ='COR', ri = df$r_NAmeanDepF, ni = df$nstudy5)

meta_esc_NAsdNAmssd <-escalc(measure ='COR', ri = df$r_NAsdNAmssd, ni = df$nstudy5)
meta_esc_NAsdNAauto <-escalc(measure ='COR', ri = df$r_NAsdNAauto, ni = df$nstudy5)
meta_esc_NAsdDepB <-escalc(measure ='COR', ri = df$r_NAsdDepB, ni = df$nstudy5)
meta_esc_NAsdDepF <-escalc(measure ='COR', ri = df$r_NAsdDepF, ni = df$nstudy5)

meta_esc_NAmssdNAauto <-escalc(measure ='COR', ri = df$r_NAmssdNAauto, ni = df$nstudy5)
meta_esc_NAmssdDepB <-escalc(measure ='COR', ri = df$r_NAmssdDepB, ni = df$nstudy5)
meta_esc_NAmssdDepF <-escalc(measure ='COR', ri = df$r_NAmssdDepF, ni = df$nstudy5)

meta_esc_NAautoDepB <-escalc(measure ='COR', ri = df$r_NAautoDepB, ni = df$nstudy5)
meta_esc_NAautoDepF <-escalc(measure ='COR', ri = df$r_NAautoDepF, ni = df$nstudy5)

meta_esc_DepBDepF <-escalc(measure ='COR', ri = df$r_DepBDepF, ni = df$nstudy5)



meta_NAmeanNAsd <-rma(meta_esc_NAmeanNAsd,method ='REML')
meta_NAmeanNAmssd <-rma(meta_esc_NAmeanNAmssd,method ='REML')
meta_NAmeanNAauto <-rma(meta_esc_NAmeanNAauto,method ='REML')
meta_NAmeanDepB <-rma(meta_esc_NAmeanDepB,method ='REML')
meta_NAmeanDepF <-rma(meta_esc_NAmeanDepF,method ='REML')

meta_NAsdNAmssd <-rma(meta_esc_NAsdNAmssd,method ='REML')
meta_NAsdNAauto <-rma(meta_esc_NAsdNAauto,method ='REML')
meta_NAsdDepB <-rma(meta_esc_NAsdDepB,method ='REML')
meta_NAsdDepF <-rma(meta_esc_NAsdDepF,method ='REML')

meta_NAmssdNAauto <-rma(meta_esc_NAmssdNAauto,method ='REML')
meta_NAmssdDepB <-rma(meta_esc_NAmssdDepB,method ='REML')
meta_NAmssdDepF <-rma(meta_esc_NAmssdDepF,method ='REML')

meta_NAautoDepB <-rma(meta_esc_NAautoDepB,method ='REML')
meta_NAautoDepF <-rma(meta_esc_NAautoDepF,method ='REML')

meta_DepBDepF <-rma(meta_esc_DepBDepF,method ='REML')




#make dataframe
var_corr <- c("r_NAmeanNAsd","r_NAmeanNAmssd","r_NAmeanNAauto",
              "r_NAmeanDepB","r_NAmeanDepF",
              
              "r_NAsdNAmssd","r_NAsdNAauto",
              "r_NAsdDepB","r_NAsdDepF",
              
              "r_NAmssdNAauto",
              "r_NAmssdDepB","r_NAmssdDepF",
              
              "r_NAautoDepB","r_NAautoDepF",
              
              "r_PAmeanDepF")

cor_corr <-round(c(coef(summary(meta_NAmeanNAsd))$estimate,                
                coef(summary(meta_NAmeanNAmssd))$estimate,
                coef(summary(meta_NAmeanNAauto))$estimate,
                coef(summary(meta_NAmeanDepB))$estimate,
                coef(summary(meta_NAmeanDepF))$estimate,
                
                coef(summary(meta_NAsdNAmssd))$estimate,
                coef(summary(meta_NAsdNAauto))$estimate,
                coef(summary(meta_NAsdDepB))$estimate,
                coef(summary(meta_NAsdDepF))$estimate,
                
                coef(summary(meta_NAmssdNAauto))$estimate,
                coef(summary(meta_NAmssdDepB))$estimate,
                coef(summary(meta_NAmssdDepF))$estimate,
                
                coef(summary(meta_NAautoDepB))$estimate,
                coef(summary(meta_NAautoDepF))$estimate,
                
                coef(summary(meta_DepBDepF))$estimate),digits = 2)

p_corr <-round(c(coef(summary(meta_NAmeanNAsd))$pval,                
                coef(summary(meta_NAmeanNAmssd))$pval,
                coef(summary(meta_NAmeanNAauto))$pval,
                coef(summary(meta_NAmeanDepB))$pval,
                coef(summary(meta_NAmeanDepF))$pval,
                
                coef(summary(meta_NAsdNAmssd))$pval,
                coef(summary(meta_NAsdNAauto))$pval,
                coef(summary(meta_NAsdDepB))$pval,
                coef(summary(meta_NAsdDepF))$pval,
                
                coef(summary(meta_NAmssdNAauto))$pval,
                coef(summary(meta_NAmssdDepB))$pval,
                coef(summary(meta_NAmssdDepF))$pval,
                
                coef(summary(meta_NAautoDepB))$pval,
                coef(summary(meta_NAautoDepF))$pval,
                
                coef(summary(meta_DepBDepF))$pval),digits = 3)



comp_study<-data.frame(var_corr,cor_corr,p_corr)

pander(comp_study,caption = "Correlation between study variables datasets 1 - 5 (excluding datasets 6 and 7")
```

### Per study

#### Dataset 1: RADAR

```{r correlations RADAR, cache=FALSE}

dataRADAR_finalW_apa<-subset(dataRADAR_finalW, select=c(   
    "imeanPA", "isdPA", "autocorPA", 
    "imeanNA", "isdNA", "autocorNA",
    "DepB", "DepF"))
apa.cor.table(dataRADAR_finalW_apa, filename="../output/CorTableRADAR.doc")

```

#### Dataset 2: Swinging Moods

```{r correlations SM, cache=FALSE}

dataSM_finalW_apa<-subset(dataSM_finalW, select=c(   
    "imeanPA", "isdPA", "autocorPA", 
    "imeanNA", "isdNA", "autocorNA",
    "DepB", "DepF"))
apa.cor.table(dataSM_finalW_apa, filename="../output/CorTableSM.doc")



```

#### Dataset 3: Mood in Emerging Adults

```{r correlations MA, cache=FALSE}

dataMA_finalW_apa<-subset(dataMA_finalW, select=c(   
    "imeanPA", "isdPA", "autocorPA", 
    "imeanNA", "isdNA", "autocorNA",
    "DepB", "DepF"))
apa.cor.table(dataMA_finalW_apa, filename="../output/CorTableMA.doc")
```

#### Dataset 4: Emotions in Daily Life

```{r correlations ED, cache=FALSE}

dataED_finalW_apa<-subset(dataED_finalW, select=c(   
    "imeanPA", "isdPA", "autocorPA", 
    "imeanNA", "isdNA", "autocorNA",
    "DepB"))
apa.cor.table(dataED_finalW_apa, filename="../output/CorTableED.doc")



```

#### Dataset 5: Emotion regulation in action

```{r correlations EA, cache=FALSE}

dataEA_finalW_apa<-subset(dataEA_finalW, select=c(   
    "imeanPA", "isdPA", "autocorPA", 
    "imeanNA", "isdNA", "autocorNA",
    "DepB"))
apa.cor.table(dataEA_finalW_apa, filename="../output/CorTableEA.doc")




```



#### Dataset 6: LASER

```{r correlations LASER, cache=FALSE}

dataLASER_finalW_apa<-subset(dataLASER_finalW, select=c("imeanNA", "isdNA", "autocorNA", "DepB"))
apa.cor.table(dataLASER_finalW_apa, filename="../output/CorTableLASER.doc")




```

#### Dataset 7: YES

```{r correlations YES, cache=FALSE}

dataYES_finalW_apa<-subset(dataYES_finalW, select=c("imeanNA", "isdNA", "autocorNA", "DepB", "DepF"))
apa.cor.table(dataYES_finalW_apa, filename="../output/CorTableYES.doc")

```

# MEGA-ANALYSIS

## Results reported in the paper
These are the models reported in the paper.

### Positive Affect

#### Baseline Depression

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  
* Dataset 4: Emotions in Daily Life  
* Dataset 5: Emotion Regulation in Action  

##### Model 1: Main effects without controlling for affect level

```{r mega-analysis ACORSD main no ML B PA, cache=FALSE}

lm_Base_PA_SDAR_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdPA + autocorPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_SDAR_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_SDAR_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

##### Model 2: Main effects with controlling for affect level

```{r mega-analysis ACORSD  main ML B PA, cache=FALSE}

lm_Base_PA_SDAR_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdPA + autocorPA + imeanPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_SDAR_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_SDAR_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```

##### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis ACORSD  mod with ML B PA, cache=FALSE}
lm_Base_PA_SDAR_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdPA + autocorPA + imeanPA + autocorPA:imeanPA + isdPA:imeanPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_SDAR_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_SDAR_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

#### Follow-up Depression

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  

##### Model 1: Main effects without controlling for affect level

```{r mega-analysis ACORSD  main no ML FU PA, cache=FALSE}

# select only datasets with PA and FU Dep data (datasets 1, 2 and 3)
mega_all_cen_std_3 <- mega_all_cen_std %>%
  dplyr::filter(study == "1" | study == "2" | study == "3") 

lm_FU_PA_SDAR_main1<-lm(DepF ~ DepB + study_2 + study_3  + isdPA + autocorPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_SDAR_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_SDAR_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

##### Model 2: Main effects with controlling for affect level

```{r mega-analysis ACORSD  main ML FU PA, cache=FALSE}
lm_FU_PA_SDAR_main2<-lm(DepF ~ DepB + study_2 + study_3  + isdPA + autocorPA + imeanPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_SDAR_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_SDAR_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```

##### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis ACORSD  mod with ML FU PA, cache=FALSE}
lm_FU_PA_SDAR_mod<-lm(DepF ~ DepB +  study_2 + study_3 + isdPA + autocorPA + imeanPA + isdPA:imeanPA + autocorPA:imeanPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_SDAR_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_SDAR_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```


### Negative Affect

#### All datasets

These are the pre-registered analyses including all datasets.

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  
* Dataset 4: Emotions in Daily Life  
* Dataset 5: Emotion Regulation in Action  
* Dataset 6: LASER  
* Dataset 7: YES  

##### Baseline Depression

###### Model 1: Main effects without controlling for affect level

```{r mega-analysis ACORSD  main no ML B NA, cache=FALSE}

#### Standard Deviation ####

#Multilevel
mlm_Base_NA_SD_main1<-lmer(DepB ~ Age + Sex + isdNA + autocorNA + (1 | study), data = mega_all_cen_std, REML = TRUE, control = lmerControl(optimizer ='optimx', optCtrl=list(method='nlminb')))
#Fit is singular

#Now fixed effects model with dummy study as predictor (see also 2. Deviations from pre-registration).
lm_Base_NA_SDAR_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + isdNA + autocorNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_SDAR_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SDAR_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

###### Model 2: Main effects with controlling for affect level

```{r mega-analysis ACORSD  main ML B NA, cache=FALSE}

lm_Base_NA_SDAR_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + isdNA + autocorNA + imeanNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_SDAR_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SDAR_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```

###### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis ACORSD  mod with ML B NA, cache=FALSE}
lm_Base_NA_SDAR_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + isdNA + autocorNA + imeanNA + isdNA:imeanNA + autocorNA:imeanNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_SDAR_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SDAR_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

##### Follow-up Depression

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  
* Dataset 7: YES  

###### Model 1: Main effects without controlling for affect level

```{r mega-analysis ACORSD  main no ML F NA, cache=FALSE}
# select only datasets with NA and FU Dep data (datasets 1, 2, 3, and 7)
mega_all_cen_std_4 <- mega_all_cen_std %>%
  dplyr::filter(study == "1" | study == "2" | study == "3" | study == "7") 

lm_FU_NA_SDAR_main1<-lm(DepF ~ DepB + study_2 + study_3 + study_7 + isdNA + autocorNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_SDAR_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_SDAR_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

###### Model 2: Main effects with controlling for affect level

```{r mega-analysis ACORSD  main ML F NA, cache=FALSE}
lm_FU_NA_SDAR_main2<-lm(DepF ~ DepB + study_2 + study_3 + study_7 + isdNA + autocorNA + imeanNA  + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_SDAR_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_SDAR_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

###### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis ACORSD  mod with ML FU NA, cache=FALSE}
lm_FU_NA_SDAR_mod<-lm(DepF ~ DepB + study_2 + study_3 + study_7 + isdNA + autocorNA + imeanNA + isdNA:imeanNA + autocorNA:imeanNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_SDAR_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_SDAR_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```


#### Datasets 1-5 (excluding datasets 6 and 7)
These are the analyses without datasets 6 (LASER) and 7 (YES), which assessed NA differently

##### Baseline Depression

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  
* Dataset 4: Emotions in Daily Life  
* Dataset 5: Emotion Regulation in Action  

###### Model 1: Main effects without controlling for affect level

```{r mega-analysis ACORSD  main no ML B NA final, cache=FALSE}
#### Standard Deviation and Autocorrelation ####
lm_Base_NA_SDAR_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdNA + autocorNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_SDAR_main1,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_SDAR_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)
```

###### Model 2: Main effects with controlling for affect level

```{r mega-analysis ACORSD  main ML B NA final, cache=FALSE}
lm_Base_NA_SDAR_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdNA + autocorNA + imeanNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_SDAR_main2,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_SDAR_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 

```

###### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis ACORSD  mod with ML B NA final, cache=FALSE}
lm_Base_NA_SDAR_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdNA + autocorNA + imeanNA + isdNA:imeanNA + autocorNA:imeanNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_SDAR_mod,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_SDAR_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


#incremental effect size Cohens f2
cohen_f2(lm_Base_NA_SDAR_main2, lm_Base_NA_SDAR_mod)

#significant interaction effect. Following up with simple slope analyses 
#Johnson Neyman
johnson_neyman(lm_Base_NA_SDAR_mod, pred=isdNA, modx=imeanNA, plot = TRUE) #plot

#I used the following following shiny app for Johnson-Neyman as well as simple slope figures (McCabe et al., 2018)
#https://connorjmccabe.shinyapps.io/interactive/ 
#for reproduction, use file mega_all_cen_std_5.csv
#focal variable: isdNA (centered), moderator variable: imeanNA (centered), dependent variable: DepB (standardized)
#control variables: Age, Sex, study_2, study_3, study_4, study_5, autocorNA


#This is information from the website:
# The simple slope of isdNA on DepB is significant and negative when imeanNA is 2.1 
# standard deviations away from the mean or further. 5.96% of observations are within 
# this region. The simple slope of isdNA on DepB is significant and positive when imeanNA 
# is -0.55 standard deviations away from the mean or further. 34.42% of observations in 
# imeanNA are within this region.


```

###### Sensitivity Meta-analyses

Note, that we initially planned on using a two-stage approach, where data was analyzed first and then results were pooled (two-stage approach).
To check consistency, I am checking here whether the interaction effect is affected by the one-stage versus two-stage approach.

Conclusion: It is not!

```{r mega-analysis ACORSD  mod with ML B NA final twostage, cache=FALSE}

## Individual regressions

# Create a vector of study IDs
study_ids <- c("1", "2", "3", "4", "5")

# Initialize an empty list to store the results
results_list <- list()

# Loop through each study ID
for (study_id in study_ids) {
  # Filter the dataset based on the study ID
  dataset <- mega_all_cen_std_5 %>% filter(study == study_id)
  
  # Perform the linear regression
  lm_model <- lm(DepB ~ isdNA + autocorNA + imeanNA + isdNA:imeanNA + autocorNA:imeanNA + Age + Sex, data = dataset)
  
  # Get the summary of the linear regression
  st_lm_model <- summ(lm_model, digits = getOption("jtools-digits", 3), confint = TRUE, scale = TRUE, transform.response = TRUE)
  
  # Extract the coefficient value of interest (7th row, 1st column)
  df_int_SD <- st_lm_model$coeftable[7, 1]
  
  # Store the result in the list
  results_list[[paste0("df", study_id, "_int_SD")]] <- df_int_SD
}

# Unlist the results to get separate variables (df1_int_SD, df2_int_SD, etc.)
SD_NA_5 <- unlist(results_list)


## Meta-analysis with metafor package
#bind moderation effects
names<-c("dataset 1", "dataset 2", "dataset 3", "dataset 4", "dataset 5")
meta_SD<-as.data.frame(cbind(names,nstudy5,SD_NA_5))
meta_SD$nstudy5<-as.numeric(meta_SD$nstudy5)
meta_SD$SD_NA_5<-as.numeric(meta_SD$SD_NA_5)

#analyse
effects_meta_SD_dat <-escalc(measure ='COR', ri = meta_SD$SD_NA_5, ni = meta_SD$nstudy5)
effects_meta_SD<-rma(effects_meta_SD_dat,method ='REML')
effects_meta_SD
forest(effects_meta_SD, xlim=c(1,-1), slab=names, ilab=nstudy5, ilab.xpos=-0.6, cex=1,header=TRUE)


```

##### Follow-up Depression

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  

###### Model 1: Main effects without controlling for affect level

```{r mega-analysis ACORSD  main no ML F NA final, cache=FALSE}
lm_FU_NA_SDAR_main1<-lm(DepF ~ DepB + study_2 + study_3 + isdNA + autocorNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_SDAR_main1,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_SDAR_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 
```

###### Model 2: Main effects with controlling for affect level

```{r mega-analysis ACORSD  main ML F NA final, cache=FALSE}
lm_FU_NA_SDAR_main2<-lm(DepF ~ DepB + study_2 + study_3 + isdNA + autocorNA + imeanNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_SDAR_main2,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_SDAR_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


```

###### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis ACORSD  mod with ML FU NA final, cache=FALSE}
lm_FU_NA_SDAR_mod<-lm(DepF ~ DepB + study_2 + study_3 + isdNA + autocorNA + imeanNA + isdNA:imeanNA + autocorNA:imeanNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_SDAR_mod,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_SDAR_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 



```


## Results reported in the supplement
These are the models for each of the different affect dynamic index separately.

### Positive Affect

#### Baseline Depression

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  
* Dataset 4: Emotions in Daily Life  
* Dataset 5: Emotion Regulation in Action  

##### Model 1: Main effects without controlling for affect level

```{r mega-analysis main no ML B PA, cache=FALSE}

#### Standard Deviation ####
lm_Base_PA_SD_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_SD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_SD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)



#### RMSSD ####
lm_Base_PA_MSSD_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + iMSSDPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_MSSD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_MSSD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

#### Autocorrelation ####
lm_Base_PA_autocor_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + autocorPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_autocor_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_autocor_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

##### Model 2: Main effects with controlling for affect level

```{r mega-analysis main ML B PA, cache=FALSE}

#### Standard Deviation ####
lm_Base_PA_SD_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdPA + imeanPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_SD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_SD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### RMSSD ####
lm_Base_PA_MSSD_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + iMSSDPA + imeanPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_MSSD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_MSSD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### Autocorrelation ####
lm_Base_PA_autocor_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + autocorPA + imeanPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_autocor_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_autocor_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```

##### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis mod with ML B PA, cache=FALSE}
#### Standard Deviation ####
lm_Base_PA_SD_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdPA + imeanPA + isdPA:imeanPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_SD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_SD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### RMSSD ####
lm_Base_PA_MSSD_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + iMSSDPA + imeanPA + iMSSDPA:imeanPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_MSSD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_MSSD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### Autocorrelation ####
lm_Base_PA_autocor_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + autocorPA + imeanPA + autocorPA:imeanPA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_autocor_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_autocor_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```

#### Follow-up Depression

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  

##### Model 1: Main effects without controlling for affect level

```{r mega-analysis main no ML FU PA, cache=FALSE}

# select only datasets with PA and FU Dep data (datasets 1, 2 and 3)
mega_all_cen_std_3 <- mega_all_cen_std %>%
  dplyr::filter(study == "1" | study == "2" | study == "3") 

#### Standard Deviation ####
lm_FU_PA_SD_main1<-lm(DepF ~ DepB + study_2 + study_3  + isdPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_SD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_SD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### RMSSD ####
lm_FU_PA_MSSD_main1<-lm(DepF ~ DepB + study_2 + study_3 + iMSSDPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_MSSD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_MSSD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

#### Autocorrelation ####
lm_FU_PA_autocor_main1<-lm(DepF ~ DepB + study_2 + study_3 + autocorPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_autocor_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_autocor_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

##### Model 2: Main effects with controlling for affect level

```{r mega-analysis main ML FU PA, cache=FALSE}

#### Standard Deviation ####
lm_FU_PA_SD_main2<-lm(DepF ~ DepB + study_2 + study_3 + isdPA + imeanPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_SD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_SD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)



#### RMSSD ####
lm_FU_PA_MSSD_main2<-lm(DepF ~ DepB + study_2 + study_3  + iMSSDPA + imeanPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_MSSD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_MSSD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### Autocorrelation ####
lm_FU_PA_autocor_main2<-lm(DepF ~ DepB + study_2 + study_3 + autocorPA + imeanPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_autocor_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_autocor_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)



```

##### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis mod with ML FU PA, cache=FALSE}
#### Standard Deviation ####
lm_FU_PA_SD_mod<-lm(DepF ~ DepB +  study_2 + study_3 + isdPA + imeanPA + isdPA:imeanPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_SD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_SD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### RMSSD ####
lm_FU_PA_MSSD_mod<-lm(DepF ~ DepB + study_2 + study_3 + iMSSDPA + imeanPA + iMSSDPA:imeanPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_MSSD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_MSSD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### Autocorrelation ####
lm_FU_PA_autocor_mod<-lm(DepF ~ DepB +  study_2 + study_3 + autocorPA + imeanPA + autocorPA:imeanPA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_autocor_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_autocor_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```


### Negative Affect

#### All datasets

These are the pre-registered analyses including all datasets.

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  
* Dataset 4: Emotions in Daily Life  
* Dataset 5: Emotion Regulation in Action  
* Dataset 6: LASER  
* Dataset 7: YES  

##### Baseline Depression

###### Model 1: Main effects without controlling for affect level

```{r mega-analysis main no ML B NA, cache=FALSE}

#### Standard Deviation ####
lm_Base_NA_SD_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + isdNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_SD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

#### RMSSD ####
lm_Base_NA_MSSD_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + iMSSDNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_MSSD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_MSSD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

#### Autocorrelation ####
lm_Base_NA_autocor_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + autocorNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_autocor_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_autocor_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)
```

###### Model 2: Main effects with controlling for affect level

```{r mega-analysis main ML B NA, cache=FALSE}

#### Standard Deviation ####
lm_Base_NA_SD_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + isdNA + imeanNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_SD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### RMSSD ####
lm_Base_NA_MSSD_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + iMSSDNA + imeanNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_MSSD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_MSSD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### Autocorrelation ####
lm_Base_NA_autocor_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + autocorNA + imeanNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_autocor_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_autocor_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```

###### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis mod with ML B NA, cache=FALSE}

#### Standard Deviation ####
lm_Base_NA_SD_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + isdNA + imeanNA + isdNA:imeanNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_SD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### RMSSD ####
lm_Base_NA_MSSD_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + iMSSDNA + imeanNA + iMSSDNA:imeanNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_MSSD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_MSSD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### Autocorrelation ####
lm_Base_NA_autocor_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 + autocorNA + imeanNA + autocorNA:imeanNA + Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_autocor_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_autocor_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```

##### Follow-up Depression

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  
* Dataset 7: YES  

###### Model 1: Main effects without controlling for affect level

```{r mega-analysis main no ML F NA, cache=FALSE}
# select only datasets with NA and FU Dep data (datasets 1, 2, 3, and 7)
mega_all_cen_std_4 <- mega_all_cen_std %>%
  dplyr::filter(study == "1" | study == "2" | study == "3" | study == "7") 

#### Standard Deviation ####
lm_FU_NA_SD_main1<-lm(DepF ~ DepB + study_2 + study_3 + study_7 + isdNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_SD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_SD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

#### RMSSD ####
lm_FU_NA_MSSD_main1<-lm(DepF ~ DepB + study_2 + study_3  + study_7 + iMSSDNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_MSSD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_MSSD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

#### Autocorrelation ####
lm_FU_NA_autocor_main1<-lm(DepF ~ DepB + study_2 + study_3  + study_7 + autocorNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_autocor_main1,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_autocor_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

###### Model 2: Main effects with controlling for affect level

```{r mega-analysis main ML F NA, cache=FALSE}
#### Standard Deviation ####
lm_FU_NA_SD_main2<-lm(DepF ~ DepB + study_2 + study_3 + study_7 + isdNA + imeanNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_SD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_SD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### RMSSD ####
lm_FU_NA_MSSD_main2<-lm(DepF ~ DepB + study_2 + study_3 + study_7 + iMSSDNA + imeanNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_MSSD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_MSSD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### Autocorrelation ####
lm_FU_NA_autocor_main2<-lm(DepF ~ DepB + study_2 + study_3  + study_7 + autocorNA + imeanNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_autocor_main2,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_autocor_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```

###### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis mod with ML FU NA, cache=FALSE}
#### Standard Deviation ####
lm_FU_NA_SD_mod<-lm(DepF ~ DepB +  study_2 + study_3  + study_7 +  isdNA + imeanNA + isdNA:imeanNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_SD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_SD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### RMSSD ####
lm_FU_NA_MSSD_mod<-lm(DepF ~ DepB +  study_2 + study_3  + study_7 + iMSSDNA + imeanNA + iMSSDNA:imeanNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_MSSD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_MSSD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


#### Autocorrelation ####
lm_FU_NA_autocor_mod<-lm(DepF ~ DepB + study_2 + study_3 + study_7 + autocorNA + imeanNA + autocorNA:imeanNA + Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_autocor_mod,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_autocor_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)


```


#### Datasets 1-5 (excluding datasets 6 and 7)
These are the analyses without datasets 6 (LASER) and 7 (YES), which assessed NA differently

##### Baseline Depression

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  
* Dataset 4: Emotions in Daily Life  
* Dataset 5: Emotion Regulation in Action  

###### Model 1: Main effects without controlling for affect level

```{r mega-analysis main no ML B NA final, cache=FALSE}

#### Standard Deviation ####
lm_Base_NA_SD_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_SD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_SD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 

#### RMSSD ####
lm_Base_NA_MSSD_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + iMSSDNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_MSSD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_MSSD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 

#### Autocorrelation ####
lm_Base_NA_autocor_main1<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + autocorNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_autocor_main1,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_autocor_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 

```

###### Model 2: Main effects with controlling for affect level

```{r mega-analysis main ML B NA final, cache=FALSE}

#### Standard Deviation ####
lm_Base_NA_SD_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdNA + imeanNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_SD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_SD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


#### RMSSD ####
lm_Base_NA_MSSD_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + iMSSDNA + imeanNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_MSSD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_MSSD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


#### Autocorrelation ####
lm_Base_NA_autocor_main2<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + autocorNA + imeanNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_autocor_main2,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_autocor_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


```

###### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis mod with ML B NA final, cache=FALSE}

#### Standard Deviation ####
lm_Base_NA_SD_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + isdNA + imeanNA + isdNA:imeanNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_SD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_SD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 



#incremental effect size Cohens f2
cohen_f2(lm_Base_NA_SD_main2, lm_Base_NA_SD_mod)

#significant interaction effect. Following up with simple slope analyses 
#Johnson Neyman
johnson_neyman(lm_Base_NA_SD_mod, pred=isdNA, modx=imeanNA, plot = TRUE) #plot

#### RMSSD ####
lm_Base_NA_MSSD_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + iMSSDNA + imeanNA + iMSSDNA:imeanNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_MSSD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_MSSD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


#incremental effect size Cohens f2
cohen_f2(lm_Base_NA_MSSD_main2, lm_Base_NA_MSSD_mod)

#significant interaction effect. Following up with simple slope analyses 
#Johnson Neyman
johnson_neyman(lm_Base_NA_MSSD_mod, pred=iMSSDNA, modx=imeanNA, plot = TRUE) #plot

#The effect we see, as we saw for SD, is only partially applicable here. While we find that more NA instability 
#is related to more depressive symptoms for low levels of NA, this relation does not exist if the mean of NA is average (in line with the results of SD)
#We do not find the reverse effect (i.e., that high instability is associated with fewer depressive symptoms for high levels of NA level)

#### Autocorrelation ####
lm_Base_NA_autocor_mod<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + autocorNA + imeanNA + autocorNA:imeanNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_autocor_mod,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_Base_NA_autocor_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


```

##### Follow-up Depression

Studies with available data:

* Dataset 1: RADAR  
* Dataset 2: Swinging Moods  
* Dataset 3: Mood in Emerging Adults  

###### Model 1: Main effects without controlling for affect level

```{r mega-analysis main no ML F NA final, cache=FALSE}
#### Standard Deviation ####
lm_FU_NA_SD_main1<-lm(DepF ~ DepB + study_2 + study_3 + isdNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_SD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_SD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 

#### RMSSD ####
lm_FU_NA_MSSD_main1<-lm(DepF ~ DepB + study_2 + study_3 + iMSSDNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_MSSD_main1,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_MSSD_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 

#### Autocorrelation ####
lm_FU_NA_autocor_main1<-lm(DepF ~ DepB + study_2 + study_3 + autocorNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_autocor_main1,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_autocor_main1,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 

```

###### Model 2: Main effects with controlling for affect level

```{r mega-analysis main ML F NA final, cache=FALSE}
#### Standard Deviation ####
lm_FU_NA_SD_main2<-lm(DepF ~ DepB + study_2 + study_3 + isdNA + imeanNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_SD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_SD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


#### RMSSD ####
lm_FU_NA_MSSD_main2<-lm(DepF ~ DepB + study_2 + study_3 + iMSSDNA + imeanNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_MSSD_main2,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_MSSD_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


#### Autocorrelation ####
lm_FU_NA_autocor_main2<-lm(DepF ~ DepB + study_2 + study_3 + autocorNA + imeanNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_autocor_main2,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_autocor_main2,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


```

###### Model 3: Interaction effects affective dynamics and level

```{r mega-analysis mod with ML FU NA final, cache=FALSE}
#### Standard Deviation ####
lm_FU_NA_SD_mod<-lm(DepF ~ DepB +  study_2 + study_3 + isdNA + imeanNA + isdNA:imeanNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_SD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_SD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


#### RMSSD ####
lm_FU_NA_MSSD_mod<-lm(DepF ~ DepB +  study_2 + study_3 + iMSSDNA + imeanNA + iMSSDNA:imeanNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_MSSD_mod,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_MSSD_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


#### Autocorrelation ####
lm_FU_NA_autocor_mod<-lm(DepF ~ DepB + study_2 + study_3 + autocorNA + imeanNA + autocorNA:imeanNA + Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_NA_autocor_mod,digits = getOption("jtools-digits", 3), vifs=TRUE) 
summ(lm_FU_NA_autocor_mod,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 


```


# EXTRA ANALYSES

In July 2023, we added the following analyses after reviews. 

## Relative SD

The reviewer was concerned about the known confounding between the SD and the mean and
was wondering whether the main result of the interaction holds when with the relative SD.
To address this comment, we repeated the main model with the relative SD, following the procedures of Mestdagh et al. (2018).


```{r mega-analysis rel SD, cache=FALSE}

# Model 1: Main effect of inertia and variability
lm_Base_NA_SDAR_main1_rel<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + relisdNA + autocorNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_SDAR_main1_rel,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SDAR_main1_rel,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 

# Model 2: Controlling for Level
lm_Base_NA_SDAR_main2_rel<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + relisdNA + autocorNA + imeanNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_SDAR_main2_rel,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SDAR_main2_rel,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 

# Model 3: Interaction
lm_Base_NA_SDAR_mod_rel<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + relisdNA + autocorNA + imeanNA + relisdNA:imeanNA + autocorNA:imeanNA + Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_NA_SDAR_mod_rel,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SDAR_mod_rel,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE) 

```

## Influence of sampling frequency

We got the comment whether sampling frequency has an influence on the results.
To address this comment, we plotted individual estimates per study, together with
the sampling frequency. I additionally added analyses, where I included interactions between the sampling
frequency and the predictors. A significant interaction should indicate that the effect
of one of our predictors on depression depends on the sampling frequency.
However, sampling frequency is a stable between-study variable with little variation and as such
the results should only be used for visual inspection, but do not allow any hard conclusions.


Note that dataset 5 had 4 beeps on Friday and 9 beeps on Saturday and Sunday. 
For these extra analyses, I took the mean of those beeps, which are rounded 7.

### Add sampling frequency to overall datasets

```{r mega-analysis sampling freq add, cache=FALSE}
mega_all_cen_std <- mega_all_cen_std %>% 
  dplyr::mutate(sampling =
                       ifelse(study == 1, 1,
                              ifelse(study == 2, 10,
                                     ifelse(study == 3 | study == 4, 5,
                                            ifelse(study == 5, 7,
                                                   ifelse(study == 6 | study == 7, 3, NA))))))

# Subset other datasets
mega_all_cen_std_3 <- mega_all_cen_std %>%
  dplyr::filter(study == "1" | study == "2" | study == "3") 

mega_all_cen_std_4 <- mega_all_cen_std %>%
  dplyr::filter(study == "1" | study == "2" | study == "3" | study == "7") 

mega_all_cen_std_5 <- mega_all_cen_std %>%
  dplyr::filter(study == "1" | study == "2" | study == "3" | study == "4" | study == "5")  

```

### Positive Affect Baseline

#### Sampling frequency as moderator

```{r mega-analysis sampling freq PA B analysis, cache=FALSE}

## Main effects only
lm_Base_PA_SDAR_main1_sam<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 +
                                isdPA + autocorPA + imeanPA +
                                sampling:isdPA + sampling:autocorPA + sampling:imeanPA +
                                Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_SDAR_main1_sam,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_SDAR_main1_sam,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

# significant interaction for isdPA:sampling. Following up with Johnson Neyman
johnson_neyman(lm_Base_PA_SDAR_main1_sam, pred=isdPA, modx=sampling, plot = TRUE) #plot

## Interaction effects 
lm_Base_PA_SDAR_mod_sam<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 +
                                isdPA + autocorPA + imeanPA + isdPA:imeanPA + autocorPA:imeanPA +
                                sampling:isdPA + sampling:autocorPA + sampling:imeanPA +
                                sampling:isdPA:imeanPA + sampling:autocorPA:imeanPA +
                                Age + Sex, data=mega_all_cen_std_5)
summ(lm_Base_PA_SDAR_mod_sam,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_PA_SDAR_mod_sam,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

#### Individual estimates

```{r mega-analysis sampling freq PA B graph, cache=FALSE}

# Create a vector of study IDs & sampling frequency to be included
study_ids <- c(1:5)
sampling <- c(1, 10, 5, 5, 7)
study_name <- c("dataset 1", "dataset 2", "dataset 3", "dataset 4", "dataset 5")

# Define a function to extract coefficients and CI values for main effects
fun_PA_B <- function(study_id) {
  dataset <- mega_all_cen_std_5 %>% filter(study == study_id)
  lm_model <- lm(DepB ~ isdPA + autocorPA + imeanPA + Age + Sex, data = dataset)
  st_lm_model <- summ(lm_model, digits = getOption("jtools-digits", 3), confint = TRUE, scale = TRUE, transform.response = TRUE)
  
  data.frame(StudyID = study_id,
             SD = st_lm_model$coeftable[2, 1],
             SD_CI_lower = st_lm_model$coeftable[2, 2],
             SD_CI_upper = st_lm_model$coeftable[2, 3],
             AR = st_lm_model$coeftable[3, 1],
             AR_CI_lower = st_lm_model$coeftable[3, 2],
             AR_CI_upper = st_lm_model$coeftable[3, 3],
             M = st_lm_model$coeftable[4, 1],
             M_CI_lower = st_lm_model$coeftable[4, 2],
             M_CI_upper = st_lm_model$coeftable[4, 3])
}

# Use map function to apply the extraction function to each study_id
results_PA_B_main <- map_dfr(study_ids, fun_PA_B)

# Define a function to extract coefficients and CI values for interaction effects
fun_PA_B_mod <- function(study_id) {
  dataset <- mega_all_cen_std_5 %>% filter(study == study_id)
  lm_model <- lm(DepB ~ isdPA + autocorPA + imeanPA + isdPA:imeanPA + autocorPA:imeanPA + Age + Sex, data = dataset)
  st_lm_model <- summ(lm_model, digits = getOption("jtools-digits", 3), confint = TRUE, scale = TRUE, transform.response = TRUE)

  data.frame(StudyID = study_id,
             SD_int = st_lm_model$coeftable[7, 1],
             SD_int_CI_lower = st_lm_model$coeftable[7, 2],
             SD_int_CI_upper = st_lm_model$coeftable[7, 3],
             AR_int = st_lm_model$coeftable[8, 1],
             AR_int_CI_lower = st_lm_model$coeftable[8, 2],
             AR_int_CI_upper = st_lm_model$coeftable[8, 3])
}



# Use map function to apply the extraction function to each study_id
results_PA_B_mod <- map_dfr(study_ids, fun_PA_B_mod)

# Bind & Add sampling frequency & remove StudyID which is in there twice
results_PA_B <- cbind(study_name, sampling, results_PA_B_main, results_PA_B_mod)[, -13]

# Sort by Sampling frequency
results_PA_B <- results_PA_B %>% arrange(sampling)

# Plot the estimates per study with confidence intervals
# Create a function to generate the ggplot for each type of plot (SD, AR, Mean)
generate_ggplot <- function(data, y_var, y_lower, y_upper, ylab) {
  ggplot(data, aes(x = sampling, y = {{ y_var }}, ymin = {{ y_lower }}, ymax = {{ y_upper }})) +
    geom_point(aes(color = as.factor(study_name)), size = 2) +
    geom_errorbar(aes(color = as.factor(study_name)), width = 0.2, linewidth = 0.5) +
    labs(x = "Sampling frequency per day", y = ylab, color = "Study") +  
    scale_x_continuous(breaks = seq(0, 10, 1)) +
    theme_classic() +
    theme(plot.title = element_text(hjust = 0.5)) + 
    scale_color_brewer(palette = "Dark2")
}

# Individual Plots
plot_sd <- generate_ggplot(results_PA_B, SD, SD_CI_lower, SD_CI_upper, "Main effect \nVariability") + 
  coord_cartesian(ylim = c(-0.4, 0.4)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_ar <- generate_ggplot(results_PA_B, AR, AR_CI_lower, AR_CI_upper, "Main effect \nInertia") + 
  coord_cartesian(ylim = c(-0.5, 0.3)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_mean <- generate_ggplot(results_PA_B, M, M_CI_lower, M_CI_upper, "Main effect \nLevel") + 
  coord_cartesian(ylim = c(-0.5, 0.3)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_sd_m <- generate_ggplot(results_PA_B, SD_int, SD_int_CI_lower, SD_int_CI_upper, "Interaction \nVariability &\n Level") + 
  coord_cartesian(ylim = c(-0.3, 0.6)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_ar_m <- generate_ggplot(results_PA_B, AR_int, AR_int_CI_lower, AR_int_CI_upper, "Interaction \nInertia &\n Level") + 
  coord_cartesian(ylim = c(-0.3, 0.6)) + scale_y_continuous(breaks = pretty_breaks(n = 5))

# Arrange the plots with a shared title and some spacing
combined_PA_B <- cowplot::plot_grid( 
                                    plot_sd + theme(legend.position="none"), 
                                    plot_ar + theme(legend.position="none"),
                                    plot_mean + theme(legend.position="none"),
                                    plot_sd_m + theme(legend.position="none"),
                                    plot_ar_m + theme(legend.position="none"),
                                    ncol = 1)

# extract the legend from one of the plots
legend <- get_legend(
  # create some space to the left of the legend
  plot_sd + theme(legend.box.margin = margin(0, 0, 0, 12))
)

# add the legend to the row we made earlier. Give it one-third of 
# the width of one plot (via rel_widths).
combined_PA_B <- plot_grid(combined_PA_B, legend, rel_widths = c(3, 1))

# Print
combined_PA_B

# Save the combined plot as an image (e.g., in PNG format)
ggsave("../output/combined_PA_B.png", combined_PA_B, width = 16, height = 22, units = "cm")

```

### Positive Affect Follow-up

#### Sampling frequency as moderator

```{r mega-analysis sampling freq PA FU analysis, cache=FALSE}

## Main effects only
lm_FU_PA_SDAR_main1_sam<-lm(DepF ~ DepB + study_2 + study_3 +
                                isdPA + autocorPA + imeanPA +
                                sampling:isdPA + sampling:autocorPA + sampling:imeanPA +
                                Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_SDAR_main1_sam,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_SDAR_main1_sam,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

## Interaction effects 
lm_FU_PA_SDAR_mod_sam<-lm(DepF ~ DepB + study_2 + study_3 + 
                                isdPA + autocorPA + imeanPA + isdPA:imeanPA + autocorPA:imeanPA +
                                sampling:isdPA + sampling:autocorPA + sampling:imeanPA +
                                sampling:isdPA:imeanPA + sampling:autocorPA:imeanPA +
                                Age + Sex, data=mega_all_cen_std_3)
summ(lm_FU_PA_SDAR_mod_sam,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_PA_SDAR_mod_sam,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

#### Individual estimates


```{r mega-analysis sampling freq PA FU, cache=FALSE}

# Create a vector of study IDs & sampling frequency to be included
study_ids <- c(1:3)
sampling <- c(1, 10, 5)
study_name <- c("dataset 1", "dataset 2", "dataset 3")

# Define a function to extract coefficients and CI values for main effects
fun_PA_F <- function(study_id) {
  dataset <- mega_all_cen_std_3 %>% filter(study == study_id)
  lm_model <- lm(DepF ~ DepB + isdPA + autocorPA + imeanPA + Age + Sex, data = dataset)
  st_lm_model <- summ(lm_model, digits = getOption("jtools-digits", 3), confint = TRUE, scale = TRUE, transform.response = TRUE)
  
  data.frame(StudyID = study_id,
             SD = st_lm_model$coeftable[3, 1],
             SD_CI_lower = st_lm_model$coeftable[3, 2],
             SD_CI_upper = st_lm_model$coeftable[3, 3],
             AR = st_lm_model$coeftable[4, 1],
             AR_CI_lower = st_lm_model$coeftable[4, 2],
             AR_CI_upper = st_lm_model$coeftable[4, 3],
             M = st_lm_model$coeftable[5, 1],
             M_CI_lower = st_lm_model$coeftable[5, 2],
             M_CI_upper = st_lm_model$coeftable[5, 3])
}

# Use map function to apply the extraction function to each study_id
results_PA_F_main <- map_dfr(study_ids, fun_PA_F)

# Define a function to extract coefficients and CI values for interaction effects
fun_PA_F_mod <- function(study_id) {
  dataset <- mega_all_cen_std_3 %>% filter(study == study_id)
  lm_model <- lm(DepF ~ DepB + isdPA + autocorPA + imeanPA + isdPA:imeanPA + autocorPA:imeanPA + Age + Sex, data = dataset)
  st_lm_model <- summ(lm_model, digits = getOption("jtools-digits", 3), confint = TRUE, scale = TRUE, transform.response = TRUE)

  data.frame(StudyID = study_id,
             SD_int = st_lm_model$coeftable[8, 1],
             SD_int_CI_lower = st_lm_model$coeftable[8, 2],
             SD_int_CI_upper = st_lm_model$coeftable[8, 3],
             AR_int = st_lm_model$coeftable[9, 1],
             AR_int_CI_lower = st_lm_model$coeftable[9, 2],
             AR_int_CI_upper = st_lm_model$coeftable[9, 3])
}

# Use map function to apply the extraction function to each study_id
results_PA_F_mod <- map_dfr(study_ids, fun_PA_F_mod)

# Bind & Add sampling frequency & remove StudyID which is in there twice
results_PA_F <- cbind(study_name, sampling, results_PA_F_main, results_PA_F_mod)[, -13]

# Sort by Sampling frequency
results_PA_F <- results_PA_F %>% arrange(sampling)

# Individual Plots
plot_sd <- generate_ggplot(results_PA_F, SD, SD_CI_lower, SD_CI_upper, "Main effect \nVariability") + 
  coord_cartesian(ylim = c(-0.3, 0.4)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_ar <- generate_ggplot(results_PA_F, AR, AR_CI_lower, AR_CI_upper, "Main effect \nInertia") + 
  coord_cartesian(ylim = c(-0.3, 0.4)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_mean <- generate_ggplot(results_PA_F, M, M_CI_lower, M_CI_upper, "Main effect \nLevel") + 
  coord_cartesian(ylim = c(-0.4, 0.1)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_sd_m <- generate_ggplot(results_PA_F, SD_int, SD_int_CI_lower, SD_int_CI_upper, "Interaction \nVariability & Level") + 
  coord_cartesian(ylim = c(-0.3, 0.2)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_ar_m <- generate_ggplot(results_PA_F, AR_int, AR_int_CI_lower, AR_int_CI_upper, "Interaction \nInertia & Level") + 
  coord_cartesian(ylim = c(-0.3, 0.2)) + scale_y_continuous(breaks = pretty_breaks(n = 5))

# Arrange the plots with a shared title and some spacing
combined_PA_F <- cowplot::plot_grid( 
                                    plot_sd + theme(legend.position="none"), 
                                    plot_ar + theme(legend.position="none"),
                                    plot_mean + theme(legend.position="none"),
                                    plot_sd_m + theme(legend.position="none"),
                                    plot_ar_m + theme(legend.position="none"),
                                    ncol = 1)

# extract the legend from one of the plots
legend <- get_legend(
  # create some space to the left of the legend
  plot_sd + theme(legend.box.margin = margin(0, 0, 0, 12))
)

# add the legend to the row we made earlier. Give it one-third of 
# the width of one plot (via rel_widths).
combined_PA_F <- plot_grid(combined_PA_F, legend, rel_widths = c(3, 1))

# Print
combined_PA_F

# Save the combined plot as an image (e.g., in PNG format)
ggsave("../output/combined_PA_F.png", combined_PA_F, width = 16, height = 22, units = "cm")

```

### Negative Affect Baseline

#### Sampling frequency as moderator

```{r mega-analysis sampling freq NA B analysis, cache=FALSE}

## Main effects only
lm_Base_NA_SDAR_main1_sam<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 + study_6 + study_7 +
                                isdNA + autocorNA + imeanNA +
                                sampling:isdNA + sampling:autocorNA + sampling:imeanNA +
                                Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_SDAR_main1_sam,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SDAR_main1_sam,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

## Interaction effects 
lm_Base_NA_SDAR_mod_sam<-lm(DepB ~ study_2 + study_3 + study_4 + study_5 +
                                isdNA + autocorNA + imeanNA + isdNA:imeanNA + autocorNA:imeanNA +
                                sampling:isdNA + sampling:autocorNA + sampling:imeanNA +
                                sampling:isdNA:imeanNA + sampling:autocorNA:imeanNA +
                                Age + Sex, data=mega_all_cen_std)
summ(lm_Base_NA_SDAR_mod_sam,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_Base_NA_SDAR_mod_sam,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```


#### Individual estimates


```{r mega-analysis sampling freq NA B, cache=FALSE}

# Create a vector of study IDs & sampling frequency to be included
study_ids <- c(1:7)
sampling <- c(1, 10, 5, 5, 7, 3, 3)
study_name <- c("dataset 1", "dataset 2", "dataset 3", "dataset 4", "dataset 5", "dataset 6", "dataset 7")

# Define a function to extract coefficients and CI values for main effects
fun_NA_B <- function(study_id) {
  dataset <- mega_all_cen_std %>% filter(study == study_id)
  lm_model <- lm(DepB ~ isdNA + autocorNA + imeanNA + Age + Sex, data = dataset)
  st_lm_model <- summ(lm_model, digits = getOption("jtools-digits", 3), confint = TRUE, scale = TRUE, transform.response = TRUE)
  
  data.frame(StudyID = study_id,
             SD = st_lm_model$coeftable[2, 1],
             SD_CI_lower = st_lm_model$coeftable[2, 2],
             SD_CI_upper = st_lm_model$coeftable[2, 3],
             AR = st_lm_model$coeftable[3, 1],
             AR_CI_lower = st_lm_model$coeftable[3, 2],
             AR_CI_upper = st_lm_model$coeftable[3, 3],
             M = st_lm_model$coeftable[4, 1],
             M_CI_lower = st_lm_model$coeftable[4, 2],
             M_CI_upper = st_lm_model$coeftable[4, 3])
}

# Use map function to apply the extraction function to each study_id
results_NA_B_main <- map_dfr(study_ids, fun_NA_B)

# Define a function to extract coefficients and CI values for interaction effects
fun_NA_B_mod <- function(study_id) {
  dataset <- mega_all_cen_std %>% filter(study == study_id)
  lm_model <- lm(DepB ~ isdNA + autocorNA + imeanNA + isdNA:imeanNA + autocorNA:imeanNA + Age + Sex, data = dataset)
  st_lm_model <- summ(lm_model, digits = getOption("jtools-digits", 3), confint = TRUE, scale = TRUE, transform.response = TRUE)

  data.frame(StudyID = study_id,
             SD_int = st_lm_model$coeftable[7, 1],
             SD_int_CI_lower = st_lm_model$coeftable[7, 2],
             SD_int_CI_upper = st_lm_model$coeftable[7, 3],
             AR_int = st_lm_model$coeftable[8, 1],
             AR_int_CI_lower = st_lm_model$coeftable[8, 2],
             AR_int_CI_upper = st_lm_model$coeftable[8, 3])
}



# Use map function to apply the extraction function to each study_id
results_NA_B_mod <- map_dfr(study_ids, fun_NA_B_mod)

# Bind & Add sampling frequency & remove StudyID which is in there twice
results_NA_B <- cbind(study_name, sampling, results_NA_B_main, results_NA_B_mod)[, -13]

# Sort by Sampling frequency
results_NA_B <- results_NA_B %>% arrange(sampling)

# Individual Plots
plot_sd <- generate_ggplot(results_NA_B, SD, SD_CI_lower, SD_CI_upper, "Main effect \nVariability") + 
  coord_cartesian(ylim = c(-0.6, 0.6)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_ar <- generate_ggplot(results_NA_B, AR, AR_CI_lower, AR_CI_upper, "Main effect \nInertia") + 
  coord_cartesian(ylim = c(-0.6, 0.4)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_mean <- generate_ggplot(results_NA_B, M, M_CI_lower, M_CI_upper, "Main effect \nLevel") + 
  coord_cartesian(ylim = c(-0.2, 1)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_sd_m <- generate_ggplot(results_NA_B, SD_int, SD_int_CI_lower, SD_int_CI_upper, "Interaction  \nVariability & Level") + 
  coord_cartesian(ylim = c(-0.5, 0.2)) + scale_y_continuous(breaks = pretty_breaks(n = 6))
plot_ar_m <- generate_ggplot(results_NA_B, AR_int, AR_int_CI_lower, AR_int_CI_upper, "Interaction \nInertia & Level") + 
  coord_cartesian(ylim = c(-0.2, 0.5)) + scale_y_continuous(breaks = pretty_breaks(n = 6))

# Arrange the plots with a shared title and some spacing
combined_NA_B <- cowplot::plot_grid( 
                                    plot_sd + theme(legend.position="none"), 
                                    plot_ar + theme(legend.position="none"),
                                    plot_mean + theme(legend.position="none"),
                                    plot_sd_m + theme(legend.position="none"),
                                    plot_ar_m + theme(legend.position="none"),
                                    ncol = 1)



# extract the legend from one of the plots
legend <- get_legend(
  # create some space to the left of the legend
  plot_sd + theme(legend.box.margin = margin(0, 0, 0, 12))
)

# add the legend to the row we made earlier. Give it one-third of 
# the width of one plot (via rel_widths).
combined_NA_B <- plot_grid(combined_NA_B, legend, rel_widths = c(3, 1))

# Print
combined_NA_B

# Save the combined plot as an image (e.g., in PNG format)
ggsave("../output/combined_NA_B.png", combined_NA_B, width = 16, height = 22, units = "cm")

```

### Negative Affect Follow-up

#### Sampling frequency as moderator

```{r mega-analysis sampling freq NA FU analysis, cache=FALSE}

## Main effects only
lm_FU_NA_SDAR_main1_sam<-lm(DepF ~ DepB + study_2 + study_3 + study_7 +
                                isdNA + autocorNA + imeanNA +
                                sampling:isdNA + sampling:autocorNA + sampling:imeanNA +
                                Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_SDAR_main1_sam,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_SDAR_main1_sam,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

## Interaction effects 
lm_FU_NA_SDAR_mod_sam<-lm(DepF ~ DepB + study_2 + study_3 + study_7 +
                                isdNA + autocorNA + imeanNA + isdNA:imeanNA + autocorNA:imeanNA +
                                sampling:isdNA + sampling:autocorNA + sampling:imeanNA +
                                sampling:isdNA:imeanNA + sampling:autocorNA:imeanNA +
                                Age + Sex, data=mega_all_cen_std_4)
summ(lm_FU_NA_SDAR_mod_sam,digits = getOption("jtools-digits", 3), vifs=TRUE)
summ(lm_FU_NA_SDAR_mod_sam,digits = getOption("jtools-digits", 3), confint = TRUE,scale = TRUE, transform.response = TRUE)

```

#### Individual estimates


```{r mega-analysis sampling freq NA FU, cache=FALSE}

# Create a vector of study IDs & sampling frequency to be included
study_ids <- c(1:3, 7)
sampling <- c(1, 10, 5, 3)
study_name <- c("dataset 1", "dataset 2", "dataset 3", "dataset 7")

# Define a function to extract coefficients and CI values for main effects
fun_NA_F <- function(study_id) {
  dataset <- mega_all_cen_std_4 %>% filter(study == study_id)
  lm_model <- lm(DepF ~ DepB + isdNA + autocorNA + imeanNA + Age + Sex, data = dataset)
  st_lm_model <- summ(lm_model, digits = getOption("jtools-digits", 3), confint = TRUE, scale = TRUE, transform.response = TRUE)
  
  data.frame(StudyID = study_id,
             SD = st_lm_model$coeftable[3, 1],
             SD_CI_lower = st_lm_model$coeftable[3, 2],
             SD_CI_upper = st_lm_model$coeftable[3, 3],
             AR = st_lm_model$coeftable[4, 1],
             AR_CI_lower = st_lm_model$coeftable[4, 2],
             AR_CI_upper = st_lm_model$coeftable[4, 3],
             M = st_lm_model$coeftable[5, 1],
             M_CI_lower = st_lm_model$coeftable[5, 2],
             M_CI_upper = st_lm_model$coeftable[5, 3])
}

# Use map function to apply the extraction function to each study_id
results_NA_F_main <- map_dfr(study_ids, fun_NA_F)

# Define a function to extract coefficients and CI values for interaction effects
fun_NA_F_mod <- function(study_id) {
  dataset <- mega_all_cen_std_4 %>% filter(study == study_id)
  lm_model <- lm(DepF ~ DepB + isdNA + autocorNA + imeanNA + isdNA:imeanNA + autocorNA:imeanNA + Age + Sex, data = dataset)
  st_lm_model <- summ(lm_model, digits = getOption("jtools-digits", 3), confint = TRUE, scale = TRUE, transform.response = TRUE)

  data.frame(StudyID = study_id,
             SD_int = st_lm_model$coeftable[8, 1],
             SD_int_CI_lower = st_lm_model$coeftable[8, 2],
             SD_int_CI_upper = st_lm_model$coeftable[8, 3],
             AR_int = st_lm_model$coeftable[9, 1],
             AR_int_CI_lower = st_lm_model$coeftable[9, 2],
             AR_int_CI_upper = st_lm_model$coeftable[9, 3])
}



# Use map function to apply the extraction function to each study_id
results_NA_F_mod <- map_dfr(study_ids, fun_NA_F_mod)

# Bind & Add sampling frequency & remove StudyID which is in there twice
results_NA_F <- cbind(study_name, sampling, results_NA_F_main, results_NA_F_mod)[, -13]

# Sort by Sampling frequency
results_NA_F <- results_NA_F %>% arrange(sampling)

# Individual Plots
plot_sd <- generate_ggplot(results_NA_F, SD, SD_CI_lower, SD_CI_upper, "Main effect \nVariability") + 
  coord_cartesian(ylim = c(-0.3, 0.4)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_ar <- generate_ggplot(results_NA_F, AR, AR_CI_lower, AR_CI_upper, "Main effect \nInertia") + 
  coord_cartesian(ylim = c(-0.3, 0.4)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_mean <- generate_ggplot(results_NA_F, M, M_CI_lower, M_CI_upper, "Main effect \nLevel") + 
  coord_cartesian(ylim = c(-0.3, 0.6)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_sd_m <- generate_ggplot(results_NA_F, SD_int, SD_int_CI_lower, SD_int_CI_upper, "Interaction \nVariability & Level") + 
  coord_cartesian(ylim = c(-0.2, 0.3)) + scale_y_continuous(breaks = pretty_breaks(n = 5))
plot_ar_m <- generate_ggplot(results_NA_F, AR_int, AR_int_CI_lower, AR_int_CI_upper, "Interaction \nInertia & Level") + 
  coord_cartesian(ylim = c(-0.2, 0.3)) + scale_y_continuous(breaks = pretty_breaks(n = 5))

# Arrange the plots with a shared title and some spacing
combined_NA_F <- cowplot::plot_grid( 
                                    plot_sd + theme(legend.position="none"), 
                                    plot_ar + theme(legend.position="none"),
                                    plot_mean + theme(legend.position="none"),
                                    plot_sd_m + theme(legend.position="none"),
                                    plot_ar_m + theme(legend.position="none"),
                                    ncol = 1)



# extract the legend from one of the plots
legend <- get_legend(
  # create some space to the left of the legend
  plot_sd + theme(legend.box.margin = margin(0, 0, 0, 12))
)

# add the legend to the row we made earlier. Give it one-third of 
# the width of one plot (via rel_widths).
combined_NA_F <- plot_grid(combined_NA_F, legend, rel_widths = c(3, 1))

# Print
combined_NA_F

# Save the combined plot as an image (e.g., in PNG format)
ggsave("../output/combined_NA_F.png", combined_NA_F, width = 16, height = 22, units = "cm")


```



# REFERENCES

de Haan-Rietdijk, S., Kuppens, P., & Hamaker, E. L. (2016). What’s in a day? A guide to decomposing the variance in intensive longitudinal data. *Frontiers in Psychology, 7*, 1–16. https://doi.org/10.3389/FPSYG.2016.00891

Eisele, G., Lafit, G., Vachon, H., Kuppens, P., Houben, M., Myin-Germeys, I., & Viechtbauer, W. (2021). Affective structure, measurement invariance, and reliability across different experience sampling protocols. *Journal of Research in Personality, 92*, 104094. https://doi.org/10.1016/J.JRP.2021.104094

McCabe, C. J., Kim, D. S., & King, K. M. (2018). Improving present practices in the visual display of interactions. *Advances in Methods and Practices in Psychological Science, 1* (2), 147–165. https://doi.org/10.1177/2515245917746792

Mestdagh, M., Pe, M., Pestman, W., Verdonck, S., Kuppens, P., & Tuerlinckx, F. (2018). Sidelining the mean: The relative variability index as a generic mean-corrected variability measure for bounded variables. *Psychological Methods, 23*(4), 690–707. https://doi.org/10.1037/met0000153

Ram, N. (2017). Analysis of Experience Sampling & EMA Data—Chapter 2.2: Univariate Intraindividual Variability Metrics (Part 2) | QuantDev Methodology. https://quantdev.ssri.psu.edu/tutorials/analysis-experience-sampling-ema-data-chapter-22-univariate-intraindividual-variability


